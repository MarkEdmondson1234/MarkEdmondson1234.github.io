<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Mark Edmondson</title>
    <link>https://code.markedmondson.me/post/index.xml</link>
    <description>Recent content in Posts on Mark Edmondson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-GB</language>
    <copyright>Powered by [Hugo](//gohugo.io). Theme by [PPOffice](http://github.com/ppoffice).</copyright>
    <lastBuildDate>Wed, 15 Aug 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://code.markedmondson.me/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Talking Google Analytics dashboards via R, Shiny and Text-to-Speech APIs</title>
      <link>https://code.markedmondson.me/talking-google-analytics-dashboards</link>
      <pubDate>Wed, 15 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://code.markedmondson.me/talking-google-analytics-dashboards</guid>
      <description>&lt;p&gt;What really makes Google Analytics stand apart from other analytics solutions for me is its integration with the Google Cloud, such as BigQuery and its machine learning APIs. An example of this integration is given in this workshop video that details how to use the &lt;a href=&#34;https://developers.google.com/analytics/devguides/reporting/core/v4/&#34;&gt;Google Analytics&lt;/a&gt; and &lt;a href=&#34;https://cloud.google.com/text-to-speech/&#34;&gt;Text-to-speech&lt;/a&gt; APIs to create a dashboard that talks through your statistics.&lt;/p&gt;
&lt;!--more--&gt;
&lt;div id=&#34;youtube-workshop-video&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;YouTube Workshop video&lt;/h2&gt;
&lt;p&gt;The whole 40min workshop is available below, which talks through &lt;a href=&#34;https://github.com/MarkEdmondson1234/verbal_ga_shiny&#34;&gt;this GitHub repo&lt;/a&gt;.&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/4Ht_vEXJ4wo&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;A demo of the speech it creates can be heard in this audio snippet:&lt;/p&gt;
&lt;audio controls&gt;
&lt;source src=&#34;../verbal-demo.wav&#34;&gt;
&lt;/audio&gt;
&lt;p&gt;I’ve also cut out shorter snippets that focus on concepts if you want to skip in and out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Part I - basic Shiny app with Google Analytics data: &lt;a href=&#34;https://youtu.be/IqAaCk_3ZKU&#34; class=&#34;uri&#34;&gt;https://youtu.be/IqAaCk_3ZKU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Part II - adding text-to-speech with googleLanguageR &lt;a href=&#34;https://youtu.be/Ny0e7vHFu6o&#34; class=&#34;uri&#34;&gt;https://youtu.be/Ny0e7vHFu6o&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Part III - custom skin for the Shiny app using gentellelaShiny &lt;a href=&#34;https://youtu.be/Dp6Y6Trn97A&#34; class=&#34;uri&#34;&gt;https://youtu.be/Dp6Y6Trn97A&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;going-further&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Going further&lt;/h2&gt;
&lt;p&gt;Whilst this proof of concept is a bit of fun to demonstrate how these APIs can work together via the R libraries &lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/&#34;&gt;&lt;code&gt;googleAnalyticsR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/&#34;&gt;&lt;code&gt;googleLanguageR&lt;/code&gt;&lt;/a&gt;, I hope the applications can go beyond this demo.&lt;/p&gt;
&lt;p&gt;Accessibility is an obvious first application, giving those that have trouble seeing another way to experience dashboard plots. Equally, the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/speech.html&#34;&gt;speech-to-text API&lt;/a&gt; offer a way to control reports without needing a dashboard. Often these services can also help enhance all users by letting them experience reports in other mediums such as by phone or in the car, and may help breathe life into stale dashboards that have a habit of slowly being ignored over time.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/translation.html&#34;&gt;Translation APIs&lt;/a&gt; also allow internationalisation of reports be it in speech or text, the &lt;a href=&#34;https://github.com/ropensci/googleLanguageR/tree/master/inst/shiny/capture_speech&#34;&gt;demo Shiny app of &lt;code&gt;googleLanguageR&lt;/code&gt;&lt;/a&gt; demonstrates talking in several languages.&lt;/p&gt;
&lt;p&gt;Possibily the most advanced but potentially most powerful application uses the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/nlp.html&#34;&gt;Natural Language API&lt;/a&gt; to parse out meaning, entities and sentiment from text. For instance, grading user generated content and then creating data reports on the fly that may address their concerns.&lt;/p&gt;
&lt;p&gt;Another interesting application may be how text/speech enabled apps can interact with other robots, such as Alexa or Google Home. As more and more apps become voice enabled, voice and computer generated speech could become a universal translator between systems that would take heavy coding otherwise.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Anyhow, I hope the video is of some use. I’m creating more video content these days as I think it helps see how a workflow happens in real life, along with all the umms and errs and mistakes that happen to everyone :) If you have any video requests do let me know, otherwise you can keep track of any new videos on my &lt;a href=&#34;https://www.youtube.com/channel/UC3pF8VqQGmtpjAr_9VUEmTQ/videos&#34;&gt;YouTube channel&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R on Kubernetes - serverless Shiny, R APIs and scheduled scripts</title>
      <link>https://code.markedmondson.me/r-on-kubernetes-serverless-shiny-r-apis-and-scheduled-scripts</link>
      <pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://code.markedmondson.me/r-on-kubernetes-serverless-shiny-r-apis-and-scheduled-scripts</guid>
      <description>&lt;h2 id=&#34;why-run-r-on-kubernetes&#34;&gt;Why run R on Kubernetes?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; is a free and open-source utility to run jobs within a computer cluster.  It abstracts away the servers the jobs are running on so you need only worry about the code to run.  It has features such as &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/&#34;&gt;scheduling&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler&#34;&gt;auto-scaling&lt;/a&gt;, and auto-healing to replace nodes if they breakdown.&lt;/p&gt;

&lt;p&gt;If you only need to run R on a single machine, then its probably a bit OTT to use Kubernetes, but if you are starting to work with multiple Docker containers and/or VMs it gets more and more attractive to have a way to easily orchestrate them.&lt;/p&gt;

&lt;p&gt;Kubernetes works via &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt; containers, so if you are already familiar with using Docker for abstracting away code environments, it should be a short step up to abstracting away the computers those Docker containers run upon.
&lt;/p&gt;

&lt;p&gt;R is a good mix with Docker since it provides a way to have stable production environments in a landscape of shifting dependencies given R&amp;rsquo;s fast changing open source foundation.  There is a growing ecosystem to help R users with Docker including the &lt;a href=&#34;https://www.rocker-project.org/&#34;&gt;Rocker Project&lt;/a&gt;, &lt;a href=&#34;http://o2r.info/2017/05/30/containerit-package/&#34;&gt;containerit&lt;/a&gt; and &lt;a href=&#34;https://github.com/wch/harbor&#34;&gt;harbor&lt;/a&gt;, and Rhys Jackson has also crafted a Kubernetes client for R in &lt;a href=&#34;https://github.com/RhysJackson/googleKubernetesR&#34;&gt;googleKubernetesR&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If your R scripts are deployed in Docker containers, you can use them in Kubernetes.  This includes applications such as R powered APIs using OpenCPU or plumber, Shiny apps, batch R jobs that can scale horizontally over many CPUs, or scheduled analysis.&lt;/p&gt;

&lt;p&gt;What you gain is reliable, flexible, production ready R applications, that will scale, run on many cloud providers including Google, and once set up easy to deploy - in most cases pushing to GitHub can be the trigger to serve your new code.&lt;/p&gt;

&lt;h3 id=&#34;alternatives&#34;&gt;Alternatives&lt;/h3&gt;

&lt;p&gt;This article deploys to Google Kubernetes Engine (GKE), but there are many other methods to cover some of Kubernetes functionality, that you may prefer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/gke_icon.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes itself can run on all the cloud providers aside Google Cloud Platform, as well as your own servers.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/&#34;&gt;Google App Engine Flexible&lt;/a&gt; is a more managed platform that takes care of some details, but you lose control of others.  I wrote about this previously for running R APIs and a demo is in this &lt;a href=&#34;https://github.com/MarkEdmondson1234/serverless-R-API-appengine&#34;&gt;GitHub repo&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;Docker Compose&lt;/a&gt; is an equivalent service that only runs on one host, &lt;a href=&#34;https://docs.docker.com/engine/swarm/&#34;&gt;Docker Swarm&lt;/a&gt; runs on multiple hosts and most comparible to Kubernetes.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/services/container-service/&#34;&gt;Azure Container Service&lt;/a&gt; is a more managed Kubernetes platform than GKE&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Regardless which one you use, the principles are similar in that they all build on Docker containers, and one of the advantages of Kubernetes is you can move to other providers a lot more easily, since Docker/Kubernetes is fast becoming the standard for cloud developments.&lt;/p&gt;

&lt;h3 id=&#34;setup-background&#34;&gt;Setup background&lt;/h3&gt;

&lt;p&gt;The below is a setup I have used for the above applications, and may be able to help if you are looking for similar functionality.&lt;/p&gt;

&lt;p&gt;I ran into several gotchas whilst developing it, so hopefully it will help you avoid some of those. Most vexing for me was finding a way to serve different R scripts on one kubernetes cluster on different URL paths (e.g. &lt;code&gt;/r-script1&lt;/code&gt;, &lt;code&gt;/shinyapp2&lt;/code&gt;, &lt;code&gt;/r-api-3&lt;/code&gt;, etc) - many thanks to Rhys&amp;rsquo; help with that.  The answer was that (at the time of writing) the default Google Kubernetes Engine doesn&amp;rsquo;t support rewriting URLs, so instead its better to install another application on your cluster to take care of the above URL serving (termed &lt;code&gt;ingress fanout&lt;/code&gt;), namely using &lt;code&gt;nginx ingress&lt;/code&gt; instead of the default GKE ingress.&lt;/p&gt;

&lt;p&gt;This article then demonstrates:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Creating a Kubernetes cluster for your R applications&lt;/li&gt;
&lt;li&gt;Installing Helm (and Tiller) to aid with a nginx ingress installation&lt;/li&gt;
&lt;li&gt;Deploying R containers, with examples for Shiny, an R API and an R scheduled script&lt;/li&gt;
&lt;li&gt;Serving up those R containers on nice URLs&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;These were used to help develop the below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer&#34;&gt;Setting up HTTP load balancing with Ingress&lt;/a&gt; Google tutorial&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/community/tutorials/nginx-ingress-gke&#34;&gt;Ingress with nginx&lt;/a&gt; modified with &lt;a href=&#34;https://docs.bitnami.com/kubernetes/how-to/configure-rbac-in-your-kubernetes-cluster/&#34;&gt;this&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;My &lt;a href=&#34;https://stackoverflow.com/questions/48452556/setting-up-a-kuberentes-cluster-with-http-load-balancing-ingress-for-rstudio-and&#34;&gt;StackOverflow question&lt;/a&gt; on http load balancing&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/kubectl/overview/&#34;&gt;Kubernetes reference documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.bitnami.com/kubernetes/how-to/secure-kubernetes-services-with-ingress-tls-letsencrypt/&#34;&gt;Kubernetes with nginx, TLS and LetsEncrypt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;part-1-creating-an-r-cluster&#34;&gt;Part 1 - Creating an R cluster&lt;/h2&gt;

&lt;p&gt;Follow the &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-container-cluster&#34;&gt;setup steps to authenticate with gcloud and kuberctl&lt;/a&gt; then create your cluster via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud container clusters create r-cluster --num-nodes=3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;or if using &lt;code&gt;googleKubernetesR&lt;/code&gt; :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleKubernetesR)

## create a 3 node cluster called r-cluster with defaults
createCluster(projectId = gcp_get_project(), zone = &amp;quot;europe-west3-b&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You&amp;rsquo;ll need to wait a bit as it provisions all the VMs.&lt;/p&gt;

&lt;p&gt;Most of the below will use the terminal/shell for working rather than R, but in the future a lot of this may be possible via &lt;code&gt;googleKubernetesR&lt;/code&gt; within an R session.&lt;/p&gt;

&lt;p&gt;Set up your shell to get the credentials for Kuberctl:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud container clusters get-credentials r-cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we are all set to start setting up this cluster for R jobs!&lt;/p&gt;

&lt;p&gt;Here is a screenshot from the web UI of what it should look like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/r-cluster.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;part-2-install-the-nginx-ingress-controller&#34;&gt;Part 2 - Install the nginx ingress controller&lt;/h2&gt;

&lt;p&gt;The first recommended setup is to enable an ingress controller.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/ingress_controller.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Image from &lt;a href=&#34;https://www.nginx.com/products/nginx/kubernetes-ingress-controller/&#34;&gt;nginx&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This is a pod that directs internet traffic to the right R container we set up later.&lt;/p&gt;

&lt;p&gt;We install the ingress controller via &lt;a href=&#34;https://helm.sh/&#34;&gt;Helm&lt;/a&gt; and its counterpart Tiller on the cluster.&lt;/p&gt;

&lt;p&gt;Helm is a package manager for Kubernetes.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You can skip installing Helm/Tiller/ingress controller if you don&amp;rsquo;t need to have R apps on a URL, for instance if its just scheduled cron jobs you want on your cluster, but it is recommended if you want to serve up APIs and Shiny.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The below is mostly copied straight from &lt;a href=&#34;https://docs.bitnami.com/kubernetes/how-to/configure-rbac-in-your-kubernetes-cluster/&#34;&gt;Bitnami&lt;/a&gt;&amp;rsquo;s article.&lt;/p&gt;

&lt;h3 id=&#34;install-helm&#34;&gt;Install Helm&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;The below assumes RBAC rules are enabled, which is true from Kubernetes version 1.8 onwards.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -o get_helm.sh https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get
chmod +x get_helm.sh
./get_helm.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install-tiller-on-cluster&#34;&gt;Install Tiller on cluster&lt;/h3&gt;

&lt;h4 id=&#34;step-1-create-the-tiller-service-account&#34;&gt;Step 1: Create the Tiller service account&lt;/h4&gt;

&lt;p&gt;Create a tiller-serviceaccount.yaml file using kubectl:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl create serviceaccount tiller --namespace kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;step-2-bind-the-tiller-service-account-to-the-cluster-admin-role&#34;&gt;Step 2: Bind the Tiller service account to the cluster-admin role&lt;/h4&gt;

&lt;p&gt;Create a &lt;code&gt;tiller-clusterrolebinding.yaml&lt;/code&gt; file with the following contents:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1beta1
      metadata:
        name: tiller-clusterrolebinding
      subjects:
      - kind: ServiceAccount
        name: tiller
        namespace: kube-system
      roleRef:
        kind: ClusterRole
        name: cluster-admin
        apiGroup: &amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Deploy the ClusterRoleBinding:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl create -f tiller-clusterrolebinding.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;step-3-update-the-existing-tiller-deployment&#34;&gt;Step 3: Update the existing Tiller deployment&lt;/h4&gt;

&lt;p&gt;Update the existing tiller-deploy deployment with the Service Account you created earlier:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;helm init --service-account tiller --upgrade
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait a few seconds for the Tiller server to be redeployed.&lt;/p&gt;

&lt;h4 id=&#34;step-4-test-the-new-helm&#34;&gt;Step 4: Test the new Helm&lt;/h4&gt;

&lt;p&gt;All being well, you should be able to execute this command without errors:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;helm ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s it! You have configured Helm in your Kubernetes cluster.&lt;/p&gt;

&lt;h4 id=&#34;step-5-deploy-ngnix-ingress-controller&#34;&gt;Step 5: Deploy ngnix Ingress Controller&lt;/h4&gt;

&lt;p&gt;Now you can deploy the &lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/&#34;&gt;nginx ingress controller&lt;/a&gt; using Helm to take care of details:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;helm install --name nginx-ingress stable/nginx-ingress --set rbac.create=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see your IP when you issue:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl get service nginx-ingress-controller
#&amp;gt; NAME                       CLUSTER-IP    EXTERNAL-IP   PORT(S)                      AGE
#&amp;gt; nginx-ingress-controller   10.7.253.89   &amp;lt;pending&amp;gt;     80:32713/TCP,443:31061/TCP   23s
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;part-3-deploying-r-workloads&#34;&gt;Part 3 - Deploying R Workloads!&lt;/h2&gt;

&lt;p&gt;Now the cluster is all setup for you to deploy your R containers.  You can install other containers in any other language too, such as normal HTML webapps or utilties.&lt;/p&gt;

&lt;p&gt;We deploy them using the command &lt;code&gt;kubectl run&lt;/code&gt;, and go through some examples below.&lt;/p&gt;

&lt;h3 id=&#34;a-shiny-apps&#34;&gt;A: Shiny apps&lt;/h3&gt;

&lt;p&gt;Below we deploy two Shiny apps, in this case the &lt;a href=&#34;https://github.com/cloudyr/googleComputeEngineR/tree/master/inst/dockerfiles/shiny-googleAuthRdemo&#34;&gt;&lt;code&gt;googleAuthR&lt;/code&gt; demo app&lt;/a&gt; that is configured to run on port 3838, in a &lt;code&gt;/shiny/&lt;/code&gt; folder, and a &lt;a href=&#34;https://github.com/flaviobarros/shiny-wordcloud&#34;&gt;wordcloud Shiny app by Flavio Barros&lt;/a&gt; that has had its Dockerfile configured to run on port 80 in the root folder, to demonstrate how to handle different Docker deployments.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl run shiny1 --image gcr.io/gcer-public/shiny-googleauthrdemo:latest --port 3838
kubectl run shiny2 --image=flaviobarros/shiny-wordcloud --port=80
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;expose-containers-as-node-ports&#34;&gt;Expose containers as node ports&lt;/h4&gt;

&lt;p&gt;At the moment we are only selecting which services the cluster will run - we won&amp;rsquo;t be able to see them yet until we expose the apps and configure the ingress rules&lt;/p&gt;

&lt;p&gt;These will be pointed to via the nginx ingress controller.&lt;/p&gt;

&lt;p&gt;We first open up their ports as nodeports, so the ingress can see them:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl expose deployment shiny1 --target-port=3838  --type=NodePort
kubectl expose deployment shiny2 --target-port=80  --type=NodePort
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;create-ingress-with-ngninx&#34;&gt;Create Ingress with ngninx&lt;/h4&gt;

&lt;p&gt;This lets you be able to call the different services from one ip address.&lt;/p&gt;

&lt;p&gt;Save this as &lt;code&gt;r-ingress-nginx.yaml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: r-ingress-nginx
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: / 
    nginx.ingress.kubernetes.io/ssl-redirect: &amp;quot;false&amp;quot;
spec:
  rules:
  - http:
      paths:
      - path: /wordcloud/
      # app deployed to /wordcloud/
        backend:
          serviceName: shiny2
          servicePort: 80
      - path: /gar/
      # app deployed to /gar/shiny/
        backend:
          serviceName: shiny1
          servicePort: 3838

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Deploy via&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl apply -f r-ingress-nginx.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Verify and keep refreshing until it has an ip address assigned in the &lt;code&gt;ADDRESS&lt;/code&gt; field (5mins)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl get ingress r-ingress-nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;see-your-deployed-containers&#34;&gt;See your deployed containers&lt;/h4&gt;

&lt;p&gt;The Nginx ingress controller takes care of any URL path not covered in the &lt;code&gt;r-ingress-nginx&lt;/code&gt; ingress.  By default this should give a &lt;code&gt;default backend - 404&lt;/code&gt; message.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/gar/shiny/&lt;/code&gt; is served by the googleAuthR demo app.  &lt;code&gt;/gar/&lt;/code&gt; covers the standard Shiny welcome screen, and &lt;code&gt;/shiny/&lt;/code&gt; is the folder that is configured in the Dockerfile&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/wordcloud/&lt;/code&gt; holds the Wordcloud Shiny app. In this case, the Shiny app is configured to appear in the root directory of the &lt;code&gt;/wordcloud/&lt;/code&gt; URL.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here are some screenshots:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/gar_app.png&#34; alt=&#34;googleAuthR app&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/wordcloud.png&#34; alt=&#34;Wordcloud app&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/default404.png&#34; alt=&#34;Default webpage if not defined in ingress&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Note it matters where you put the app in your Dockerfile.  The &lt;code&gt;nginx.ingress.kubernetes.io/rewrite-target: /&lt;/code&gt; line in the ingress lets you specify a URL that is relative to your specified URLs.&lt;/p&gt;

&lt;p&gt;If you want to have your app served on the absolute URL specified in the Dockerfile, see below for a no-redirect version.&lt;/p&gt;

&lt;h4 id=&#34;no-redirects&#34;&gt;No redirects&lt;/h4&gt;

&lt;p&gt;Without redirects work too, but the Shiny app needs to have the correct URL path configured in its Docker that matches the URLs you are specifying in the Ingress e.g. for our examples as the googleAuthR demo is in path &lt;code&gt;/shiny/&lt;/code&gt; and the Wordcloud demo in the root &lt;code&gt;/&lt;/code&gt;, the Ingress rules need to match those. Beware clashes where two Shiny apps have configured to the same URL path, e.g. root &lt;code&gt;/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Save this as &lt;code&gt;r-ingress-nginx-noredirect.yaml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: r-ingress-nginx
  annotations:
    kubernetes.io/ingress.class: nginx
    #nginx.ingress.kubernetes.io/rewrite-target: / # not used in this example
    nginx.ingress.kubernetes.io/ssl-redirect: &amp;quot;false&amp;quot;
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          serviceName: shiny2
          servicePort: 80
      - path: /shiny/
        backend:
          serviceName: shiny1
          servicePort: 3838
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Deploy via&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f r-ingress-nginx-noredirect.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again, the Nginx ingress controller takes care of any URL path not covered in the &lt;code&gt;r-ingress-nginx&lt;/code&gt; ingress.&lt;/p&gt;

&lt;p&gt;However now:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/shiny/&lt;/code&gt; is served by the googleAuthR demo app.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/&lt;/code&gt; holds the Wordcloud Shiny app.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;b-r-apis-opencpu&#34;&gt;B: R APIs - OpenCPU&lt;/h3&gt;

&lt;p&gt;Kubernetes is well suited to serving R APIs as they can auto-scale to demand, covering peaks and troughs in demand.&lt;/p&gt;

&lt;p&gt;A demo API is shown below, using &lt;a href=&#34;http://code.markedmondson.me/predictClickOpenCPU/&#34;&gt;&lt;code&gt;openCPU&lt;/code&gt; for predicting website clicks&lt;/a&gt;, but the same principle applies if using &lt;code&gt;plumber&lt;/code&gt; or another framework by swapping out the Dockerfile to serve the API on a port.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This assumes you have configured the ingress controller above&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl run opencpu-demo --image gcr.io/gcer-public/opencpu-demo:latest --port 8004
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Expose the port as before:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl expose deployment opencpu-demo --target-port=8004  --type=NodePort
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we do another Ingress deployment for the R-API, in addition to the Shiny apps above.&lt;/p&gt;

&lt;p&gt;Save the below as &lt;code&gt;r-ingress-opencpu.yaml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: r-ingress-nginx-ocpu
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: &amp;quot;false&amp;quot;
spec:
  rules:
  - http:
      paths:
      - path: /ocpu/
        backend:
          serviceName: opencpu-demo
          servicePort: 8004
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;and deploy via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl apply -f r-ingress-opencpu.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your ip address will be as shown as your &lt;code&gt;ADDRESS&lt;/code&gt; when you issue:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl get ingress r-ingress-nginx-ocpu
#&amp;gt; NAME                   HOSTS     ADDRESS         PORTS     AGE
#&amp;gt; r-ingress-nginx-ocpu   *         35.205.249.34   80        10m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note no rewriting this time as OpenCPU takes care of itself on &lt;code&gt;/ocpu/&lt;/code&gt; folder.&lt;/p&gt;

&lt;h3 id=&#34;use-your-opencpu-installation&#34;&gt;Use your OpenCPU installation&lt;/h3&gt;

&lt;p&gt;Assuming your IP is &lt;code&gt;1.2.3.4&lt;/code&gt;; we can see the OpenCPU is installed via &lt;code&gt;1.2.3.4/ocpu/info&lt;/code&gt; and the test page at &lt;code&gt;1.2.3.4/ocpu/test/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For our example, the custom package uploaded is available at &lt;code&gt;1.2.3.4/ocpu/library/predictClickOpenCPU&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/open-cpu-running.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We use the API for this package by creating a POST hit to API endpoint&lt;/p&gt;

&lt;p&gt;&lt;code&gt;1.2.3.4/ocpu/library/predictClickOpenCPU/R/predictMarkov/json&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;with the parameter &lt;code&gt;pageview_names=[&amp;quot;/example/96&amp;quot;,&amp;quot;/example/213&amp;quot;,&amp;quot;/example/107&amp;quot;]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Constructing the above in the terminal with &lt;code&gt;curl&lt;/code&gt;, we test the API is functioning:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl --data &#39;pageview_names=[&amp;quot;/example/96&amp;quot;,&amp;quot;/example/213&amp;quot;,&amp;quot;/example/107&amp;quot;]&#39; http://35.233.42.6/ocpu/library/predictClickOpenCPU/R/predictMarkov/json
#&amp;gt; {
#  &amp;quot;page&amp;quot;: [&amp;quot;/example/251&amp;quot;],
#  &amp;quot;probability&amp;quot;: [0.5657]
#&amp;gt; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Web API endpoints can also be used as webhooks, for instance to react and do something when something is pushed to GitHub for continuous integration purposes.&lt;/p&gt;

&lt;h3 id=&#34;c-scheduled-r-tasks-via-cronjobs&#34;&gt;C: Scheduled R - Tasks via cronJobs&lt;/h3&gt;

&lt;p&gt;Kuberenetes has a special option for deploying scheduled tasks, called cronJobs.&lt;/p&gt;

&lt;p&gt;When configuring your Dockerfile, you can choose so it runs entirely self-contained, perhaps calling an API and writing the result to a database.  For each new job you can make a new Dockerfile and build them on something like build triggers.&lt;/p&gt;

&lt;p&gt;In that case the Dockerfile should run with all configuration files included in the Docker build.  One advantage of running on GKE is that the job can be authenticated with the same project via &lt;a href=&#34;http://code.markedmondson.me/googleAuthR/reference/gar_gce_auth.html&#34;&gt;&lt;code&gt;googleAuthR::gar_gce_auth()&lt;/code&gt;&lt;/a&gt; so will have access to Google Cloud Storage, BigQuery etc.&lt;/p&gt;

&lt;p&gt;An alternative is to configure the Dockerfile to rely on a config file, and have that config file mounted as a volume in the pod - that isn&amp;rsquo;t covered here yet.&lt;/p&gt;

&lt;h4 id=&#34;creating-an-executable-r-dockerfile&#34;&gt;Creating an executable R Dockerfile&lt;/h4&gt;

&lt;p&gt;This is an example Dockerfile that installs dependencies and R packages, loads a local configuration file then runs a custom function when it is called upon by the cronJob:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM rocker/verse
MAINTAINER Mark Edmondson (r@sunholo.com)

# install R package dependencies
RUN apt-get update &amp;amp;&amp;amp; apt-get install -y \
    gcc gsl-bin libblas-dev \
    ## etc....
    ## clean up
    &amp;amp;&amp;amp; apt-get clean \ 
    &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/ \ 
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds

# any necessary environment arguments
ENV SECRET_ARG hellomum

# Install packages from CRAN and GitHub
RUN install2.r --error \ 
    -r &#39;http://cran.rstudio.com&#39; \
    googleAuthR googleAnalyticsR bigQueryR \
    &amp;amp;&amp;amp; Rscript -e &amp;quot;devtools::install_github(&#39;MarkEdmondson1234/youtubeAnalyticsR&#39;)&amp;quot; \
    &amp;amp;&amp;amp; Rscript -e &amp;quot;devtools::install_github(&#39;yourprivate/customPackage&#39;)&amp;quot; \
    ## clean up
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds

# my_config.yaml is in same folder as Dockerfile
COPY my_config.yaml my_config.yaml

## this runs when this Docker container is called
CMD [&amp;quot;Rscript&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;customPackage::do_something(&#39;my_config.yaml&#39;)&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;running-cronjobs&#34;&gt;Running cronJobs&lt;/h4&gt;

&lt;p&gt;For &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/&#34;&gt;cronJobs&lt;/a&gt;, specify the &lt;code&gt;schedule&lt;/code&gt; argument, and we change the restart policy to only do so on failure.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl run scheduled-r --schedule=&amp;quot;23 4 * * *&amp;quot; --restart=OnFailure --image=gcr.io/xxxxxx/private_docker_image:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That is getting a bit complicated for the terminal , so I prefer to deploy via the file method:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: scheduledr
spec:
  schedule: &amp;quot;23 4 * * *&amp;quot;
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: my_image
            image: gcr.io/xxxxxx/private_docker_image:latest
          restartPolicy: OnFailure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save the file as say &lt;code&gt;cron-r.yaml&lt;/code&gt; and deploy via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl create -f ./cron-r.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should then see the job has been created via&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get cronjob scheduledr
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And in the Web UI under &lt;a href=&#34;https://console.cloud.google.com/kubernetes/workload?project=iih-tools-analytics&amp;amp;authuser=0&amp;amp;workload_list_tablesize=50&#34;&gt;Workloads&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/cronJob.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;part-4-going-further&#34;&gt;Part 4 - Going further&lt;/h2&gt;

&lt;p&gt;This article turned into a lot more words than I intended, so I&amp;rsquo;ll stop now, but see below on some more things I could have written about:&lt;/p&gt;

&lt;h3 id=&#34;updating-your-cluster&#34;&gt;Updating your cluster&lt;/h3&gt;

&lt;p&gt;Its a bit of configuration to get there, but once deployed since the services are running off Docker, and those in turn are built using build triggers that are from GitHub pushes, it means that changes to scripts can all be controlled via GitHub pushes and branches, which you are probably using anyway for version control.&lt;/p&gt;

&lt;p&gt;When you change a configuration file and want to deploy, use &lt;code&gt;kubectl apply&lt;/code&gt; instead of &lt;code&gt;kubectl create&lt;/code&gt; to change the settings.&lt;/p&gt;

&lt;p&gt;Its best practice to specify a dev and production branch to your GitHub Docker images, so you can safely test containers before you break your production.&lt;/p&gt;

&lt;h3 id=&#34;secrets-and-https-ssl-configuration&#34;&gt;Secrets and https / SSL configuration&lt;/h3&gt;

&lt;p&gt;Kubernetes supports more robust methods to guard your secrets that including them in the Dockerfiles - see &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables&#34;&gt;here&lt;/a&gt; for proper treatment.&lt;/p&gt;

&lt;p&gt;Related to that is serving over https.  In this case I found Bitnami the easiest method to follow, using Helm again to &lt;a href=&#34;https://docs.bitnami.com/kubernetes/how-to/secure-kubernetes-services-with-ingress-tls-letsencrypt/&#34;&gt;configure TLS with LetsEncrypt and Kube-lego&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;auto-scaling&#34;&gt;Auto scaling&lt;/h3&gt;

&lt;p&gt;Launching the cluster with these flags enables auto-scaling&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud container clusters create r-cluster --num-nodes=0 --enable-autoscaling --min-nodes=0 --max-nodes=3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On Google Kubernetes Engine this scales down to 0, which should be cheaper than running a few CPUs running &lt;sup&gt;24&lt;/sup&gt;&amp;frasl;&lt;sub&gt;7&lt;/sub&gt;, and you can configure when new nodes are launched such as when it hits 80% CPU.&lt;/p&gt;

&lt;h3 id=&#34;parallel-tasks&#34;&gt;Parallel tasks&lt;/h3&gt;

&lt;p&gt;For big parallel processing tasks, Kubernetes is an option that can launch as many CPUs you need.&lt;/p&gt;

&lt;p&gt;However, you will need to configure your workload so that each service works on a different part of the data, so some form of central ledger is needed that keeps track of which rows on the database or files have been treated, and to remove them from the input files once completed.&lt;/p&gt;

&lt;p&gt;A strategy to handle this is to use message brokers that specify which work needs to be done. An example of using a &lt;a href=&#34;https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/&#34;&gt;message broker is here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;and-finally&#34;&gt;And finally&amp;hellip;&lt;/h3&gt;

&lt;p&gt;After all that, do remember to tear down your cluster and/or load balancers when finished with, as you are charged by minute of uptime per node.  Most tutorials include this at the end - you can either delete in the web UI but to be complete here:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Delete the Ingress: This deallocates the ephemeral external IP address and the load balancing resources associated to your application:&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl delete ingress your-ingress
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Delete the cluster: This deletes the compute nodes of your container cluster and other resources such as the Deployments in the cluster:&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud container clusters delete r-cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;When working with Kubernetes documentation you can tell that it is a fast changing developing product, which can be a bit frustrating when looking for solutions, but I anticipate the APIs to settle down in the future.&lt;/p&gt;

&lt;p&gt;This should hopefully serve as a good jumping off point for you to develop your own Kubernetes R cluster.  Having got Kubernetes under my belt I now feel fully tooled up for deploying R (and other applications) at scale, and in an exciting and developing framework that should stand in good sted for a few years to come.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Embedding Google Data Studio into RMarkdown</title>
      <link>https://code.markedmondson.me/embedding-google-data-studio-into-rmarkdown</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://code.markedmondson.me/embedding-google-data-studio-into-rmarkdown</guid>
      <description>&lt;div id=&#34;adding-google-data-studio-dashboards-to-rmarkdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding Google Data Studio Dashboards to RMarkdown&lt;/h2&gt;
&lt;p&gt;This week I learnt you can take &lt;a href=&#34;https://datastudio.google.com&#34;&gt;Google Data Studio&lt;/a&gt; reports and, via their embed feature, paste them straight into an RMarkdown HTML document.&lt;/p&gt;
&lt;p&gt;This is cool as you can then combine the strengths of both for client reports or similar.&lt;/p&gt;
&lt;p&gt;Data Studio is a free online solution, that I have found to be very quick to get something decent looking. It also has excellent connectors to data sources that can be difficult to get access to otherwise, such as DoubleClick. I often use it when working with BigQuery by linking via its BigQuery connector (takes 10 seconds) and then plotting to examine the data trends.&lt;/p&gt;
&lt;p&gt;However, Data Studio isn’t suitable for more advanced analysis or custom visualisations, so the fact you can put them in RMarkdown documents means you can then combine its reports alongside more in-depth analysis within R, side by side with advanced R plotting such as &lt;a href=&#34;http://ggplot2.org/&#34;&gt;&lt;code&gt;ggplot2&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;http://gallery.htmlwidgets.org/&#34;&gt;&lt;code&gt;htmlwidgets&lt;/code&gt;&lt;/a&gt; or &lt;a href=&#34;https://plot.ly/r/&#34;&gt;&lt;code&gt;plot.ly&lt;/code&gt;&lt;/a&gt;. RMarkdown excels as a way to document your analysis steps, and I often use it as a way to keep a working document I can check into GitHub that contains all the SQL, code and assumptions of an analysis.&lt;/p&gt;
&lt;div id=&#34;how-to-add-data-studio-to-rmarkdown&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to add Data Studio to RMarkdown&lt;/h3&gt;
&lt;p&gt;First create your Data Studio report, then select the &lt;a href=&#34;https://support.google.com/datastudio/answer/7450249?hl=en&#34;&gt;“Embed” option&lt;/a&gt; top right of Google Data Studio:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../images/embed.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Then you can paste in that code straight in an &lt;code&gt;.Rmd&lt;/code&gt; document, like I did with this blog (&lt;a href=&#34;http://code.markedmondson.me/new-blog-down/&#34;&gt;which runs on RMarkdown via Hugo&lt;/a&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;iframe width=&amp;quot;600&amp;quot; height=&amp;quot;500&amp;quot; 
src=&amp;quot;https://datastudio.google.com/embed/reporting/1q8dO3-IuGU-YXbgTiW6mchvKCg_kRZ9Y/page/wN5O&amp;quot; 
frameborder=&amp;quot;0&amp;quot; 
style=&amp;quot;border:0&amp;quot; allowfullscreen&amp;gt;&amp;lt;/iframe&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Above I put in backticks to demo the code I pasted in, but you don’t need to do that - below is result of the code pasted in as is into the .Rmd doc&lt;/em&gt;&lt;/p&gt;
&lt;iframe width=&#34;600&#34; height=&#34;500&#34; src=&#34;https://datastudio.google.com/embed/reporting/1q8dO3-IuGU-YXbgTiW6mchvKCg_kRZ9Y/page/wN5O&#34; frameborder=&#34;0&#34; style=&#34;border:0&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;Tweak the width and height so it fits your layout - I set the height and width of the Data Studio dashboard to the same as the iframe to avoid the scroll bars appearing.&lt;/p&gt;
&lt;p&gt;For standalone &lt;code&gt;.Rmd&lt;/code&gt; documents, 800 pixel width looks best, but this blog is narrower so I went for 600 pixel width.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;private-data-studio-reports&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Private Data Studio reports&lt;/h2&gt;
&lt;p&gt;The above report is shared publically on the web, but you can do the same with private Data Studio reports.&lt;/p&gt;
&lt;p&gt;Follow the same procedure, but only share privately in your Sharing settings - only people who have private access to this report will see it, others will get the &lt;code&gt;This Data Studio report is private&lt;/code&gt; message.&lt;/p&gt;
&lt;p&gt;e.g. You see this:&lt;/p&gt;
&lt;iframe width=&#34;600&#34; height=&#34;500&#34; src=&#34;https://datastudio.google.com/embed/reporting/1O6QttLeXrV7zesk0lNFeWmb5H3aPIhMa/page/wN5O&#34; frameborder=&#34;0&#34; style=&#34;border:0&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;…but I see this:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../images/private-embed.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The authentication works in the same browser you are logged in with Google, since that is shared with a Google login cookie - e.g. if you are logged into Google Analytics in the same browser, you should see it. This means however you won’t see it work if viewing in the RStudio View panel.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;If you are in the R world and maybe have not come across Google Data Studio, this may be helpful as another option for visualisation. Its main strengths are that it is free, is quick to get to visualisations, has powerful filter controls and lots of nice API connectors (including &lt;a href=&#34;https://developers.google.com/datastudio/connector/gallery/&#34;&gt;user made community connectors&lt;/a&gt; that connect to stuff like Facebook, Bing, LinkedIn etc.) although if you want to do more complex custom visuals you will find it not as fully tweakable as a paid for solution such as Tableau or Qlikview.&lt;/p&gt;
&lt;p&gt;If you are in the Digital Marketing world you probably already know Data Studio, but perhaps knowing you have the ability to embed Data Studio into RMarkdown provides you with a quick way to combine existing reports with the more advanced data analysis and visuals that R can provide.&lt;/p&gt;
&lt;p&gt;There is to my knowledge no way to connect the two (e.g. change a filter in Data Studio and it affects the R visuals) but that would be cool, if anyone figures out a way to do that let me know!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Comparing Google Search Console queries with Google&#39;s Cloud Natural Language API</title>
      <link>https://code.markedmondson.me/searchconsoler-vs-googlelanguager</link>
      <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://code.markedmondson.me/searchconsoler-vs-googlelanguager</guid>
      <description>&lt;p&gt;With the launch of the Google Natural Language API (NLP API), and the emphasis of machine learning that is said to account for up to 30% of the SEO algorithmn for Google search, a natural question is whether you can use Google’s own macine learning APIs to help optimise your website for search.&lt;/p&gt;
&lt;p&gt;Whilst I don’t believe they will offer exactly the same results, I can see useful applications that include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Identifying what entities are on your website, to see what topics Google Knowledge Graph may categorise your website as&lt;/li&gt;
&lt;li&gt;Running your development website through the API to see if the topics are what you expect your SEO to cover&lt;/li&gt;
&lt;li&gt;Identify content that has very similar topics, that may be competing with one another in search&lt;/li&gt;
&lt;li&gt;Auto-optimisation of content by altering content to desired query targets&lt;/li&gt;
&lt;li&gt;Competitor analysis of SEO website performance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both these data sources are available through R via &lt;code&gt;searchConsoleR&lt;/code&gt; and &lt;code&gt;googleLanguageR&lt;/code&gt;, so below is a workflow on using them together to help answer questions like above.&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;For this proof of concept we will use search console API and the NLP API to generate keywords for the same URLs, then compare the results.&lt;/p&gt;
&lt;p&gt;The general outline is:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Get Search Console data for a website&lt;/li&gt;
&lt;li&gt;For each SEO landing page, generate a corpus of NLP results&lt;/li&gt;
&lt;li&gt;Look for evidence that there is a relationship to a high agreement between NLP and SEO rankings&lt;/li&gt;
&lt;li&gt;Identify optimisation opportunities with suggested topics.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;As we’re authenticating with two &lt;code&gt;googleAuthR&lt;/code&gt; libraries, we set the scopes and authenticate directly with &lt;code&gt;googleAuthR::gar_auth()&lt;/code&gt; rather than authenticate seperatly. The NLP API requires you to set up your own Google Cloud project, so that projects client ID, secret etc are used to generate one authentication token that covers both. See the &lt;a href=&#34;http://code.markedmondson.me/googleAuthR/articles/google-authentication-types.html#multiple-authentication-tokens&#34;&gt;googleAuthR website&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;The libraries used are also below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleAuthR)     # Authentication
library(googleLanguageR) # Google NLP API
library(searchConsoleR)  # Webmasters API
library(tidyverse)       # Data processing
library(rvest)           # URL scraping
library(cld2)            # Offline language detection

# set google project to your own
# assumes you have downloaded your own client ID JSON and set in environment argument GAR_CLIENT_JSON
gar_set_client(scopes = c(&amp;quot;https://www.googleapis.com/auth/webmasters&amp;quot;,
                          &amp;quot;https://www.googleapis.com/auth/cloud-platform&amp;quot;))

# creates an auth token for reuse called &amp;quot;scgl.oauth&amp;quot; that works with search console and Language API
gar_auth(&amp;quot;scgl.oauth&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;gather-search-console-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gather Search Console data&lt;/h2&gt;
&lt;p&gt;For this we need the keywords and each SEO landing page that appeared in the Google results, so the dimensions &lt;code&gt;query&lt;/code&gt; and &lt;code&gt;page&lt;/code&gt; are required:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(searchConsoleR)

test_website &amp;lt;- &amp;quot;https://www.example.co.uk&amp;quot;
sc &amp;lt;- search_analytics(test_website, dimensions = c(&amp;quot;query&amp;quot;,&amp;quot;page&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The NLP API supports &lt;a href=&#34;https://cloud.google.com/natural-language/docs/languages&#34;&gt;these 10 languages&lt;/a&gt;, so the queries’ language also need to be on that list.&lt;/p&gt;
&lt;p&gt;For this example, we only use English keywords so we can limit to just those by detecting the query keywords.&lt;/p&gt;
&lt;p&gt;I suggest using the &lt;a href=&#34;https://CRAN.R-project.org/package=cld2&#34;&gt;&lt;code&gt;cld2&lt;/code&gt; library&lt;/a&gt;. I use this offline library for language detection as its free and fine for quick processing, whilst if more heavy duty detection and translation needed, then I would use &lt;code&gt;gl_translate()&lt;/code&gt; from &lt;code&gt;googleLanguageR&lt;/code&gt; although that has a cost.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(cld2)

sc$language &amp;lt;- detect_language(sc$query)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keeping the languages that are &lt;code&gt;en&lt;/code&gt; or &lt;code&gt;NA&lt;/code&gt; (we couldn’t tell)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sc_eng &amp;lt;- sc %&amp;gt;% filter(is.na(language) | language == &amp;quot;en&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a clean dataset to send to the NLP API.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gather-nlp-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gather NLP data&lt;/h2&gt;
&lt;p&gt;To avoid running NLP on lots of unnecessary HTML boiler plate, we need to consider what data we want to send in.&lt;/p&gt;
&lt;p&gt;For example, as we’re interested in SEO, the relevant data will include the title tags and the body content. Limiting the data sent to those will mean we have cleaner data out.&lt;/p&gt;
&lt;p&gt;There are two approaches to this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Examine the HTML or the website to only extract the useful bits using &lt;code&gt;rvest&lt;/code&gt;’s CSS selectors (needs bespoke programming for each website)&lt;/li&gt;
&lt;li&gt;Take the text only cache from Google as the text source instead (cleaner data, but you’ll be blocked by Google if you scrape to many URLs)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For this example, we take the latter method, but if you are running this beyond proof of concept scale I would advise the first.&lt;/p&gt;
&lt;p&gt;To take the “Text Only” cached version of a page, we use the URL that is supplied by Google Search’s cache:&lt;/p&gt;
&lt;p&gt;e.g.&lt;/p&gt;
&lt;p&gt;If your URL is - &lt;code&gt;http://example.com/your-website&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;…then the text only webcache will be: &lt;code&gt;http://webcache.googleusercontent.com/search?q=cache:example.com/your-website&amp;amp;num=1&amp;amp;strip=1&amp;amp;vwsrc=0#/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;An example function that does the above is shown below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
library(tidyverse)

## create a function for scraping this website
scrape &amp;lt;- function(the_url){
  message(&amp;quot;Scraping &amp;quot;, the_url)
  
  read &amp;lt;- read_html(the_url)
  
  Sys.sleep(5)   # be nice to website
  
  read %&amp;gt;%
    html_text() %&amp;gt;% 
    trimws()
}

scrape_googlecache &amp;lt;- function(the_url){
  
  wc &amp;lt;- gsub(&amp;quot;https?://&amp;quot;,&amp;quot;&amp;quot;, the_url)
  cache_me &amp;lt;- sprintf(
    &amp;quot;http://webcache.googleusercontent.com/search?q=cache:%s&amp;amp;num=1&amp;amp;strip=1&amp;amp;vwsrc=0#/&amp;quot;,
    wc
  )
  
  scraped &amp;lt;- scrape(cache_me)
  
  # remove whitespace and double whitespace
  out &amp;lt;- gsub(&amp;quot;(\r|\n|\t)&amp;quot;,&amp;quot; &amp;quot;, scraped)
  out &amp;lt;- gsub(&amp;quot;\\s\\s+&amp;quot;,&amp;quot; &amp;quot;, out)
  
  # remove first 1000 characters of boilerplate
  out &amp;lt;- substr(out, 1000, nchar(out))

  data.frame(cached = out, url = the_url, stringsAsFactors = FALSE)

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a demo of how it works on one URL:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;the_url &amp;lt;- &amp;quot;https://code.markedmondson.me&amp;quot;

html_url &amp;lt;- scrape_googlecache(the_url)

substr(html_url$cached, 1000, 2000)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can now apply this function to all the unique URLs in your search console URLs using a purrr loop, but hit it too fast and you’ll be blocked.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

all_urls_scrape &amp;lt;- sc_eng %&amp;gt;% 
  distinct(page) %&amp;gt;% 
  select(page) %&amp;gt;% 
  map_df(scrape_googlecache)
  &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;do-the-nlp-api&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Do the NLP API&lt;/h3&gt;
&lt;p&gt;Now we can get results from Google NLP - test this out first with a couple of URLs as it costs money for each API call.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(googleLanguageR)

nlp_results &amp;lt;- all_urls_scrape %&amp;gt;% 
  select(cached) %&amp;gt;% 
  map(gl_nlp) # this line makes the API calls&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The NLP API returns lots of data, for now we are interested in the entities section:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
entities &amp;lt;- map(nlp_results, &amp;quot;entities&amp;quot;)

## only get the types that are not &amp;quot;OTHER&amp;quot;
types &amp;lt;- map(entities, function(x){
  y &amp;lt;- x[[1]]
  y &amp;lt;- y[y$type != &amp;quot;OTHER&amp;quot;,]
  y &amp;lt;- y[!is.na(y$type),]
  y
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a list of the NLP results for each URL, for all the URLs that were in the search console data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nlp-features&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;NLP features&lt;/h3&gt;
&lt;p&gt;You can see the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/nlp.html&#34;&gt;googleLanguageR website&lt;/a&gt; for more details on what is returned, as one example here is what recognised entities the API returned that have Wikipedia links:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

## only get entries that have wikipedia links
types[[1]] %&amp;gt;% 
  filter(!is.na(wikipedia_url)) %&amp;gt;% 
  distinct(name, wikipedia_url) %&amp;gt;% 
  head %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;name&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;wikipedia_url&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Afghanistan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Afghanistan&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Afghanistan&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;British&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/United_Kingdom&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/United_Kingdom&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Charing Cross&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Charing&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Charing&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;England&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/England&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/England&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Epilepsy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Epilepsy&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Epilepsy&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Euro&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Euro&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Euro&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison-between-search-console-and-nlp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison between Search Console and NLP&lt;/h2&gt;
&lt;p&gt;We can now compare the URL entities in &lt;code&gt;types&lt;/code&gt; and the Search Console data in &lt;code&gt;sc_eng&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;For demo purposes, we only look at the home page for the example website, but you can repeat this by looping over the list of &lt;code&gt;types&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;homepage_types &amp;lt;- types[[1]]
homepage_search_console &amp;lt;- sc_eng %&amp;gt;% filter(page = &amp;quot;https://www.example.co.uk/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;data-processing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data processing&lt;/h3&gt;
&lt;p&gt;We first get the NLP entity names and the search console queries into the same format, lowercase and deduplicated, and for the NLP results the &lt;code&gt;salience&lt;/code&gt; score is how important it thinks that entity is to the page, so we’ll sort downwards from that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
page_types &amp;lt;- page_types %&amp;gt;% 
  distinct(name, .keep_all = TRUE) %&amp;gt;% 
  mutate(name = tolower(name)) %&amp;gt;% 
  arrange(desc(salience))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the vector of entity names in &lt;code&gt;page_types$name&lt;/code&gt; and the vector of search console queries in &lt;code&gt;sc_eng$query&lt;/code&gt; are the two vectors of keywords we want to compare. Are they similar?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;similarity-of-keywords&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Similarity of keywords&lt;/h3&gt;
&lt;p&gt;In my example case I have 10 distinct queries from search console, and 464 unique named entites extracted out of the HTML from the NLP API, sorted by salience.&lt;/p&gt;
&lt;p&gt;A simple way of matching strings in R is using its base function &lt;code&gt;agrep()&lt;/code&gt; which is an implementation of Levenshtein distance AKA “fuzzy matching”. I’ll run all the queries and compare against the entities:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
## named vector
queries &amp;lt;- page_search_console$query %&amp;gt;% unique()
queries &amp;lt;- setNames(queries, queries) 

fuzzy_ranks &amp;lt;- queries %&amp;gt;% 
  map(~ agrep(., page_types$name, value = FALSE)) %&amp;gt;% 
  enframe(name = &amp;quot;query&amp;quot;, value = &amp;quot;nlp_rank&amp;quot;) %&amp;gt;% 
  mutate(nlp_hits = map_int(nlp_rank, length)) %&amp;gt;% 
  filter(nlp_hits &amp;gt; 0)

fuzzy_values &amp;lt;- queries %&amp;gt;% 
  map(~ agrep(., page_types$name, value = TRUE)) %&amp;gt;% 
  enframe(name = &amp;quot;query&amp;quot;, value = &amp;quot;nlp_value&amp;quot;)

fuzzy &amp;lt;- left_join(fuzzy_ranks, fuzzy_values, by = &amp;quot;query&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since its a one-to-many relationship (one query can match several entities on the page), a list-column created by &lt;code&gt;enframe()&lt;/code&gt; keeps it all neat and tidy within the tidyverse style.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nlp_rank&lt;/code&gt; is a vector of rankings of the salience of the words&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nlp_hits&lt;/code&gt; is a numeric of how many entities each query had&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nlp_value&lt;/code&gt; is a vector of what entities are matched&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ve had to change a few values to protect client data, but here is an example of the output:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;query&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;nlp_rank&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nlp_hits&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;nlp_value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;non uk resident handbag cleaning&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;non uk residents handbag cleaning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;non uk residents handbag cleaning&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;non uk residents handbag cleaning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;eu resident handbag cleaning&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;non uk residents handbag cleaning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;eu residents handbag cleaning&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;non uk residents handbag cleaning&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;does-the-nlp-help-with-any-insight&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Does the NLP help with any insight?&lt;/h2&gt;
&lt;p&gt;A few take aways from the above in the real case, which was an e-commerce page targeting customers in Europe was:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The NLP ranked the salience of the top keyword referrer as 8 - meaning it had 7 more entities it thought the page was more relevant to the page. This may mean we should work more on stressing the subject we want to be recognised in Google for.&lt;/li&gt;
&lt;li&gt;One of these was “British” which didn’t appear on the page which only mentioned “UK”, so it looks like it adds synonyms as well into its analysis.&lt;/li&gt;
&lt;li&gt;The NLP ranked “customers” as rank 3, even though it was mentioned only two times on the page. It got that this was an ecommerce page. Likewise it identified “European Econmic area” as important, even though it was only mentioned once. Since this was a sales page aimed at EEC users, this looks fair.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Its early days but I am starting to run through new client websites to see what comes up (by writing more custom code to parse out the content) to pass on to the SEO specialists, at the very least its another perspective to inform topic and keyword choices. If anyone takes this approach and finds it useful, do please comment as I’ll be interested if this helps. It would also be good to compare this with other non-Google NLP APIs such as on Azure and AWS to see if the Google one is especially useful for Google SEO.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Run RStudio Server on a Chromebook as a Cloud Native</title>
      <link>https://code.markedmondson.me/rstudio-server-chromebook/</link>
      <pubDate>Tue, 05 Sep 2017 13:55:57 +0100</pubDate>
      
      <guid>https://code.markedmondson.me/rstudio-server-chromebook/</guid>
      <description>

&lt;p&gt;I recently got an &lt;a href=&#34;https://www.asus.com/us/Laptops/ASUS-Chromebook-Flip-C302CA/&#34;&gt;Asus Chromebook Flip&lt;/a&gt; with which I&amp;rsquo;m very happy, but it did make me realise that if a Chromebook was to replace my normal desktop as my primary workstation, my RStudio Server setup would need to be more cloud native than was available up until now.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;TL;DR - A how-to on making RStudio Server run on a Chromebook that automatically backs up data and configuration settings to Google Cloud Storage is on the &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/persistent-rstudio.html&#34;&gt;googleComputeEngineR website here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;broken-promises-of-the-cloud&#34;&gt;Broken promises of the cloud?&lt;/h2&gt;

&lt;p&gt;Whilst there are lots of other solutions around for hosting RStudio Server in the cloud, &lt;a href=&#34;http://code.markedmondson.me/setting-up-scheduled-R-scripts-for-an-analytics-team/&#34;&gt;including some of my own&lt;/a&gt;, I don&amp;rsquo;t believe they are a serious replacement for a desktop station running RStudio.&lt;/p&gt;

&lt;p&gt;To date I&amp;rsquo;ve treated RStudio Server as a temporary UI whilst setting up scheduled scripts and so forth, and for running workshops for students with pre-installed packages.  Yes, the convenience of running RStudio in a browser and being able to launch different configurations is great, but it wasn&amp;rsquo;t a total desktop replacement for the following reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;More expensive&lt;/em&gt; - running RStudio Server permanently in the cloud can be done for as little as 5 dollars a month, but for that you get a machine about as powerful as a Raspberry Pi.  For a proper workstation with decent RAM etc, you are looking at more like 30 dollars a month, which if you&amp;rsquo;re running for 2 years is around $700 that you could have spent on a laptop that can do more things.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;More risky&lt;/em&gt; - keeping the only copy of your data on a cloud server is not a good idea, as I learnt to my cost when a VM&amp;rsquo;s hard disk filled up. Unable to log in, a convoluted panic to increase the disk size occurred.  A physical laptop is a lot easier to troubleshoot if something goes awry.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Not flexible enough&lt;/em&gt; - if you run out of RAM or want things to compute quicker, you are going to need to transfer your data and boot up another VM with stronger specs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But isn&amp;rsquo;t the allure of cheaper, safer, more flexible computing the reason we are using the cloud in the first place?&lt;/p&gt;

&lt;h2 id=&#34;going-cloud-native&#34;&gt;Going Cloud Native&lt;/h2&gt;

&lt;p&gt;I believe the reason for the dichotomy between expectation and reality is due to not being &amp;ldquo;Cloud Native&amp;rdquo; enough, something I realised whilst attending this year&amp;rsquo;s &lt;a href=&#34;https://cloudnext.withgoogle.com/&#34;&gt;Google NEXT event&lt;/a&gt;.  There they described three phases of business cloud evolution:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Lift and Shift - moving your existing infrastructure into equivalent cloud versions&lt;/li&gt;
&lt;li&gt;Separation of compute and data - abstracting out the computing away from the data the computing is running on.  Essentially letting the cloud take on the roles of your hard disk vs your CPU.&lt;/li&gt;
&lt;li&gt;Platform-as-a-service (PaaS) - abstracting away the servers the computing and data are running on, so as to run on purely managed services such as BigQuery, Kubernetes and AppEngine&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;From the above, existing RStudio Server solutions are in the Lift and Shift vein, whilst what we may imagine to be The Cloud are in the more evolved options.&lt;/p&gt;

&lt;h2 id=&#34;docker-ftw&#34;&gt;Docker FTW&lt;/h2&gt;

&lt;p&gt;To truly take advantage of the cloud, I believe this means you must become familiar with containers, specifically &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt;.  Containers are a rapidly evolving standard that makes cloud evolution possible by allowing breakout of computation, and is being embraced by all cloud providers.&lt;/p&gt;

&lt;p&gt;Using Docker with R has other benefits.  Reproducibility of R code is enhanced when you can pin the exact environment to run code within, and addresses criticisms of R that it is not production ready: normally R&amp;rsquo;s open source and rapidly evolving nature means R code you wrote two years ago may not work with the most modern R package versions.&lt;/p&gt;

&lt;p&gt;I am in a lucky position to work on this as I have developed a niche creating Google API libraries in R.  Starting with my needs from digital marketing to wrap the Search Console and Google Analytics APIs, I then used that experience to move into the more general Cloud APIs such as Cloud Storage and Compute Engine, and now get support through my Google Developer Expert status network to experiment with the Google Cloud platform.&lt;/p&gt;

&lt;h2 id=&#34;a-persistent-rstudio-server&#34;&gt;A Persistent RStudio Server&lt;/h2&gt;

&lt;p&gt;Its with two of my R libraries, &lt;a href=&#34;http://code.markedmondson.me/googleCloudStorageR/&#34;&gt;googleCloudStorageR&lt;/a&gt; and &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/&#34;&gt;googleComputeEngineR&lt;/a&gt;, that I&amp;rsquo;ve put together something much closer to the cheap, resilient, and flexible version of the cloud I want to be using when running RStudio in the cloud.&lt;/p&gt;

&lt;p&gt;The role of a harddrive is delegated to Google Cloud Storage, whilst RStudio is served from within Docker containers.  With some new functions that are in the &lt;code&gt;.Rprofile&lt;/code&gt; of a custom RStudio Docker image, Google Cloud Storage is called to download on startup, or upload on exit, all the files to a specific bucket.  These files can include SSH and GitHub settings, or a project folder.  Backups are activated by putting a &lt;code&gt;_gcssave.yaml&lt;/code&gt; file in a folder, or via the VM&amp;rsquo;s metadata.&lt;/p&gt;

&lt;p&gt;What this all means is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RStudio Server runs within its own Docker container, and can be relaunched with custom package setups&lt;/li&gt;
&lt;li&gt;Data is persistent between Docker containers and cloud compute instances.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;I can turn off the RStudio Server to avoid charges, then turn it on again and start from where I left off without reconfiguring git etc.&lt;/li&gt;
&lt;li&gt;I can work locally in one RStudio project, then switch to the cloud to work on the same project.&lt;/li&gt;
&lt;li&gt;SSH keys and GitHub configurations are set up only once and then automatically available across Docker containers, cloud computers and local RStudio projects.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have a bookmark to the Google Cloud console to startup/shutdown the instance:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/startup-rstudio.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Once launched, I log in and configurations are auto loaded by the &lt;code&gt;_gcssave.yaml&lt;/code&gt; configuration:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/startup-rstudio-persistent.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As this includes the home directory, so long as I login with the same username, and point to the same bucket, any RStudio launched (within a Docker/locally, whereever) don&amp;rsquo;t need to reconfigure Git - downloading a GitHub repo is as simple as copying the SSH GitHub URL&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/ssh-clone-github.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;and starting a new RStudio project:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/rstudio-github.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This saves me significant cost by letting me stop/start servers as and when I need them via the GCP Web UI. My data is safer than keeping it on my local harddrive, and switching to a bigger VM takes seconds since all data and code upload/download automatically.&lt;/p&gt;

&lt;p&gt;The details for the configuration setup is &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/persistent-rstudio.html&#34;&gt;here on the googleComputeEngineR website&lt;/a&gt;, for which you&amp;rsquo;ll need the latest development versions of &lt;code&gt;googleComputeEngineR&lt;/code&gt; and &lt;code&gt;googleCloudStorageR&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;rstudio-as-a-service&#34;&gt;RStudio-as-a-service?&lt;/h2&gt;

&lt;p&gt;Whats next?  Could this evolve further into a RStudio-as-a-service offering?  To qualify, we would need to not worry about starting or stopping servers at all, and scale under any load.&lt;/p&gt;

&lt;p&gt;Well, prompted by this blog post I had a good look.  It IS possible to deploy RStudio on App Engine and I got excited, but unfortunately the minimum number of permanent instances on a flexible App Engine app is 1, so all told for a solo analyst it is a more expensive solution than running a VM that you stop and start yourself.  However, if you had enough demand to pay for 1 VM 24 hours a day (~$30 a month),  it does start to make sense to scale on App Engine.  A setup that does that is on this GitHub repo &lt;a href=&#34;https://github.com/MarkEdmondson1234/appengine-rstudio&#34;&gt;running RStudio on App Engine&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/appengine-rstudio.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Using R on App Engine is possible due to App Engine&amp;rsquo;s new beta support for &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/&#34;&gt;flexible custom runtime containers&lt;/a&gt;.  Any language deployed through a Docker container will work, something I recently worked through in a proof of concept to deploy a &lt;a href=&#34;https://github.com/MarkEdmondson1234/serverless-R-API-appengine&#34;&gt;serverless R API using Plumber&lt;/a&gt;.  Shiny may be another beneficiary of App Engine, although at time of writing App Engine doesn&amp;rsquo;t support the necessary websockets.&lt;/p&gt;

&lt;p&gt;I suspect using &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt;, a container orchestrator upon Google Container Engine the above could be achieved, but I haven&amp;rsquo;t quite grok&amp;rsquo;d that yet so perhaps I&amp;rsquo;ll update this in the future.&lt;/p&gt;

&lt;p&gt;You can still benefit from PaaS within R if you are using services such as BigQuery.  There, analysis of large datasets is performed without you knowing how many shards of instances are being used to do so, and you can access the results within R via bigrquery/bigQueryR.  I did a presentation of &lt;a href=&#34;https://docs.google.com/presentation/d/1510xJzDuWgbLgoNY3Fs5-CGtMCJEYs5msaxIpINt03g/edit?usp=sharing&#34;&gt;BigQuery&amp;rsquo;s usefulness for analytics here&lt;/a&gt; for MeasureCamp Copenhagen.&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Of course, all this still doesn&amp;rsquo;t work if you do not have an internet connection. :)  But I&amp;rsquo;m now much more comfortable using my Chromebook to start and maintain R projects, and could imagine it being my main work station.&lt;/p&gt;

&lt;p&gt;Typical workflows include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Work on local RStudio&lt;/li&gt;
&lt;li&gt;create project _gcssave.yaml&lt;/li&gt;
&lt;li&gt;log off, auto-save to GCE&lt;/li&gt;
&lt;li&gt;Move to Chromebook&lt;/li&gt;
&lt;li&gt;Start up VM via the Google Cloud console&lt;/li&gt;
&lt;li&gt;create a new project with same name&lt;/li&gt;
&lt;li&gt;auto-load project&lt;/li&gt;
&lt;li&gt;Do code&lt;/li&gt;
&lt;li&gt;Shutdown (auto-save project)&lt;/li&gt;
&lt;li&gt;switch to local RStudio&lt;/li&gt;
&lt;li&gt;etc. etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;hellip;and:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Save project to (private) GitHub&lt;/li&gt;
&lt;li&gt;Move to Chromebook&lt;/li&gt;
&lt;li&gt;Start up VM via the Google Cloud console&lt;/li&gt;
&lt;li&gt;Open RStudio Server IP&lt;/li&gt;
&lt;li&gt;GitHub settings autoload&lt;/li&gt;
&lt;li&gt;Clone GitHub repo via New RStudio Projects&lt;/li&gt;
&lt;li&gt;Do code, commit, push, pull&lt;/li&gt;
&lt;li&gt;etc. etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;None of this would be possible without the skills of the &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;RStudio&lt;/a&gt; team, &lt;a href=&#34;https://cloud.google.com/&#34;&gt;Google Cloud&lt;/a&gt; and &lt;a href=&#34;https://hub.docker.com/u/rocker/&#34;&gt;Rocker&lt;/a&gt;, so a massive thanks to them.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;d imagine if you want to do this with another cloud provider they should have very similar services that you can build upon, let me know in the comments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Four Ways to Schedule R scripts on Google Cloud Platform</title>
      <link>https://code.markedmondson.me/4-ways-schedule-r-scripts-on-google-cloud-platform</link>
      <pubDate>Sun, 09 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://code.markedmondson.me/4-ways-schedule-r-scripts-on-google-cloud-platform</guid>
      <description>&lt;p&gt;A common question I come across is how to automate scheduling of R scripts downloading data. This post goes through some options that I have played around with, which I’ve mostly used for downloading API data such as Google Analytics using the Google Cloud platform, but the same principles could apply for AWS or Azure.&lt;/p&gt;
&lt;!--more--&gt;
&lt;div id=&#34;scheduling-scripts-advice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scheduling scripts advice&lt;/h2&gt;
&lt;p&gt;But first, some notes on the scripts you are scheduling, that I’ve picked up.&lt;/p&gt;
&lt;div id=&#34;dont-save-data-to-the-scheduling-server&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Don’t save data to the scheduling server&lt;/h3&gt;
&lt;p&gt;I would suggest to not save or use data in the same place you are doing the scheduling. Use a service like BigQuery (&lt;code&gt;bigQueryR&lt;/code&gt;) or googleCloudStorageR (&lt;code&gt;googleCloudStorageR&lt;/code&gt;) to first load any necessary data, do your work then save it out again. This may be a bit more complicated to set up, but will save you tears if the VM or service goes down - you still have your data.&lt;/p&gt;
&lt;p&gt;To help with this, on Google Cloud you can authenticate with the same details you used to launch a VM to authenticate with the storage services above (as all are covered under the &lt;code&gt;http://www.googleapis.com/auth/cloud-services&lt;/code&gt; scope) - you can access this auth when on a GCE VM in R via &lt;code&gt;googleAuthR::gar_gce_auth()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;An example skeleton script is shown below that may be something you are scheduling.&lt;/p&gt;
&lt;p&gt;It downloads authentication files, does an API call, then saves it up to the cloud again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleAuthR)
library(googleCloudStorageR)

gcs_global_bucket(&amp;quot;my-bucket&amp;quot;)
## auth on the VM
options(googleAuthR.scopes.selected = &amp;quot;https://www.googleapis.com/auth/cloud-platform&amp;quot;)
gar_gce_auth()

## use the GCS auth to download the auth files for your API
auth_file &amp;lt;- &amp;quot;auth/my_auth_file.json&amp;quot;
gcs_get_object(auth_file, saveToDisk = TRUE)

## now auth with the file you just download
gar_auth_service(auth_file)

## do your work with APIs etc.
.....

## upload results back up to GCS (or BigQuery, etc.)
gcs_upload(my_results, name = &amp;quot;results/my_results.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;set-up-a-schedule-for-logs-too&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Set up a schedule for logs too&lt;/h3&gt;
&lt;p&gt;Logs are important for scheduled jobs, so you have some idea on whats happened when things go wrong. To help with scheduling debugging, most &lt;code&gt;googleAuthR&lt;/code&gt; packages now have a timestamp on their output messages.&lt;/p&gt;
&lt;p&gt;You can send the output of your scripts to log files, if using cron and RScript it looks something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;RScript /your-r-script.R &amp;gt; your-r-script.log&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;…where &lt;code&gt;&amp;gt;&lt;/code&gt; sends the output to the new file.&lt;/p&gt;
&lt;p&gt;Over time though, this can get big and (sigh) fill up your disk so you can’t log in to the VM (speaking from experience here!) so I now set up another scheduled job that every week takes the logs and uploads to GCS, then deletes the current ones.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;consider-using-docker-for-environments&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Consider using Docker for environments&lt;/h3&gt;
&lt;p&gt;Several of the methods below use &lt;a href=&#34;https://www.docker.com/&#34;&gt;&lt;code&gt;Docker&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The reasons for that is Docker provides a nice reprodueable way to define exactly what packages and dependencies you need for your script to run, which can run on top of any type of infrastructure as &lt;code&gt;Docker&lt;/code&gt; has quickly become a cloud standard.&lt;/p&gt;
&lt;p&gt;For instance, migrating from Google Cloud to AWS is much easier if both can be deployed using Docker, and below Docker is instrumental in allowing you to run on multiple solutions.&lt;/p&gt;
&lt;p&gt;Bear in mind that when a Docker container relaunches it won’t save any data, so any non-saved state will be lost (you should make a new container if you need it to contain data), but you’re not saving your data to the docker container anyway, aren’t you?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;scheduling-options---pros-and-cons&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scheduling options - Pros and cons&lt;/h2&gt;
&lt;p&gt;Here is an overview of the pros and cons of the options presented in more detail below:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;29%&#34; /&gt;
&lt;col width=&#34;35%&#34; /&gt;
&lt;col width=&#34;35%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Pros&lt;/th&gt;
&lt;th&gt;Cons&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1. &lt;code&gt;cronR&lt;/code&gt; Addin on &lt;code&gt;RStudio Server&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Simple and quick&lt;/td&gt;
&lt;td&gt;Not so robust, need to log into server to make changes, versioning packages.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2. &lt;code&gt;gce_vm_scheduler&lt;/code&gt; and &lt;code&gt;Dockerfiles&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Robust and can launch from local R session, support versioning&lt;/td&gt;
&lt;td&gt;Need to build Dockerfiles, all scripts on one VM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;3. Master &amp;amp; Slave VM&lt;/td&gt;
&lt;td&gt;Tailor a fresh VM for each script, cheaper&lt;/td&gt;
&lt;td&gt;Need to build Dockerfiles, more complicated VM setup.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;4. Google AppEngine with flexible containers&lt;/td&gt;
&lt;td&gt;Managed platform&lt;/td&gt;
&lt;td&gt;Need to turn script into web responses, more complicated setup&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;cronr-plus-rstudio-server&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1 - cronR plus RStudio Server&lt;/h2&gt;
&lt;p&gt;This is the simplest and the one to start with.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Start up an RStudio Server instance&lt;/li&gt;
&lt;li&gt;Install &lt;a href=&#34;https://github.com/bnosac/cronR&#34;&gt;&lt;code&gt;cronR&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Upload your R script&lt;/li&gt;
&lt;li&gt;Schedule your script using &lt;code&gt;cronR&lt;/code&gt; RStudio addin&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With &lt;code&gt;googleComputeEngineR&lt;/code&gt; and the new &lt;code&gt;gcer-public&lt;/code&gt; project containing public images that include one with &lt;code&gt;cronR&lt;/code&gt; already installed, this is as simple as the few lines of code below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleComputeEngineR)

## get the tag for prebuilt Docker image with googleAuthRverse, cronR and tidyverse
tag &amp;lt;- gce_tag_container(&amp;quot;google-auth-r-cron-tidy&amp;quot;, project = &amp;quot;gcer-public&amp;quot;)
# gcr.io/gcer-public/google-auth-r-cron-tidy

## start a custom Rstudio instance
vm &amp;lt;- gce_vm(name = &amp;quot;my-rstudio&amp;quot;,
              predefined_type = &amp;quot;n1-highmem-8&amp;quot;,
              template = &amp;quot;rstudio&amp;quot;,
              dynamic_image = tag,
              username = &amp;quot;me&amp;quot;, password = &amp;quot;mypassword&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait for it to launch and give you an IP, then log in, upload a script and configure the schedule via the &lt;code&gt;cronR&lt;/code&gt; addin.&lt;/p&gt;
&lt;p&gt;Some more detail about this workflow can be found at these &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/rstudio-team.html&#34;&gt;custom RStudio example workflows&lt;/a&gt; on the &lt;code&gt;googleComputeEngineR&lt;/code&gt; website.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gce_vm_scheduler-and-dockerfiles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2- &lt;em&gt;gce_vm_scheduler&lt;/em&gt; and &lt;em&gt;Dockerfiles&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;This method I prefer to the above since it lets you create the exact environment (e.g. package versions, dependencies) to run your script in, that you can trail dev and production versions with. It also works locally without needing to log into the server each time to deploy a script.&lt;/p&gt;
&lt;div id=&#34;handy-tools-for-docker---containerit-and-build-triggers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Handy tools for Docker - containerit and Build Triggers&lt;/h3&gt;
&lt;p&gt;Here we introduce Docker images, which may have been more a technical barrier for some before (but worth knowing, I think)&lt;/p&gt;
&lt;div id=&#34;containerit&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;containerit&lt;/h4&gt;
&lt;p&gt;Things are much easier now though, as we have the magic new R package &lt;a href=&#34;http://o2r.info/2017/05/30/containerit-package/&#34;&gt;&lt;code&gt;containerit&lt;/code&gt;&lt;/a&gt; which can generate these Docker files for you - just send &lt;code&gt;containerit::dockerfile()&lt;/code&gt; around the script file.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build-triggers&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Build Triggers&lt;/h4&gt;
&lt;p&gt;Along with auto-generating Dockerfiles, for Google Cloud in particular we now also have &lt;a href=&#34;https://cloud.google.com/container-builder/docs/how-to/build-triggers&#34;&gt;Build Triggers&lt;/a&gt; which automates building the Docker image for you.&lt;/p&gt;
&lt;p&gt;Just make the Dockerfile, then set up a trigger for when you push that file up to GitHub - you can see the ones used to create the public R resources here in the &lt;a href=&#34;https://github.com/cloudyr/googleComputeEngineR/tree/master/inst/dockerfiles&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt; repo&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;recipe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Recipe&lt;/h3&gt;
&lt;p&gt;Putting it all together then, documentation of this workflow for &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/scheduled-rscripts.html&#34;&gt;scheduling R scripts is found here&lt;/a&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If you don’t already have one, start up a scheduler VM using &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/gce_vm_scheduler.html&#34;&gt;&lt;code&gt;gce_vm_scheduler&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Create a Dockerfile either manually or using &lt;code&gt;containerit&lt;/code&gt; that will run your script upon execution&lt;/li&gt;
&lt;li&gt;Upload the Dockerfile to a git repo (private or public)&lt;/li&gt;
&lt;li&gt;Setup a build trigger for that Dockerfile&lt;/li&gt;
&lt;li&gt;Once built, set a script to schedule within that Dockerfile with &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/gce_schedule_docker.html&#34;&gt;&lt;code&gt;gce_schedule_docker&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is still in beta at time of writing but should be stable by the time &lt;code&gt;googlecomputeEngineR&lt;/code&gt; hits CRAN &lt;code&gt;0.2.0&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;master-and-slave-vms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3 - Master and Slave VMs&lt;/h2&gt;
&lt;p&gt;Some scripts take more resources than others, and since you are using VMs already you can have more control over what specifications of VM to launch based on the script you want to run.&lt;/p&gt;
&lt;p&gt;This means you can have a cheap scheduler server, that launch biggers VMs for the duration of the job. As GCP charges per minute, this can save you money over having a schedule server that is as big as what your most expensive script needs running 24/7.&lt;/p&gt;
&lt;p&gt;This method is largely like the scheduled scripts above, except in this case the scheduled script is also launching VMs to run the job upon.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;googleCloudStorageR::gcs_source&lt;/code&gt; you can run an R script straight from where it is hosted upon GCS, meaning all data, authentication files and scripts can be kept seperate from the computation. An example master script is shown below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## intended to be run on a small instance via cron
## use this script to launch other VMs with more expensive tasks
library(googleComputeEngineR)
library(googleCloudStorageR)
gce_global_project(&amp;quot;my-project&amp;quot;)
gce_global_zone(&amp;quot;europe-west1-b&amp;quot;)
gcs_global_bucket(&amp;quot;your-gcs-bucket&amp;quot;)

## auth to same project we&amp;#39;re on
googleAuthR::gar_gce_auth()

## launch the premade VM
vm &amp;lt;- gce_vm(&amp;quot;slave-1&amp;quot;)

## set SSH to use &amp;#39;master&amp;#39; username as configured before
vm &amp;lt;- gce_ssh_setup(vm, username = &amp;quot;master&amp;quot;, ssh_overwrite = TRUE)

## run the script on the VM that will source from GCS
runme &amp;lt;- &amp;quot;Rscript -e \&amp;quot;googleAuthR::gar_gce_auth();googleCloudStorageR::gcs_source(&amp;#39;download.R&amp;#39;, bucket = &amp;#39;your-gcs-bucket&amp;#39;)\&amp;quot;&amp;quot;
out &amp;lt;- docker_cmd(vm, 
                  cmd = &amp;quot;exec&amp;quot;, 
                  args = c(&amp;quot;rstudio&amp;quot;, runme), 
                  wait = TRUE)

## once finished, stop the VM
gce_vm_stop(vm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More detail is again available at the &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/scheduled-rscripts.html#master-slave-scheduler&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;google-app-engine-with-flexible-custom-runtimes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4 - Google App Engine with flexible custom runtimes&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/appengine/docs/&#34;&gt;Google App Engine&lt;/a&gt; has always had schedule options, but only for its supported languages of Python, Java, PHP etc. Now with the &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/&#34;&gt;introduction of flexible containers&lt;/a&gt;, any Docker container running any language (including R) can also be run.&lt;/p&gt;
&lt;p&gt;This is potentially the best solution since it runs upon a 100% managed platform, meaning you don’t need to worry about servers at all, and it takes care of things like server maintence, logging etc.&lt;/p&gt;
&lt;div id=&#34;setting-up-your-script-for-app-engine&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting up your script for App Engine&lt;/h3&gt;
&lt;p&gt;There are some &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/build&#34;&gt;requirements for the container&lt;/a&gt; that need configuring so it can run:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can not use &lt;code&gt;googleAuthR::gar_gce_auth()&lt;/code&gt; so will need to upload the auth token within the Dockerfile.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;AppEngine expects a web service to be listening on port 8080, so your schedule script needs to be triggered via HTTP requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For authentication, I use the system environment arguments (i.e. those usually set in &lt;code&gt;.Renviron&lt;/code&gt;) that &lt;code&gt;googleAuthR&lt;/code&gt; packages use for auto-authentication. Put the auth file (such as JSON or a &lt;code&gt;.httr-oauth&lt;/code&gt; file) into the deployment folder, then point to its location via specifying in the &lt;code&gt;app.yaml&lt;/code&gt;. Details below.&lt;/p&gt;
&lt;p&gt;To solve the need for being a webservice on port 8080 (which is then proxied to normal webports 80/443), &lt;a href=&#34;https://www.rplumber.io/&#34;&gt;&lt;code&gt;plumber&lt;/code&gt;&lt;/a&gt; is a great service by Jeff Allen of RStudio, which already comes with its own Docker solution. You can then modify that &lt;code&gt;Dockerfile&lt;/code&gt; slightly so that it works on App Engine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;recipe-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Recipe&lt;/h3&gt;
&lt;p&gt;To then schedule your R script on app engine, follow the guide below, first making sure you have setup the &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/&#34;&gt;gcloud CLI&lt;/a&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create a Google Appengine project in the US region (only region that supports flexible containers at the moment)&lt;/li&gt;
&lt;li&gt;Create a scheduled script e.g. &lt;code&gt;schedule.R&lt;/code&gt; - you can use auth from environment files specified in &lt;code&gt;app.yaml&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Make an API out of the script by using &lt;code&gt;plumber&lt;/code&gt; - example:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleAuthR)         ## authentication
library(googleCloudStorageR)  ## google cloud storage
library(readr)                ## 
## gcs auto authenticated via environment file 
## pointed to via sys.env GCS_AUTH_FILE

#* @get /demoR
demoScheduleAPI &amp;lt;- function(){
  
  ## download or do something
  something &amp;lt;- tryCatch({
      gcs_get_object(&amp;quot;schedule/test.csv&amp;quot;, 
                     bucket = &amp;quot;mark-edmondson-public-files&amp;quot;)
    }, error = function(ex) {
      NULL
    })
      
  something_else &amp;lt;- data.frame(X1 = 1,
                               time = Sys.time(), 
                               blah = paste(sample(letters, 10, replace = TRUE), collapse = &amp;quot;&amp;quot;))
  something &amp;lt;- rbind(something, something_else)
  
  tmp &amp;lt;- tempfile(fileext = &amp;quot;.csv&amp;quot;)
  on.exit(unlink(tmp))
  write.csv(something, file = tmp, row.names = FALSE)
  ## upload something
  gcs_upload(tmp, 
             bucket = &amp;quot;mark-edmondson-public-files&amp;quot;, 
             name = &amp;quot;schedule/test.csv&amp;quot;)
  
  message(&amp;quot;Done&amp;quot;, Sys.time())
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create Dockerfile. If using &lt;code&gt;containerit&lt;/code&gt; then replace FROM with &lt;code&gt;trestletech/plumber&lt;/code&gt; and add the below lines to use correct AppEngine port:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(containerit)

dockerfile &amp;lt;- dockerfile(&amp;quot;schedule.R&amp;quot;, copy = &amp;quot;script_dir&amp;quot;, soft = TRUE)
write(dockerfile, file = &amp;quot;Dockerfile&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then change/add these lines to the created Dockerfile:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;EXPOSE 8080
ENTRYPOINT [&amp;quot;R&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;pr &amp;lt;- plumber::plumb(commandArgs()[4]); pr$run(host=&amp;#39;0.0.0.0&amp;#39;, port=8080)&amp;quot;]
CMD [&amp;quot;schedule.R&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Example final Dockerfile below. This doesn’t need to be built in say a build trigger as its built upon app engine deployment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FROM trestletech/plumber
LABEL maintainer=&amp;quot;mark&amp;quot;
RUN export DEBIAN_FRONTEND=noninteractive; apt-get -y update \
 &amp;amp;&amp;amp; apt-get install -y libcairo2-dev \
    libcurl4-openssl-dev \
    libgmp-dev \
    libpng-dev \
    libssl-dev \
    libxml2-dev \
    make \
    pandoc \
    pandoc-citeproc \
    zlib1g-dev
RUN [&amp;quot;install2.r&amp;quot;, &amp;quot;-r &amp;#39;https://cloud.r-project.org&amp;#39;&amp;quot;, &amp;quot;readr&amp;quot;, &amp;quot;googleCloudStorageR&amp;quot;, &amp;quot;Rcpp&amp;quot;, &amp;quot;digest&amp;quot;, &amp;quot;crayon&amp;quot;, &amp;quot;withr&amp;quot;, &amp;quot;mime&amp;quot;, &amp;quot;R6&amp;quot;, &amp;quot;jsonlite&amp;quot;, &amp;quot;xtable&amp;quot;, &amp;quot;magrittr&amp;quot;, &amp;quot;httr&amp;quot;, &amp;quot;curl&amp;quot;, &amp;quot;testthat&amp;quot;, &amp;quot;devtools&amp;quot;, &amp;quot;hms&amp;quot;, &amp;quot;shiny&amp;quot;, &amp;quot;httpuv&amp;quot;, &amp;quot;memoise&amp;quot;, &amp;quot;htmltools&amp;quot;, &amp;quot;openssl&amp;quot;, &amp;quot;tibble&amp;quot;, &amp;quot;remotes&amp;quot;]
RUN [&amp;quot;installGithub.r&amp;quot;, &amp;quot;MarkEdmondson1234/googleAuthR@7917351&amp;quot;, &amp;quot;hadley/rlang@ff87439&amp;quot;]
WORKDIR /payload/
COPY [&amp;quot;.&amp;quot;, &amp;quot;./&amp;quot;]

EXPOSE 8080
ENTRYPOINT [&amp;quot;R&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;pr &amp;lt;- plumber::plumb(commandArgs()[4]); pr$run(host=&amp;#39;0.0.0.0&amp;#39;, port=8080)&amp;quot;]
CMD [&amp;quot;schedule.R&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Specify &lt;code&gt;app.yaml&lt;/code&gt; for flexible containers as &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/&#34;&gt;detailed here&lt;/a&gt;. Add any environment vars such as auth files, that will be included in same deployment folder.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;runtime: custom
env: flex

env_variables:
  GCS_AUTH_FILE: auth.json&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Specify &lt;code&gt;cron.yaml&lt;/code&gt; for the schedule needed:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;cron:
- description: &amp;quot;test cron&amp;quot;
  url: /demoR
  schedule: every 1 hours&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;You should now have these files in the deployment folder:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;app.yaml&lt;/code&gt; - configuration of general app settings&lt;/li&gt;
&lt;li&gt;&lt;code&gt;auth.json&lt;/code&gt; - an authentication file specified in env arguments or app.yaml&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cron.yaml&lt;/code&gt; - specification of when your scheduling is&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Dockerfile&lt;/code&gt; - specification of the environment&lt;/li&gt;
&lt;li&gt;&lt;code&gt;schedule.R&lt;/code&gt; - the plumber version of your script containing your endpoints&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Open the terminal in that folder, and deploy via &lt;code&gt;gcloud app deploy --project your-project&lt;/code&gt; and the cron schedule via &lt;code&gt;gcloud app deploy cron.yaml --project your-project&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It will take a while (up to 10 mins) the first time.&lt;/p&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The App Engine should then be deployed on &lt;a href=&#34;https://your-project.appspot.com/&#34; class=&#34;uri&#34;&gt;https://your-project.appspot.com/&lt;/a&gt; - every &lt;code&gt;GET&lt;/code&gt; request to &lt;a href=&#34;https://your-project.appspot.com/demoR&#34; class=&#34;uri&#34;&gt;https://your-project.appspot.com/demoR&lt;/a&gt; (or other endpoints you have specified in R script) will run the R code. The cron example above will run every hour to this endpoint.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Logs for the instance are found &lt;a href=&#34;https://console.cloud.google.com/logs/viewer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This approach is the most flexible, and offers a fully managed platform for your scripts. Scheduled scripts are only the beginning, since deploying such actually gives you a way to run R scripts in response to any HTTP request from any language - triggers could also include if someone updates a spreadsheet, adds a file to a folder, pushes to GitHub etc. which opens up a lot of exciting possibilities. You can also scale it up to become a fully functioning R API.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Hopefully this has given you an idea on your options for R on Google Cloud regarding scheduling. If you have some other easier workflows or suggestions for improvements please put them in the comments below!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>My R Packages</title>
      <link>https://code.markedmondson.me/r-packages/</link>
      <pubDate>Tue, 24 Jan 2017 21:20:50 +0100</pubDate>
      
      <guid>https://code.markedmondson.me/r-packages/</guid>
      <description>&lt;p&gt;A full list of R packages I have published are on &lt;a href=&#34;https://github.com/MarkEdmondson1234&#34;&gt;my Github&lt;/a&gt;, but some notable ones are below.&lt;/p&gt;
&lt;p&gt;Some are part of the &lt;a href=&#34;http://cloudyr.github.io/&#34;&gt;cloudyR project&lt;/a&gt;, which has many packages useful for using R in the cloud. &lt;img src=&#34;https://code.markedmondson.me/images/cloudyr2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I concentrate on the Google cloud below, but be sure to check out the other packages if you’re looking to work with AWS or other cloud based services.&lt;/p&gt;
&lt;div id=&#34;cran&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CRAN&lt;/h2&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;19%&#34; /&gt;
&lt;col width=&#34;25%&#34; /&gt;
&lt;col width=&#34;55%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;URL&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=googleAuthR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/googleAuthR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/googleAuthR/&#34;&gt;googleAuthR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The central workhorse for authentication on Google APIs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=googleAnalyticsR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/googleAnalyticsR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/&#34;&gt;googleAnalyticsR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Works with Google Analytics Reporting V3/V4 and Management APIs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=googleComputeEngineR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/googleComputeEngineR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/&#34;&gt;googleComputeEngineR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Launch Virtual Machines within the Google Cloud, via templates or your own Docker containers.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=googleCloudStorageR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/googleCloudStorageR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/googleCloudStorageR/&#34;&gt;googleCloudStorageR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Interact with Google Cloud Storage via R&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=bigQueryR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/bigQueryR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/bigQueryR/&#34;&gt;bigQueryR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Interact with Google BigQuery via R&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=searchConsoleR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/searchConsoleR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/searchConsoleR/&#34;&gt;searchConsoleR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Download Search Console data into R&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can tell, most are aimed towards helping R users with help in digital analytics and cloud based services. You can get some idea of how they can &lt;a href=&#34;https://code.markedmondson.me/digital-analytics-workflow-through-google-cloud/&#34;&gt;work together in a digital analytics workflow here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;github&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GITHUB&lt;/h2&gt;
&lt;p&gt;More experimental packages:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;19%&#34; /&gt;
&lt;col width=&#34;25%&#34; /&gt;
&lt;col width=&#34;55%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;URL&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Dev&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/googleMeasureR&#34;&gt;googleMeasureR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Send tracking hits to Google Analytics from R code using the Google Analytics Measurement Protocol&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;https://travis-ci.org/MarkEdmondson1234/googleLanguageR&#34;&gt;&lt;img src=&#34;https://travis-ci.org/MarkEdmondson1234/googleLanguageR.png?branch=master&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/googleLanguageR&#34;&gt;googleLanguageR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Access Speech to text, translation and NLP text processing APIs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;https://travis-ci.org/MarkEdmondson1234/googleID&#34;&gt;&lt;img src=&#34;https://travis-ci.org/MarkEdmondson1234/googleID.svg?branch=master&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/googleID&#34;&gt;googleID&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;In production, but very small so not on CRAN. Allows user authentication via Google+ API for Shiny and RMarkdown documents.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Dev&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/youtubeAnalyticsR&#34;&gt;youtubeAnalyticsR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Access YouTube Analytics data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Deprecated&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/gtmR&#34;&gt;gtmR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Superceded by &lt;a href=&#34;https://github.com/IronistM/googleTagManageR&#34;&gt;googleTagManagerR&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Reference&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/autoGoogleAPI&#34;&gt;autoGoogleAPI&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;152 R packages auto generated via &lt;code&gt;googleAuthR&lt;/code&gt;’s discovery API feature&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Done&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/gentelellaShiny&#34;&gt;gentelellaShiny&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A custom Shiny theme available in a package&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Deprecated&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/stripeR&#34;&gt;stripeR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Interact with the Stripe payment API, but superseded by another R package, &lt;a href=&#34;https://cran.r-project.org/web/packages/RStripe/index.html&#34;&gt;RStripe&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>New Blog Down</title>
      <link>https://code.markedmondson.me/new-blog-down/</link>
      <pubDate>Mon, 23 Jan 2017 22:45:03 +0100</pubDate>
      
      <guid>https://code.markedmondson.me/new-blog-down/</guid>
      <description>&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/proj4js/proj4.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/css/motion.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/highstock.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/highcharts-3d.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/highcharts-more.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/modules/annotations.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/modules/broken-axis.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/modules/data.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/modules/drilldown.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/modules/exporting.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/modules/funnel.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/modules/heatmap.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/modules/map.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/modules/no-data-to-display.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/modules/offline-exporting.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/modules/solid-gauge.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/modules/treemap.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/plugins/annotations.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/plugins/draggable-legend.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/plugins/draggable-points.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/plugins/export-csv.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/plugins/grouped-categories.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/plugins/motion.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/plugins/pattern-fill-v2.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/plugins/tooltip-delay.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/custom/reset.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/custom/symbols-extra.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highcharts/custom/text-symbols.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;https://code.markedmondson.me/rmarkdown-libs/fontawesome/font-awesome.min.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;https://code.markedmondson.me/rmarkdown-libs/htmlwdgtgrid/htmlwdgtgrid.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://code.markedmondson.me/rmarkdown-libs/highchart-binding/highchart.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;A new year, a new blogging platform!&lt;/p&gt;
&lt;p&gt;This time I’m moving from &lt;a href=&#34;https://code.markedmondson.me/hello-world&#34;&gt;Jekyll&lt;/a&gt; to RStudio’s new &lt;a href=&#34;https://github.com/rstudio/blogdown&#34;&gt;blogdown&lt;/a&gt; format.&lt;/p&gt;
&lt;p&gt;This keeps the advantages of Jekyll (a static, high performance website; markdown for editing; free hosting on Github) but with the extra bonus of being able to render in RMarkdown plus adding some nice looking capabilities from the &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt; project.&lt;/p&gt;
&lt;p&gt;As I blog a lot about R it makes sense to have a platform that is more R-aware, as well as it letting me do things like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleAnalyticsR)
library(highcharter)
library(forecast)

ga_auth()
gaid &amp;lt;- Sys.getenv(&amp;quot;GA_ID&amp;quot;)
gadata &amp;lt;- google_analytics_4(gaid, 
                             date_range = c(&amp;quot;2013-08-01&amp;quot;, &amp;quot;2017-07-31&amp;quot;),
                             dimensions = c(&amp;quot;yearMonth&amp;quot;),
                             metrics = &amp;quot;sessions&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: &amp;#39;google_analytics_4&amp;#39; is deprecated.
## Use &amp;#39;google_analytics&amp;#39; instead.
## See help(&amp;quot;Deprecated&amp;quot;) and help(&amp;quot;googleAnalyticsR-deprecated&amp;quot;).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ga_ts &amp;lt;- ts(gadata$sessions, start = c(2013,08), end = c(2017,07), frequency = 12)

forecast1 &amp;lt;- HoltWinters(ga_ts)

## forecast for next 12 months of the blog sessions
hchart(forecast(forecast1, h = 12))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:500px;&#34; class=&#34;highchart html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;hc_opts&#34;:{&#34;title&#34;:{&#34;text&#34;:null},&#34;yAxis&#34;:{&#34;title&#34;:{&#34;text&#34;:null}},&#34;credits&#34;:{&#34;enabled&#34;:false},&#34;exporting&#34;:{&#34;enabled&#34;:false},&#34;plotOptions&#34;:{&#34;series&#34;:{&#34;turboThreshold&#34;:0},&#34;treemap&#34;:{&#34;layoutAlgorithm&#34;:&#34;squarified&#34;},&#34;bubble&#34;:{&#34;minSize&#34;:5,&#34;maxSize&#34;:25}},&#34;annotationsOptions&#34;:{&#34;enabledButtons&#34;:false},&#34;tooltip&#34;:{&#34;delayForDisplay&#34;:10},&#34;xAxis&#34;:{&#34;type&#34;:&#34;datetime&#34;},&#34;series&#34;:[{&#34;data&#34;:[[1375315200000,0],[1377993600000,0],[1380585600000,0],[1383264000000,0],[1385856000000,0],[1388534400000,317],[1391212800000,197],[1393632000000,279],[1396310400000,59],[1398902400000,162],[1401580800000,107],[1404172800000,140],[1406851200000,229],[1409529600000,283],[1412121600000,347],[1414800000000,693],[1417392000000,621],[1420070400000,569],[1422748800000,1397],[1425168000000,1309],[1427846400000,1196],[1430438400000,1015],[1433116800000,1260],[1435708800000,2887],[1438387200000,2105],[1441065600000,2607],[1443657600000,3165],[1446336000000,1994],[1448928000000,2228],[1451606400000,2061],[1454284800000,2146],[1456790400000,2406],[1459468800000,2698],[1462060800000,3103],[1464739200000,2909],[1467331200000,3410],[1470009600000,3125],[1472688000000,3221],[1475280000000,3917],[1477958400000,3838],[1480550400000,8267],[1483228800000,13818],[1485907200000,6872],[1488326400000,7392],[1491004800000,6566],[1493596800000,6562],[1496275200000,5840],[1498867200000,5989]],&#34;name&#34;:&#34;Series&#34;,&#34;zIndex&#34;:3},{&#34;data&#34;:[[1501545600000,5799.88296109746],[1504224000000,5953.82702697917],[1506816000000,6286.04960256189],[1509494400000,5919.18174699678],[1512086400000,7512.16192182951],[1514764800000,8349.27443790722],[1517443200000,4701.14915567939],[1519862400000,6100.01802849025],[1522540800000,6101.59851593583],[1525132800000,6404.34402681755],[1527811200000,6192.00418324618],[1530403200000,6785.03321678322]],&#34;name&#34;:&#34;HoltWinters&#34;,&#34;zIndex&#34;:2,&#34;id&#34;:&#34;j6lq5wbmkt&#34;},{&#34;data&#34;:[[1501545600000,8009.15429421046,3590.61162798446],[1504224000000,8605.30544712378,3302.34860683456],[1506816000000,9315.86738613737,3256.23181898641],[1509494400000,9285.0774362373,2553.28605775626],[1512086400000,11183.4984332413,3840.82541041768],[1514764800000,12302.5224403987,4396.02643541569],[1517443200000,8917.50157908768,484.796732271103],[1519862400000,10563.9944786475,1636.04157833298],[1522540800000,10800.1667804965,1403.0302513752],[1525132800000,11326.3356690203,1482.35238461478],[1527811200000,11327.7085862557,1056.29978023663],[1530403200000,12125.9055897453,1444.16084382116]],&#34;name&#34;:&#34;HoltWinters level 80&#34;,&#34;type&#34;:&#34;arearange&#34;,&#34;fillOpacity&#34;:0.1,&#34;zIndex&#34;:1,&#34;lineWidth&#34;:0,&#34;linkedTo&#34;:&#34;j6lq5wbmkt&#34;},{&#34;data&#34;:[[1501545600000,9178.67189272986,2421.09402946506],[1504224000000,10008.9133371896,1898.74071676878],[1506816000000,10919.756037155,1652.34316796876],[1509494400000,11066.874983848,771.488510145552],[1512086400000,13126.9865851167,1897.33725854234],[1514764800000,14395.2455210494,2303.30335476499],[1517443200000,11149.5037269577,-1747.20541559893],[1519862400000,12927.0808680766,-727.044811096055],[1522540800000,13287.438574996,-1084.24154312437],[1525132800000,13931.8806552989,-1123.19260166379],[1527811200000,14046.3862742742,-1662.37790778189],[1530403200000,14953.1926404598,-1383.12620689336]],&#34;name&#34;:&#34;HoltWinters level 95&#34;,&#34;type&#34;:&#34;arearange&#34;,&#34;fillOpacity&#34;:0.1,&#34;zIndex&#34;:1,&#34;lineWidth&#34;:0,&#34;linkedTo&#34;:&#34;j6lq5wbmkt&#34;}]},&#34;theme&#34;:{&#34;chart&#34;:{&#34;backgroundColor&#34;:&#34;transparent&#34;}},&#34;conf_opts&#34;:{&#34;global&#34;:{&#34;Date&#34;:null,&#34;VMLRadialGradientURL&#34;:&#34;http =//code.highcharts.com/list(version)/gfx/vml-radial-gradient.png&#34;,&#34;canvasToolsURL&#34;:&#34;http =//code.highcharts.com/list(version)/modules/canvas-tools.js&#34;,&#34;getTimezoneOffset&#34;:null,&#34;timezoneOffset&#34;:0,&#34;useUTC&#34;:true},&#34;lang&#34;:{&#34;contextButtonTitle&#34;:&#34;Chart context menu&#34;,&#34;decimalPoint&#34;:&#34;.&#34;,&#34;downloadJPEG&#34;:&#34;Download JPEG image&#34;,&#34;downloadPDF&#34;:&#34;Download PDF document&#34;,&#34;downloadPNG&#34;:&#34;Download PNG image&#34;,&#34;downloadSVG&#34;:&#34;Download SVG vector image&#34;,&#34;drillUpText&#34;:&#34;Back to {series.name}&#34;,&#34;invalidDate&#34;:null,&#34;loading&#34;:&#34;Loading...&#34;,&#34;months&#34;:[&#34;January&#34;,&#34;February&#34;,&#34;March&#34;,&#34;April&#34;,&#34;May&#34;,&#34;June&#34;,&#34;July&#34;,&#34;August&#34;,&#34;September&#34;,&#34;October&#34;,&#34;November&#34;,&#34;December&#34;],&#34;noData&#34;:&#34;No data to display&#34;,&#34;numericSymbols&#34;:[&#34;k&#34;,&#34;M&#34;,&#34;G&#34;,&#34;T&#34;,&#34;P&#34;,&#34;E&#34;],&#34;printChart&#34;:&#34;Print chart&#34;,&#34;resetZoom&#34;:&#34;Reset zoom&#34;,&#34;resetZoomTitle&#34;:&#34;Reset zoom level 1:1&#34;,&#34;shortMonths&#34;:[&#34;Jan&#34;,&#34;Feb&#34;,&#34;Mar&#34;,&#34;Apr&#34;,&#34;May&#34;,&#34;Jun&#34;,&#34;Jul&#34;,&#34;Aug&#34;,&#34;Sep&#34;,&#34;Oct&#34;,&#34;Nov&#34;,&#34;Dec&#34;],&#34;thousandsSep&#34;:&#34; &#34;,&#34;weekdays&#34;:[&#34;Sunday&#34;,&#34;Monday&#34;,&#34;Tuesday&#34;,&#34;Wednesday&#34;,&#34;Thursday&#34;,&#34;Friday&#34;,&#34;Saturday&#34;]}},&#34;type&#34;:&#34;chart&#34;,&#34;fonts&#34;:[],&#34;debug&#34;:false},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;As I type R code blocks, the code output gets rendered right into the blog.&lt;/p&gt;
&lt;p&gt;This is the example from the &lt;a href=&#34;http://www.dartistics.com&#34;&gt;Dartistics.com&lt;/a&gt; homepage, and adding interactive HTML widgets to my posts is going to be lovely, with no other hassle other than writing the code.&lt;/p&gt;
&lt;div id=&#34;setup-blogdown&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup blogdown&lt;/h2&gt;
&lt;p&gt;Now, blogdown is in beta so the documentation is a bit sparse at the moment, and I had some teething troubles migrating across from my old blog, but hopefully I have that all sorted now (although I never want to hear about Git submodules again).&lt;/p&gt;
&lt;p&gt;I mostly followed the guide on &lt;a href=&#34;https://gohugo.io/tutorials/github-pages-blog/&#34;&gt;Hugo about hosting on Github pages&lt;/a&gt; and then added the deploy script it recommends to RStudio’s custom project settings, meaning I can now publish with a click of a button:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://code.markedmondson.me/images/hugo-deploy-rstudio.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;My work flow now is:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Click the “Add Post” RStudio addin&lt;/li&gt;
&lt;li&gt;Write my markdown or RMarkdown post&lt;/li&gt;
&lt;li&gt;Preview the website using the “Live Preview Site” RStudio addin&lt;/li&gt;
&lt;li&gt;Copy the public folder over to my website hosting GitHub via a “Build All” button triggering this script:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;# Build the project.
RScript -e &amp;#39;blogdown::build_site()&amp;#39;

# Copy all over to other GitHub repo that holds public files
cp -a ~/Documents/markedmondson.me-hugo/public/. ~/Documents/MarkEdmondson1234.github.io/&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Push the new changes to the website hosting GitHub&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For migration I had to go through and add the &lt;code&gt;url&lt;/code&gt; field to the posts with my old URLs to preserve old links to the blog, and also turn off forced lower case via the &lt;code&gt;disablePathToLower = true&lt;/code&gt; option in the &lt;code&gt;config.toml&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;I maintain two Github repos - one with the &lt;a href=&#34;https://github.com/MarkEdmondson1234/MarkEdmondson1234.github.io&#34;&gt;actual site content&lt;/a&gt; and the other holding the &lt;a href=&#34;https://github.com/MarkEdmondson1234/markedmondson.me-hugo&#34;&gt;Hugo configuration&lt;/a&gt;, if you’re interested in my setup.&lt;/p&gt;
&lt;p&gt;I had/have a lot of hassles with git submodules, so much so I now just copy over the files in the Hugo repo’s public folder manually to the GitHub repo that hosts the HTML.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Real-time forecasting dashboard with Google Tag Manager, Google Cloud and R Shiny - Part two</title>
      <link>https://code.markedmondson.me/real-time-GTM-google-cloud-r-shiny-2/</link>
      <pubDate>Sun, 22 Jan 2017 14:20:57 +0100</pubDate>
      
      <guid>https://code.markedmondson.me/real-time-GTM-google-cloud-r-shiny-2/</guid>
      <description>

&lt;p&gt;In part two of this two part series we walk through the steps to stream data from a &lt;a href=&#34;https://www.google.com/analytics/tag-manager/&#34;&gt;Google Tag Manager&lt;/a&gt; (GTM) implementation into a &lt;a href=&#34;https://cloud.google.com/appengine/&#34;&gt;Google App Engine&lt;/a&gt; (GAE) web app, which then adds data to a &lt;a href=&#34;https://cloud.google.com/bigquery/&#34;&gt;BigQuery&lt;/a&gt; table via BigQuery&amp;rsquo;s data streaming capability.  In part two, we go into how to query that table in realtime from R, make a forecast using R, then visualise it in &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt; and the JavaScript visualisation library &lt;a href=&#34;http://jkunst.com/highcharter/&#34;&gt;Highcharts&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Read &lt;a href=&#34;https://code.markedmondson.me/real-time-GTM-google-cloud-r-shiny-1/&#34;&gt;part one here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The project combines several languages where their advantages lie: Python for its interaction with Google APIs and its quick start creating your own API on App Engine, SQL to query the BigQuery data itself, R for its forecasting libraries and the reactive Shiny framework, and JavaScript for the visualisation and data capture at the Google Tag Manager end.&lt;/p&gt;

&lt;h2 id=&#34;thanks&#34;&gt;Thanks&lt;/h2&gt;

&lt;p&gt;This project wouldn&amp;rsquo;t have been possible without the help of the excellent work gone beforehand by &lt;a href=&#34;https://www.analyticspros.com/blog/data-science/streaming-prebid-data-google-bigquery/&#34;&gt;Luke Cempre&amp;rsquo;s post on AnalyticsPros&lt;/a&gt; for streaming data from Google Tag Manager to BigQuery, and &lt;a href=&#34;http://jkunst.com/&#34;&gt;Joshua Kunst&lt;/a&gt; for his help with the Highcharts JavaScript.&lt;/p&gt;

&lt;h2 id=&#34;data-flows&#34;&gt;Data flows&lt;/h2&gt;

&lt;p&gt;There are two data flows in this project.  The first adds data to BigQuery:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;GTM collects data into its dataLayer from a web visit.&lt;/li&gt;
&lt;li&gt;A custom HTML tag in GTM collects the data you want to stream then calls an App Engine URL with its data payload.&lt;/li&gt;
&lt;li&gt;The app engine URL is sent to a queue to add the data to BigQuery.&lt;/li&gt;
&lt;li&gt;The data plus a timestamp is put into a BigQuery row.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then to read the data:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The Shiny app calls Big Query every X seconds.&lt;/li&gt;
&lt;li&gt;The data is aggregated&lt;/li&gt;
&lt;li&gt;A forecast is made with the updated data.&lt;/li&gt;
&lt;li&gt;The Highcharts visualisation reads the changing dataset, and updates the visualisation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This blog will cover the second. A demo &lt;a href=&#34;https://github.com/MarkEdmondson1234/realtimeShiny&#34;&gt;Github repo for the Shiny app is available here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;calling-data-your-options&#34;&gt;Calling data: your options&lt;/h2&gt;

&lt;p&gt;The Google App Engine app on Github includes functions to both read and write data from BigQuery.  You can either call the data via the app engine app, which in turn reads the data via the Python BigQuery library, or if you are using a platform that supports reading the data from BigQuery then you can use that directly.&lt;/p&gt;

&lt;p&gt;In most cases, you will be better off with the latter, as you will be cutting out the middle man.  In some cases the app engine will time out so if you are using it you should make sure your app can handle null results.  But it is useful to have, for those platforms that do not have Big query SDKs, such as some visualisation BI tools.&lt;/p&gt;

&lt;h3 id=&#34;option-1-google-app-engine-reading-realtime-data-from-bigquery&#34;&gt;Option 1 - Google App Engine: Reading realtime data from BigQuery&lt;/h3&gt;

&lt;p&gt;The full code for reading and writing data is available at the &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream&#34;&gt;supporting Github repo here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The first blog in this series went through its data input, we now look at the data output.  In production this may be separated out into a different app, but for brevity its here in the same application.&lt;/p&gt;

&lt;p&gt;We first define some environmental variables in the &lt;code&gt;app.yaml&lt;/code&gt; setup file, with the dataset and table and a secret code word:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#[START env]
env_variables:
  DATASET_ID: tests
  TABLE_ID: realtime_markedmondsonme
  SECRET_SALT: change_this_to_something_unique
#[END env]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first function below then queries the BigQuery table we defined in the environmental variables, and turns it into JSON.  By default it will get the last row, or you can pass in the &lt;code&gt;limit&lt;/code&gt; argument to get more rows, or your own &lt;code&gt;q&lt;/code&gt; argument with custom SQL to query the table directly:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# queries and turns into JSON
def get_data(q, limit = 1):
	datasetId = os.environ[&#39;DATASET_ID&#39;]
	tableId   = os.environ[&#39;TABLE_ID&#39;]

	if len(q) &amp;gt; 0:
		query = q % (datasetId, tableId)
	else:
		query = &#39;SELECT * FROM %s.%s ORDER BY ts DESC LIMIT %s&#39; % (datasetId, tableId, limit)

	bqdata = sync_query(query)

	return json.dumps(bqdata)
	
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This next class is called when the &amp;ldquo;get data&amp;rdquo; URL is requested.  A lot of headers are set to ensure no browser caching is done which we don&amp;rsquo;t want since this is a realtime feed.&lt;/p&gt;

&lt;p&gt;For security, we also test via a &lt;code&gt;hash&lt;/code&gt; parameter to make sure its an authorised request, and decide how much data to return via the &lt;code&gt;limit&lt;/code&gt; parameter.&lt;/p&gt;

&lt;p&gt;Finally we call the function above and write that out to the URL response.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class QueryTable(webapp2.RequestHandler):

	def get(self):

    # no caching
		self.response.headers.add_header(&amp;quot;Access-Control-Allow-Origin&amp;quot;, &amp;quot;*&amp;quot;)
		self.response.headers.add_header(&amp;quot;Pragma&amp;quot;, &amp;quot;no-cache&amp;quot;)
		self.response.headers.add_header(&amp;quot;Cache-Control&amp;quot;, &amp;quot;no-cache, no-store, must-revalidate, pre-check=0, post-check=0&amp;quot;)
		self.response.headers.add_header(&amp;quot;Expires&amp;quot;, &amp;quot;Thu, 01 Dec 1994 16:00:00&amp;quot;)
		self.response.headers.add_header(&amp;quot;Content-Type&amp;quot;, &amp;quot;application/json&amp;quot;)

		q      = cgi.escape(self.request.get(&amp;quot;q&amp;quot;))
		myhash = cgi.escape(self.request.get(&amp;quot;hash&amp;quot;))
		limit  = cgi.escape(self.request.get(&amp;quot;limit&amp;quot;))

		salt = os.environ[&#39;SECRET_SALT&#39;]
		test = hashlib.sha224(q+salt).hexdigest()

		if(test != myhash):
			logging.debug(&#39;Expected hash: {}&#39;.format(test))
			logging.error(&amp;quot;Incorrect hash&amp;quot;)
			return

		if len(limit) == 0:
			limit = 1

		self.response.out.write(get_data(q, limit))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To find the hash, you can run this line and copy the result:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
q = &amp;quot;&amp;quot; # change to the query you use directly if you donøt use the default
salt = os.environ[&#39;SECRET_SALT&#39;] # the secret passphrase
test = hashlib.sha224(q+salt).hexdigest()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;or easier is to just run one call, then check the logs for the hash in the debug messages:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://code.markedmondson.me/images/secrethash.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;full-app-engine-script&#34;&gt;Full App Engine Script&lt;/h3&gt;

&lt;p&gt;The full script uploaded is available in the Github repository here:  &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream/blob/master/main.py&#34;&gt;main.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;With this script you then need some configuration files for the app and upload it to your Google Project.  A guide on how to deploy this is and more is available from the &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream/blob/master/README.md&#34;&gt;Github repository README&lt;/a&gt;, but once done the app will be available at &lt;code&gt;https://YOUR-PROJECT-ID.appspot.com&lt;/code&gt; and you will call the &lt;code&gt;/bq-streamer&lt;/code&gt; and &lt;code&gt;/bq-get&lt;/code&gt; URLs to send and get data.&lt;/p&gt;

&lt;h3 id=&#34;option-2-calling-bigquery-directly&#34;&gt;Option 2 - Calling BigQuery directly&lt;/h3&gt;

&lt;p&gt;This is the preferred method, as it will be on average 5-10 seconds quicker to get your results, and avoid timeouts.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m using Shiny as it fits into existing work, but you may prefer to use some of the other &lt;a href=&#34;https://cloud.google.com/bigquery/docs/reference/libraries&#34;&gt;BigQuery SDKs out there&lt;/a&gt;. I&amp;rsquo;ll be using &lt;a href=&#34;https://github.com/cloudyr/bigQueryR&#34;&gt;Github version of bigQueryR&lt;/a&gt; &lt;code&gt;&amp;gt; 0.2.0.9000&lt;/code&gt; as it has support for non-cached queries that are needed to see the tables update in realtime.&lt;/p&gt;

&lt;p&gt;A demo &lt;a href=&#34;https://github.com/MarkEdmondson1234/realtimeShiny&#34;&gt;Github repo for the Shiny app is available here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this case, the function in R that is in the Shiny &lt;code&gt;server.R&lt;/code&gt; is below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(bigQueryR)


do_bq &amp;lt;- function(limit){

  ## authenticate offline first, upload the .httr-oauth token
  bqr_auth()
  q &amp;lt;- sprintf(&amp;quot;SELECT * FROM [big-query-r:tests.realtime_markedmondsonme] ORDER BY ts DESC LIMIT %s&amp;quot;, 
               limit)
  
  bqr_query(projectId = &amp;quot;big-query-r&amp;quot;, 
            datasetId = &amp;quot;tests&amp;quot;, 
            query = q, 
            useQueryCache = FALSE)  
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;bqr_auth()&lt;/code&gt; if first run offline to generate the authentication token, which is then uploaded with the app.  Alternatively you can use the &lt;a href=&#34;http://code.markedmondson.me/googleAuthR/reference/gar_auth_service.html&#34;&gt;JSON service auth&lt;/a&gt; or if deploying on Google Compute Engine the automatic Google Cloud auth features of googleAuthR (&lt;code&gt;googleAuthR::gar_gce_auth()&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;The query itself selects all the columns, and orders by the timestamp we supplied in the input post. The function has a parameter so you can select how many rows to collect, which we use later.&lt;/p&gt;

&lt;p&gt;Note the use of &lt;code&gt;useQueryCache = FALSE&lt;/code&gt; to ensure you always get the freshest results.  If this wasn&amp;rsquo;t selected queries of the same type will return the first result they queried, which is no good for these purposes.&lt;/p&gt;

&lt;h2 id=&#34;reactivepoll-the-shiny-realtime-function&#34;&gt;reactivePoll - the Shiny realtime function&lt;/h2&gt;

&lt;p&gt;For realtime applications, &lt;code&gt;shiny::reactivePoll()&lt;/code&gt; is a function that periodically checks a datasource for changes.&lt;/p&gt;

&lt;p&gt;Now, what constitutes &amp;ldquo;realtime&amp;rdquo; is debatable here - for my applications I really only need an update every ~30 seconds.  Practically the Shiny output dims when updating with data, so for periods less than say 10 seconds it may not be the best approach for you - updating directly via JavaScript libraries may be better, and rely on say OpenCPU to provide the forecasting or another JS library.&lt;/p&gt;

&lt;p&gt;However, for my purposes I just need something better than the Google Analytics 4-hour lag in data (for GA360) and this suits well, particularly as you can apply a whole host of R data functions to the output.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;reactivePoll&lt;/code&gt; needs to be supplied with two functions: one to check if the data has changed, the other to make the complete fetch once a change is detected.  For these we just check if the timestamp of the last entry has changed, and if so, then fetch the last 1000 results to make the prediction:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## get 1000 rows of data
get_bq &amp;lt;- function(){

  message(&amp;quot;Getting new data...&amp;quot;)
  check &amp;lt;- do_bq(1000)
  
  rt &amp;lt;- as.data.frame(check, stringsAsFactors = FALSE)
  names(rt) &amp;lt;- c(&amp;quot;pageURL&amp;quot;,&amp;quot;Referrer&amp;quot;,&amp;quot;ts&amp;quot;)
  
  ## turn string into JS timestamp
  rt$timestamp &amp;lt;- as.POSIXct(as.numeric(as.character(rt$ts)), origin=&amp;quot;1970-01-01&amp;quot;)
  
  rt
}

## get 1 row of data, output its timestamp
check_bq &amp;lt;- function(){
  
  check &amp;lt;- do_bq(1)
  
  message(&amp;quot;Checking....check$ts: &amp;quot;, check$ts)
  check$ts
  
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is then called in the Shiny server function like so, in this case every 5 seconds:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;shinyServer(function(input, output, session) {
  
  ## checks every 5 seconds for changes
  realtime_data &amp;lt;- reactivePoll(5000, 
                                session, 
                                checkFunc = check_bq, 
                                valueFunc = get_bq)

  ### ... do stuff with realtime_data() ...

}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;transforming-data&#34;&gt;Transforming data&lt;/h2&gt;

&lt;p&gt;We then need to make the forecast, and put the data into the correct format it can be used in the chosen visualisation library, &lt;a href=&#34;http://jkunst.com/highcharter/&#34;&gt;highcharter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The next function takes the output of the &lt;code&gt;realtime_data()&lt;/code&gt; function, aggregates per hour (my lowly blog hasn&amp;rsquo;t enough data to make it worth doing per minute, but YNMV), turns the aggregation into time series objects suitable for the forecast and highcharts functions, then outputs a list.&lt;/p&gt;

&lt;p&gt;In this case I have chosen a very simple forecast function using all the defaults of &lt;code&gt;forecast::forecast()&lt;/code&gt; but this should be tweaked to your particular use cases, such as taking more account of seasonality and so forth.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;transform_rt &amp;lt;- function(rt){
  ## aggregate per hour
  rt_agg &amp;lt;- rt %&amp;gt;% 
    mutate(hour = format(timestamp, format = &amp;quot;%Y-%m-%d %H:00&amp;quot;)) %&amp;gt;% 
    count(hour)
  
  rt_agg$hour &amp;lt;- as.POSIXct(rt_agg$hour, origin=&amp;quot;1970-01-01&amp;quot;)
  
  # ## the number of hits per timestamp
  rt_xts &amp;lt;- xts::xts(rt_agg$n, frequency = 24, order.by = rt_agg$hour)
  rt_ts &amp;lt;- ts(rt_agg$n, frequency = 24)
  
  list(forecast = forecast::forecast(rt_ts, h = 12),
       xts = rt_xts)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All that remains now is to apply this transformation to new data as it appears (e.g. for each new visit, the hourly aggregate for the last hour increases, and the forecast updates)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  plot_data &amp;lt;- reactive({
    
    req(realtime_data())
    rt &amp;lt;- realtime_data()
    message(&amp;quot;plot_data()&amp;quot;)
    ## aggregate
    transform_rt(rt)
    
  })
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;output-to-highcharts&#34;&gt;Output to Highcharts&lt;/h2&gt;

&lt;p&gt;The final output to Highcharts has been tweaked a bit to get the prediction intervals and so forth:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  output$hc &amp;lt;- renderHighchart({
    
    req(plot_data())
    ## forcast values object
    fc &amp;lt;- plot_data()$forecast
    
    ## original data
    raw_data &amp;lt;- plot_data()$xts
    
    # plot last 48 hrs only, although forecast accounts for all data
    raw_data &amp;lt;- tail(raw_data, 48)
    raw_x_date &amp;lt;- as.numeric(index(raw_data)) * 1000
    
    ## start time in JS time
    forecast_x_start &amp;lt;- as.numeric(index(raw_data)[length(raw_data)])*1000
    ## each hour after that in seconds, 
    forecast_x_sequence &amp;lt;- seq(3600000, by = 3600000, length.out = 12)
    ## everything * 1000 to get to Javascript time
    forecast_times &amp;lt;- as.numeric(forecast_x_start + forecast_x_sequence)
    
    forecast_values &amp;lt;- as.numeric(fc$mean)
    
    hc &amp;lt;- highchart() %&amp;gt;%
      hc_chart(zoomType = &amp;quot;x&amp;quot;) %&amp;gt;%
      hc_xAxis(type = &amp;quot;datetime&amp;quot;) %&amp;gt;% 
      hc_add_series(type = &amp;quot;line&amp;quot;,
                    name = &amp;quot;data&amp;quot;,
                    data = list_parse2(data.frame(date = raw_x_date, 
                                                  value = raw_data))) %&amp;gt;%
      hc_add_series(type = &amp;quot;arearange&amp;quot;, 
                    name = &amp;quot;80%&amp;quot;,
                    fillOpacity = 0.3,
                    data = list_parse2(data.frame(date = forecast_times,
                                                  upper = as.numeric(fc$upper[,1]),
                                                  lower = as.numeric(fc$lower[,1])))) %&amp;gt;%
      hc_add_series(type = &amp;quot;arearange&amp;quot;, 
                    name = &amp;quot;95%&amp;quot;,
                    fillOpacity = 0.3,
                    data = list_parse2(data.frame(date = forecast_times,
                                                  upper = as.numeric(fc$upper[,2]),
                                                  lower = as.numeric(fc$lower[,2])))) %&amp;gt;% 
      hc_add_series(type = &amp;quot;line&amp;quot;,
                    name = &amp;quot;forecast&amp;quot;,
                    data = list_parse2(data.frame(date = forecast_times, 
                                                  value = forecast_values)))
    
    hc
    
  })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can then be displayed now in a very simple &lt;code&gt;ui.R&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(shiny)
library(highcharter)

shinyUI(fluidPage(
  titlePanel(&amp;quot;Realtime Shiny Dashboard from BigQuery&amp;quot;),
  highchartOutput(&amp;quot;hc&amp;quot;)
  )
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;to be tweaked and put into a template as needed.&lt;/p&gt;

&lt;p&gt;The gif doesn&amp;rsquo;t quite do it justice, but you get the idea:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/realtime_forcast.gif&#34; alt=&#34;Update as visits registered to the blog&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;improvements&#34;&gt;Improvements&lt;/h2&gt;

&lt;p&gt;Ideally I&amp;rsquo;d like to avoid the Shiny grey-out when new data is fetched and the graph redraw - I fiddled a bit trying to get the JavaScript to take data from an R table and pull it in directly put that didn&amp;rsquo;t work out - I may update it if its figured out later.&lt;/p&gt;

&lt;p&gt;However, as I said above for my application I needed an update only every 60 seconds so it wasn&amp;rsquo;t worth too much trouble over.  But if say you needed (and who really &lt;em&gt;needs&lt;/em&gt; this?) a smooth update every 5 seconds, the grey out would be too often to be useable.&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;The full app then can be tested easily as its realtime :-)&lt;/p&gt;

&lt;p&gt;As I visit my blog it sends data from Google Tag Manager to BigQuery; that tables is queried every 5 seconds from the Shiny app to see if any new visits have occured; if they have the full data set is downloaded; a new forecast is made and output to the Highcharts.&lt;/p&gt;

&lt;p&gt;Whatever your application, the biggest thing I got from trying this project was it was a lot easier than I expected, which I credit the BigQuery platform for, so give it a go and let me know how it goes for you. Improve on the base I have made here, and I&amp;rsquo;d be really interested in the applications beyond reporting you may use it for - real time traffic predictions that modify bids being one example.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Real-time forecasting dashboard with Google Tag Manager, Google Cloud and R Shiny - Part one</title>
      <link>https://code.markedmondson.me/real-time-GTM-google-cloud-r-shiny-1/</link>
      <pubDate>Thu, 12 Jan 2017 23:03:57 +0100</pubDate>
      
      <guid>https://code.markedmondson.me/real-time-GTM-google-cloud-r-shiny-1/</guid>
      <description>

&lt;p&gt;In part one of this two part series we walk through the steps to stream data from a &lt;a href=&#34;https://www.google.com/analytics/tag-manager/&#34;&gt;Google Tag Manager&lt;/a&gt; (GTM) implementation into a &lt;a href=&#34;https://cloud.google.com/appengine/&#34;&gt;Google App Engine&lt;/a&gt; (GAE) web app, which then adds data to a &lt;a href=&#34;https://cloud.google.com/bigquery/&#34;&gt;BigQuery&lt;/a&gt; table via BigQuery&amp;rsquo;s data streaming capability.  In part two, we go into how to query that table in realtime from R, make a forecast using R, then visualise it in &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt; and the JavaScript visualisation library &lt;a href=&#34;http://jkunst.com/highcharter/&#34;&gt;Highcharts&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Read &lt;a href=&#34;https://code.markedmondson.me/real-time-GTM-google-cloud-r-shiny-2/&#34;&gt;part two here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The project combines several languages where their advantages lie: Python for its interaction with Google APIs and its quick start creating your own API on App Engine, SQL to query the BigQuery data itself, R for its forecasting libraries and the reactive Shiny framework, and JavaScript for the visualisation and data capture at the Google Tag Manager end.&lt;/p&gt;

&lt;h2 id=&#34;thanks&#34;&gt;Thanks&lt;/h2&gt;

&lt;p&gt;This project wouldn&amp;rsquo;t have been possible without the help of the excellent work gone beforehand by &lt;a href=&#34;https://www.analyticspros.com/blog/data-science/streaming-prebid-data-google-bigquery/&#34;&gt;Luke Cempre&amp;rsquo;s post on AnalyticsPros&lt;/a&gt; for streaming data from Google Tag Manager to BigQuery, and &lt;a href=&#34;http://jkunst.com/&#34;&gt;Joshua Kunst&lt;/a&gt; for his help with the Highcharts JavaScript.&lt;/p&gt;

&lt;h2 id=&#34;data-flows&#34;&gt;Data flows&lt;/h2&gt;

&lt;p&gt;There are two data flows in this project.  The first adds data to BigQuery:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;GTM collects data into its dataLayer from a web visit.&lt;/li&gt;
&lt;li&gt;A custom HTML tag in GTM collects the data you want to stream then calls an App Engine URL with its data payload.&lt;/li&gt;
&lt;li&gt;The app engine URL is sent to a queue to add the data to BigQuery.&lt;/li&gt;
&lt;li&gt;The data plus a timestamp is put into a BigQuery row.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then to read the data:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The Shiny app calls Big Query every X seconds.&lt;/li&gt;
&lt;li&gt;The data is aggregated&lt;/li&gt;
&lt;li&gt;A forecast is made with the updated data.&lt;/li&gt;
&lt;li&gt;The Highcharts visualisation reads the changing dataset, and updates the visualisation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This blog will cover the first, putting data into BigQuery. The code for the finished app is available on Github here:  &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream&#34;&gt;https://github.com/MarkEdmondson1234/ga-bq-stream&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;bigquery-configuration&#34;&gt;BigQuery configuration&lt;/h2&gt;

&lt;p&gt;Starting with BigQuery, you need to create a project, dataset and a table where the data will stream to.  The script we will use on App Engine assumes you have one field called &amp;ldquo;ts&amp;rdquo; which will hold a timestamp, other than that add the fields you will add in the Google Tag Manager script.&lt;/p&gt;

&lt;p&gt;Select &amp;ldquo;partitioned&amp;rdquo; table when creating, which is useful if holding more than one days worth of data.&lt;/p&gt;

&lt;p&gt;A demo is shown below, where the &lt;code&gt;ts&lt;/code&gt; field is joined by the page URL and referrer for that page:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/BQconfig.png&#34; alt=&#34;bqconfig&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;google-app-engine&#34;&gt;Google App Engine&lt;/h2&gt;

&lt;p&gt;Next we get to the meat with the Google App Engine app.&lt;/p&gt;

&lt;p&gt;There is a guide on how to install and configure the app there too on its &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream/blob/master/README.md&#34;&gt;README&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this example the app both reads and writes the data to BigQuery, but in production this should be separated out to avoid hitting quotas.&lt;/p&gt;

&lt;p&gt;App Engine is useful in providing a way to run a script (in this case Python) whenever a URL is called, and also providing the infrastructure that lets you scale those hits from a free small amount to billions if you pay up.&lt;/p&gt;

&lt;p&gt;In essence we upload a Python script and tell App Engine to run the script when certain URL endpoints are called, and then we shall call that URL from Google Tag Manager with the data we want to stream.&lt;/p&gt;

&lt;p&gt;We now walk through the important functions of the app:&lt;/p&gt;

&lt;h3 id=&#34;adding-data-to-bigquery&#34;&gt;Adding data to BigQuery&lt;/h3&gt;

&lt;p&gt;You can read more about &lt;a href=&#34;https://cloud.google.com/bigquery/streaming-data-into-bigquery&#34;&gt;streaming data into BigQuery here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The first function is modified from the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/bigquery/cloud-client/stream_data.py&#34;&gt;python BigQuery examples&lt;/a&gt; and takes care of authentication, loading the JSON sent to the app into a Python list and sending to BigQuery:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def stream_data(dataset_name, table_name, json_data, time_stamp = time.time()):
    bigquery_client = bigquery.Client()
    dataset = bigquery_client.dataset(dataset_name)
    table = dataset.table(table_name)
    data = json_data

    data[&#39;ts&#39;] = time_stamp

    # Reload the table to get the schema.
    table.reload()

    ## get the names of schema
    schema = table.schema
    schema_names = [o.name for o in schema]

    logging.debug(&#39;BQ Schema: {}&#39;.format(schema_names))

    # from schema names get list of tuples of the values
    rows = [(data[x] for x in schema_names)]

    # Send data to bigquery, returning any errors
    errors = table.insert_data(rows, row_ids = str(uuid.uuid4()))

    if not errors:
    	logging.debug(&#39;Loaded 1 row into {}:{}&#39;.format(dataset_name, table_name))
    else:
        logging.error(errors)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next class reads the data from a GET or POST request to the URL we specify later, and puts the job into a task queue, along with the timestamp.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class MainHandler(webapp2.RequestHandler):

	## for debugging
	def get(self):
		## allows CORS
		self.response.headers.add_header(&amp;quot;Access-Control-Allow-Origin&amp;quot;, &amp;quot;*&amp;quot;)

		## get example.com?bq=blah
		b = self.request.get(&amp;quot;bq&amp;quot;)

		## send to async task URL
		task = taskqueue.add(url=&#39;/bq-task&#39;, params={&#39;bq&#39;: b, &#39;ts&#39;: str(time.time())})

	# use in prod
	def post(self):
		## allows CORS
		self.response.headers.add_header(&amp;quot;Access-Control-Allow-Origin&amp;quot;, &amp;quot;*&amp;quot;)

		## get example.com?bq=blah
		b = self.request.get(&amp;quot;bq&amp;quot;)

		## send to task URL
		task = taskqueue.add(url=&#39;/bq-task&#39;, params={&#39;bq&#39;: b, &#39;ts&#39;: str(time.time())})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The task queue then reads the JSON data and calls the function to send data into BigQuery.  App Engine task queues will rerun if any connection problems and act as a buffer, so you can configure them to suit the needs and volumes of your app.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class BqHandler(webapp2.RequestHandler):
	def post(self):

		## get example.com/bq-task?bq=blah
		b = self.request.get(&amp;quot;bq&amp;quot;)
		ts = self.request.get(&amp;quot;ts&amp;quot;)

		b = json.loads(b)

		logging.debug(&#39;json load: {}&#39;.format(b))

		if len(b) &amp;gt; 0:
			datasetId = os.environ[&#39;DATASET_ID&#39;]
			tableId   = os.environ[&#39;TABLE_ID&#39;]

			today = date.today().strftime(&amp;quot;%Y%m%d&amp;quot;)

			tableId = &amp;quot;%s$%s&amp;quot;%(tableId, today)

			stream_data(datasetId, tableId, b, ts)

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;full-app-engine-script&#34;&gt;Full App Engine Script&lt;/h3&gt;

&lt;p&gt;The full script uploaded is available in the Github repository here: &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream/blob/master/main.py&#34;&gt;main.py&lt;/a&gt; which also includes the read functions used in the next blogpost.&lt;/p&gt;

&lt;p&gt;With this script you then need some configuration files for the app and upload it to your Google Project.  A guide on how to deploy this is and more is available from the &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream/blob/master/README.md&#34;&gt;Github repository README&lt;/a&gt;, but once done the app will be available at &lt;code&gt;https://YOUR-PROJECT-ID.appspot.com&lt;/code&gt; and you will call the &lt;code&gt;/bq-streamer&lt;/code&gt; and &lt;code&gt;/bq-get&lt;/code&gt; URLs to send and get data.&lt;/p&gt;

&lt;h2 id=&#34;google-tag-manager&#34;&gt;Google Tag Manager&lt;/h2&gt;

&lt;p&gt;With the app ready, we now move to sending it data via Google Tag Manager.  This is relatively simple, since we just need to decide which data to add to the endpoint URL:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;&amp;lt;script&amp;gt;

  var bqArray = {};
        
  // put the variables you want realtime here      
  bqArray[&amp;quot;fieldname&amp;quot;] = &amp;quot;{{dataLayer}}&amp;quot;;
  bqArray[&amp;quot;fieldname2&amp;quot;] = &amp;quot;{{dataLayer2}}&amp;quot;;
  	
  jQuery.post(&amp;quot;https://YOUR-PROJECT-ID.appspot.com/bq-streamer&amp;quot;, {&amp;quot;bq&amp;quot;:JSON.stringify(bqArray)});

&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The script assumes you have jQuery defined on your website, if you haven&amp;rsquo;t you will need to load it either on the page or hack it a bit by loading above the script via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;&amp;lt;script src=&amp;quot;//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For my blog example, here is a screenshot from GTM all configured:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/GTMconfig.png&#34; alt=&#34;gtm&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The app engine supports GET or POST hits, GET is useful for testing in the browser yourself but its better to POST in production as it supports more data.&lt;/p&gt;

&lt;p&gt;Add this as a custom HTML tag and deploy on a trigger that occurs after the data you want to collect is there.  Thats pretty much it.&lt;/p&gt;

&lt;p&gt;Once the tag is published, make sure you have deployed the App Engine app and you are using the exact same field names as the BigQuery table.&lt;/p&gt;

&lt;h1 id=&#34;checking-its-all-working&#34;&gt;Checking its all working&lt;/h1&gt;

&lt;p&gt;You should then be able to start seeing hits in the App Engine logs and in BigQuery.  By default the BQ queries in the UI cache the results, so don&amp;rsquo;t forget to turn those off, but then as new hits are made to the GTM container you should be able to refresh and see the results in BigQuery within a few seconds.  Here is the example from my blog:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/itsalive.png&#34; alt=&#34;its-alive&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And thats it!  You could now query this table from various solutions such as Tableau or Data Studio, but in part two of this post I&amp;rsquo;ll go in to how to query this table from an R Shiny application, updating a forecast and displaying using the Highcharts library.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Insights sorting by delta metrics in the Google Analytics API v4</title>
      <link>https://code.markedmondson.me/quicker-insight-sort-metric-delta/</link>
      <pubDate>Thu, 01 Dec 2016 23:03:57 +0100</pubDate>
      
      <guid>https://code.markedmondson.me/quicker-insight-sort-metric-delta/</guid>
      <description>

&lt;p&gt;As analysts, we are often called upon to see how website metrics have improved or declined over time.  This is easy enough when looking at trends, but if you are looking to break down over other dimensions, it can involve a lot of ETL to get to what you need.&lt;/p&gt;

&lt;p&gt;For instance, if you are looking at landing page performance of SEO traffic you can sort by the top performers, but not by the top &lt;em&gt;most improved&lt;/em&gt; performers.  To see that you need to first extract your metrics for one month, extract it again for the comparison month, join the datasets on the page dimension and then create and sort by a delta metric.  For large websites, you can be exporting millions of URLs just so you can see say the top 20 most improved.&lt;/p&gt;

&lt;p&gt;This comes from the fact the Google Analytics web UI and Data Studio don&amp;rsquo;t let you sort by the &lt;em&gt;change&lt;/em&gt; of a metric.  However, this is available in the Google Analytics API v4 so a small demo on how to it and how it can be useful is shown here.&lt;/p&gt;

&lt;h2 id=&#34;extracting-the-data&#34;&gt;Extracting the data&lt;/h2&gt;

&lt;p&gt;In v4, you can pass in two date ranges in one call.  When you do this a new ordering type comes available, the &lt;a href=&#34;https://developers.google.com/analytics/devguides/reporting/core/v4/basics#delta_ordering&#34;&gt;&lt;code&gt;DELTA&lt;/code&gt;&lt;/a&gt; which is what we can use to sort the results.&lt;/p&gt;

&lt;p&gt;Bear in mind any metric filters you add will apply to the first date range, not the second.&lt;/p&gt;

&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;

&lt;p&gt;The below is implemented in R using &lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/&#34;&gt;&lt;code&gt;googleAnalyticsR&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We first load the library, authenticate and set our ViewID:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleAnalyticsR)
ga_auth()

al &amp;lt;- google_analytics_account_list()

gaid &amp;lt;- yourViewID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These are some helper functions to get the start and end dates of last month, and the same month the year before:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Start of the month
#&#39; @param x A date
som &amp;lt;- function(x) {
  as.Date(format(x, &amp;quot;%Y-%m-01&amp;quot;))
}

#&#39; End of the month
#&#39; @param x A date
eom &amp;lt;- function(x) {
  som(som(x) + 35) - 1
}

#&#39; Start and end of month
get_start_end_month &amp;lt;- function(x = Sys.Date()){
  c(som(som(x) - 1), som(x) - 1)
}

last_month &amp;lt;- get_start_end_month()
year_before &amp;lt;- get_start_end_month(Sys.Date() - 365)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We now create an SEO filter as we only want to examine SEO traffic, and a transactions over 0 metric filter:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## only organic traffic
seo_filter &amp;lt;- filter_clause_ga4(list(dim_filter(&amp;quot;medium&amp;quot;, 
                                                 &amp;quot;EXACT&amp;quot;, 
                                                 &amp;quot;organic&amp;quot;)
                               ))
                               
## met filters are on the first date
transaction0 &amp;lt;- filter_clause_ga4(list(met_filter(&amp;quot;transactions&amp;quot;, 
                                                  &amp;quot;GREATER_THAN&amp;quot;, 
                                                  0)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the sorting parameter, that we specify to be by the biggest change in transactions from last year at the top of the results:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## order by the delta change of year_before - last_month
delta_trans &amp;lt;- order_type(&amp;quot;transactions&amp;quot;,&amp;quot;DESCENDING&amp;quot;, &amp;quot;DELTA&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We now make the Google Analytics API v4 call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gadata &amp;lt;- google_analytics_4(gaid,
                             date_range = c(year_before, last_month),
                             metrics = c(&amp;quot;visits&amp;quot;,&amp;quot;transactions&amp;quot;,&amp;quot;transactionRevenue&amp;quot;),
                             dimensions = c(&amp;quot;landingPagePath&amp;quot;),
                             dim_filters = seo_filter,
                             met_filters = transaction0,
                             order = delta_trans,
                             max = 20)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should now have the top 20 most declined landing pages from last year measured by e-commerce transactions.  Much easier than downloading all pages and doing the delta calculations yourself.&lt;/p&gt;

&lt;p&gt;If you want to get the absolute number of declined transactions, you can add the column via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gadata$transactions.delta &amp;lt;- gadata$transactions.d2 - gadata$transactions.d1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;With this data you can now focus on making SEO improvements to those pages so they can reclaim their past glory, at the very least its a good starting point for investigations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Launch RStudio Server in the Google Cloud with two lines of R</title>
      <link>https://code.markedmondson.me/launch-rstudio-server-google-cloud-in-two-lines-r/</link>
      <pubDate>Thu, 20 Oct 2016 23:03:57 +0100</pubDate>
      
      <guid>https://code.markedmondson.me/launch-rstudio-server-google-cloud-in-two-lines-r/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;ve written previously about how to get &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download-server/&#34;&gt;RStudio Server&lt;/a&gt; running on Google Compute Engine: the &lt;a href=&#34;http://markedmondson.me/run-r-rstudio-and-opencpu-on-google-compute-engine-free-vm-image&#34;&gt;first in July 2014&lt;/a&gt; gave you a snapshot to download then customise, the second in &lt;a href=&#34;http://code.markedmondson.me/setting-up-scheduled-R-scripts-for-an-analytics-team/&#34;&gt;April 2016&lt;/a&gt; launched via a Docker container.&lt;/p&gt;

&lt;p&gt;Things move on, and I now recommend using the process below that uses the RStudio template in the new on CRAN &lt;a href=&#34;https://cran.r-project.org/web/packages/googleComputeEngineR/&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt;&lt;/a&gt; package.  Not only does it abstract away a lot of the dev-ops set up, but it also gives you more flexibility by taking advantage of &lt;code&gt;Dockerfiles&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;launching-an-rstudio-server&#34;&gt;Launching an Rstudio Server&lt;/h2&gt;

&lt;p&gt;This example is taken from the &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/example-workflows.html#custom-team-rstudio-server&#34;&gt;example workflows&lt;/a&gt; that are on the &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt; website&lt;/a&gt;, which includes other examples for Shiny, OpenCPU and R-clusters.&lt;/p&gt;

&lt;p&gt;You do need to do a bit of &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/installation-and-authentication.html&#34;&gt;initial setup&lt;/a&gt; to setup your Google project and download the authentication file, but after that you just need to issue these two commands:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleComputeEngineR)
vm &amp;lt;- gce_vm(template = &amp;quot;rstudio&amp;quot;,
             name = &amp;quot;my-rstudio&amp;quot;,
             username = &amp;quot;mark&amp;quot;, password = &amp;quot;mark1234&amp;quot;,
             predefined_type = &amp;quot;n1-highmem-2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And thats it.  Wait a bit, it will output an IP address for you to log in with.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://code.markedmondson.me/images/rstudio-launch-example.png&#34; alt=&#34;rstudio-googleComputeEngineR-launch&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://code.markedmondson.me/images/rstudio-login.png&#34; alt=&#34;rstudio login&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can now carry on by logging in and installing packages as you would on RStudio Desktop, then use &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/gce_vm_stop.html&#34;&gt;&lt;code&gt;gce_vm_stop(vm)&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/gce_vm_start.html&#34;&gt;&lt;code&gt;gce_vm_start(vm)&lt;/code&gt;&lt;/a&gt; to stop and start your instance, or if say you are on a Chromebook and cannot run R locally, use the Google Cloud Web UI to start and stop it.&lt;/p&gt;

&lt;h2 id=&#34;further-customisation&#34;&gt;Further customisation&lt;/h2&gt;

&lt;p&gt;You customise further by creating a custom image that launches a fresh RStudio Server instance with your own packages and files installed.  This takes advantage of some Google Cloud benefits such as the &lt;a href=&#34;https://cloud.google.com/container-registry/&#34;&gt;Container Registry&lt;/a&gt; which lets you save private Docker containers.&lt;/p&gt;

&lt;p&gt;With that, you can save your custom RStudio server to its own custom image, that can be used to launch anew in another instance as needed:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## push your rstudio image to container registry
gce_push_registry(vm, &amp;quot;my-rstudio&amp;quot;, container_name = &amp;quot;my-rstudio&amp;quot;)

## launch another rstudio instance with your settings
vm2 &amp;lt;- gce_vm(template = &amp;quot;rstudio&amp;quot;,
              name = &amp;quot;my-rstudio-2&amp;quot;,
              username = &amp;quot;mark&amp;quot;, password = &amp;quot;mark1234&amp;quot;,
              predefined_type = &amp;quot;n1-highmem-2&amp;quot;,
              dynamic_image = gce_tag_container(&amp;quot;my-rstudio&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you want to go further still, use &lt;a href=&#34;https://docs.docker.com/engine/reference/builder/&#34;&gt;&lt;code&gt;Dockerfiles&lt;/code&gt;&lt;/a&gt; to customise the underlying linux libraries and CRAN/github packages to install in a more replicable manner - a good way to keep track in Github exactly how your server is configured.&lt;/p&gt;

&lt;p&gt;A &lt;code&gt;Dockerfile&lt;/code&gt; example is shown below - construct this locally:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM rocker/hadleyverse
MAINTAINER Mark Edmondson (r@sunholo.com)

# install cron and R package dependencies
RUN apt-get update &amp;amp;&amp;amp; apt-get install -y \
    cron \
    nano \
    ## clean up
    &amp;amp;&amp;amp; apt-get clean \ 
    &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/ \ 
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds
    
## Install packages from CRAN
RUN install2.r --error \ 
    -r &#39;http://cran.rstudio.com&#39; \
    googleAuthR shinyFiles googleCloudStorage bigQueryR gmailR googleAnalyticsR \
    ## install Github packages
    &amp;amp;&amp;amp; Rscript -e &amp;quot;devtools::install_github(c(&#39;bnosac/cronR&#39;))&amp;quot; \
    ## clean up
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds \
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then build it on your VM via &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/docker_build.html&#34;&gt;&lt;code&gt;docker_build&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;docker_build(vm, 
             dockerfile = &amp;quot;file/location/dockerfile&amp;quot;, 
             new_image = &amp;quot;my-custom-image&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can then save this up to the Container Registry and launch as before:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gce_push_registry(vm, &amp;quot;my-custom-image&amp;quot;, image_name = &amp;quot;my-custom-image&amp;quot;
vm3 &amp;lt;- gce_vm(template = &amp;quot;rstudio&amp;quot;,
              name = &amp;quot;my-rstudio-3&amp;quot;,
              username = &amp;quot;mark&amp;quot;, password = &amp;quot;mark1234&amp;quot;,
              predefined_type = &amp;quot;n1-highmem-2&amp;quot;,
              dynamic_image = gce_tag_container(&amp;quot;my-custom-image&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A digital analytics workflow through the Google Cloud using R</title>
      <link>https://code.markedmondson.me/digital-analytics-workflow-through-google-cloud/</link>
      <pubDate>Mon, 10 Oct 2016 23:03:57 +0100</pubDate>
      
      <guid>https://code.markedmondson.me/digital-analytics-workflow-through-google-cloud/</guid>
      <description>

&lt;p&gt;There are now several packages built upon the &lt;code&gt;googleAuthR&lt;/code&gt; framework which are helpful to a digital analyst who uses R, so this post looks to demonstrate how they all work together.  If you&amp;rsquo;re new to R, and would like to know how it helps with your digital analytics, &lt;a href=&#34;http://analyticsdemystified.com/blog/tim-wilson/&#34;&gt;Tim Wilson&lt;/a&gt; and I ran a workshop last month aimed at getting a digital analyst up and running.  The course material is online at &lt;a href=&#34;http://www.dartistics.com/&#34;&gt;www.dartistics.com&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;diagram&#34;&gt;Diagram&lt;/h2&gt;

&lt;p&gt;A top level overview is shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/r-infrastructure.png&#34; alt=&#34;R-infrastructure-digital-analytics&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The diagram shows &lt;a href=&#34;http://code.markedmondson.me/googleAuthR/&#34;&gt;&lt;code&gt;googleAuthR&lt;/code&gt;&lt;/a&gt; packages, other packages, servers and APIs all of which interact to turn data into actionable analytics.&lt;/p&gt;

&lt;p&gt;The most recent addition is &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt;&lt;/a&gt; which has helped make great savings in the time it takes to set up servers.  I have in the past blogged about &lt;a href=&#34;http://code.markedmondson.me/setting-up-scheduled-R-scripts-for-an-analytics-team/&#34;&gt;setting up R servers in the Google Cloud&lt;/a&gt;, but it was still more dev-ops than I really wanted to be doing.  Now, I can do setups similar to those I have written about in a couple of lines of R.&lt;/p&gt;

&lt;h2 id=&#34;data-workflow&#34;&gt;Data workflow&lt;/h2&gt;

&lt;p&gt;A suggested workflow for working with data is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Infrastructure&lt;/em&gt; - Creating a computer to run your analysis.  This can be either your own laptop, or a server in the cloud.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Collection&lt;/em&gt; - Downloading your raw data from one or multiple sources, such as APIs or your CRM database.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Transformation&lt;/em&gt; - Turning your raw data into useful information via ETL, modelling or statistics.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Storage&lt;/em&gt; - Storing the information you have created so that it can be automatically updated and used.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Output&lt;/em&gt; - Displaying or using the information into an end user application.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The components of the diagram can be combined into the workflow above.  You can swap out various bits for your own needs, but its possible to use R for all of these steps.&lt;/p&gt;

&lt;p&gt;You could do all of this with an Excel workbook on your laptop.  However, as data analysis becomes more complicated, it makes sense to start breaking out the components into more specialised tools, since Excel will start to strain when you increase the data volume or want reproducibility.&lt;/p&gt;

&lt;h3 id=&#34;infrastructure&#34;&gt;Infrastructure&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt;&lt;/a&gt; uses the Google Clouds&amp;rsquo; virtual machine instances to create servers, with specific support for R.&lt;/p&gt;

&lt;p&gt;It uses &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker containers&lt;/a&gt; to make launching the servers and applications you need quick and simple, without you needing to know too much dev-ops to get started.&lt;/p&gt;

&lt;p&gt;Due to Docker, the applications created can be more easily transferred into other IT environments, such as within internal client intranets or AWS.&lt;/p&gt;

&lt;p&gt;To help with a digital analytics workflow, &lt;code&gt;googleComputeEngineR&lt;/code&gt; includes templates for the below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RStudio Server&lt;/strong&gt; - an R IDE you can work with from your browser. The server edition means you can access it from anywhere, and can always ensure the correct packages are installed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shiny&lt;/strong&gt; - A server to run your Shiny apps that display your data in dashboards or applications end users can work with.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenCPU&lt;/strong&gt; - A server that turns your R code into a JSON API.  Used for turning R output into a format web development teams can use straight within a website.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For instance, launching an RStudio server is as simple as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleComputeEngineR)
vm &amp;lt;- gce_vm_template(&amp;quot;rstudio&amp;quot;, 
                      name = &amp;quot;rstudio-server&amp;quot;, 
                      predefined_type = &amp;quot;f1-micro&amp;quot;, 
                      username = &amp;quot;mark&amp;quot;, 
                      password = &amp;quot;mark1234&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The instance will launch within a few minutes and give you a URL you can then login with.&lt;/p&gt;

&lt;h3 id=&#34;collection&#34;&gt;Collection&lt;/h3&gt;

&lt;p&gt;Once you are logged in to your RStudio Server, you can use all of R&amp;rsquo;s power to download and work with your data.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;googleAuthR&lt;/code&gt; packages can all be authenticated under the same OAuth2 token, to simplify access.&lt;/p&gt;

&lt;p&gt;Other packages useful to digital analytics include APIs such as &lt;a href=&#34;https://github.com/randyzwitch/RSiteCatalyst&#34;&gt;&lt;code&gt;RSiteCatalyst&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;twitteR&lt;/code&gt;.  A full list of digital analytics R packages can be found in the &lt;a href=&#34;https://cran.r-project.org/web/views/WebTechnologies.html&#34;&gt;web technologies section of CRAN Task Views&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another option is the R package &lt;a href=&#34;https://github.com/jennybc/googlesheets&#34;&gt;&lt;code&gt;googlesheets&lt;/code&gt;&lt;/a&gt; by Jenny Bryan, which could either be used as a data source or as a data storage for reports, to be processed onwards later.&lt;/p&gt;

&lt;p&gt;The below example R script fetches data from Google Analytics, SEO data from Google Search Console and CRM data from BigQuery.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleAnalyticsR)
library(searchConsoleR)
library(bigQueryR)
library(googleAuthR)

## authenticate with all services 
gar_auth_service(&amp;quot;auth.json&amp;quot;)

## get search console data
seo_data &amp;lt;- search_analytics(&amp;quot;http://example.com&amp;quot;, 
                             &amp;quot;2015-07-01&amp;quot;, &amp;quot;2015-07-31&amp;quot;, 
                              c(&amp;quot;date&amp;quot;, &amp;quot;query&amp;quot;, &amp;quot;page&amp;quot;))

## get google analytics data
ga_data &amp;lt;- google_analytics_4(1235678,
                              c(&amp;quot;2015-07-01&amp;quot;, &amp;quot;2015-07-31&amp;quot;),
                              metrics = c(&amp;quot;users&amp;quot;),
                              dimensions = c(&amp;quot;date&amp;quot;, &amp;quot;userID&amp;quot; , &amp;quot;landingPagePath&amp;quot;, &amp;quot;campaign&amp;quot;))

## get CRM data from BigQuery
crm_data &amp;lt;- bqr_query(&amp;quot;big-query-database&amp;quot;, &amp;quot;my_data&amp;quot;,
                      &amp;quot;SELECT userID, lifetimeValue FROM [my_dataset]&amp;quot;)
                              
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;transformation&#34;&gt;Transformation&lt;/h3&gt;

&lt;p&gt;This ground is well covered by existing R packages.  My suggestion here is to embrace the &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyverse/index.html&#34;&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/a&gt; and use that to create and generate your information.&lt;/p&gt;

&lt;p&gt;Applications include anomaly detection, measurement of causal effect, clustering and forecasting. Hadley Wickham&amp;rsquo;s book &lt;a href=&#34;http://r4ds.had.co.nz/index.html&#34;&gt;&amp;ldquo;R for Data Science&amp;rdquo;&lt;/a&gt; is a recent compendium of knowledge on this topic, which includes this suggested work flow:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/tidy-r.png&#34; alt=&#34;tidyr&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;storage&#34;&gt;Storage&lt;/h3&gt;

&lt;p&gt;Once you have your data in the format you want, you often need to keep it somewhere it is easily accessible for other systems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/storage/&#34;&gt;Google Cloud Storage&lt;/a&gt; is a cheap, reliable method of storing any type of data object so that its always available for further use, and is heavily used within Google Cloud applications as a central data store.  I use it for storing &lt;code&gt;RData&lt;/code&gt; files or storing &lt;code&gt;csv&lt;/code&gt; files with a public link that is emailed to users when available.  It is accessible in R via the &lt;a href=&#34;http://code.markedmondson.me/googleCloudStorageR/&#34;&gt;&lt;code&gt;googleCloudStorageR&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;

&lt;p&gt;For database style access, &lt;a href=&#34;https://cloud.google.com/bigquery/&#34;&gt;BigQuery&lt;/a&gt; can be queried from many data services, including data visualisation platforms such as Google&amp;rsquo;s Data Studio or Tableau.  BigQuery offers incredibly fast analytical queries for TBs of data, accessible via the &lt;a href=&#34;http://code.markedmondson.me/bigQueryR/&#34;&gt;&lt;code&gt;bigQueryR&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;

&lt;p&gt;An example of uploading data is below - again only one authentication is needed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleCloudStorageR)
library(bigQueryR)

## authenticate with all services 
gar_auth_service(&amp;quot;auth.json&amp;quot;)

## upload to Big Query
bqr_upload_data(&amp;quot;projectID&amp;quot;, &amp;quot;datasetID&amp;quot;, &amp;quot;tableID&amp;quot;,
                my_data_for_sql_queries)

## upload to Google Cloud Storage
gcs_upload(&amp;quot;my_rdata_file&amp;quot;)


&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;output&#34;&gt;Output&lt;/h3&gt;

&lt;p&gt;Finally, outputs include &lt;a href=&#34;http://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt; apps, &lt;a href=&#34;http://rmarkdown.rstudio.com/&#34;&gt;RMarkdown&lt;/a&gt;, a &lt;a href=&#34;https://gist.github.com/MarkEdmondson1234/ddcac436cbdfd4557639522573bfc7b6&#34;&gt;scheduled email&lt;/a&gt; or an R API call using &lt;a href=&#34;https://www.opencpu.org/&#34;&gt;OpenCPU&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All &lt;code&gt;googleAuthR&lt;/code&gt; functions are Shiny and RMarkdown compatible for user authentication - this means a user can login themselves and access their own data whilst using the logic of your app to gain insight, without you needing access to their data at all.  An example of an RMarkdown app taking advantage of this is the demo of the &lt;a href=&#34;https://github.com/MarkEdmondson1234/gentelellaShiny&#34;&gt;GentelellaShiny GA dashboard&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/gentellaShinyGA.png&#34; alt=&#34;gentella-shiny&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can launch OpenCPU and Shiny servers just as easily as RStudio Server via &lt;code&gt;googleComputeEngineR&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleComputeEngineR)

## creates a Shiny server
vm2 &amp;lt;- gce_vm_template(&amp;quot;shiny&amp;quot;, 
                      name = &amp;quot;shiny-server&amp;quot;, 
                      predefined_type = &amp;quot;f1-micro&amp;quot;, 
                      username = &amp;quot;mark&amp;quot;, 
                      password = &amp;quot;mark1234&amp;quot;)

## creates an OpenCPU server                      
vm3 &amp;lt;- gce_vm_template(&amp;quot;opencpu&amp;quot;, 
                      name = &amp;quot;opencpu-server&amp;quot;, 
                      predefined_type = &amp;quot;f1-micro&amp;quot;, 
                      username = &amp;quot;mark&amp;quot;, 
                      password = &amp;quot;mark1234&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Shiny Apps or RMarkdown HTML files can then be uploaded for display to the end user.  If the server needs more power, simply save the container and relaunch with a bigger RAM or CPU.&lt;/p&gt;

&lt;p&gt;OpenCPU is the technology demonstrated in my recent EARL London talk on using R to &lt;a href=&#34;http://code.markedmondson.me/predictClickOpenCPU/supercharge.html&#34;&gt;forecast HTML prefetching and deploying through Google Tag Manager&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Your Shiny, RMarkdown or OpenCPU functions can download data via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleCloudStorageR)
library(bigQueryR)

## authenticate with all services 
gar_auth_service(&amp;quot;auth.json&amp;quot;)

## download Google Cloud Storage
my_data &amp;lt;- gce_get_object(&amp;quot;my_rdata_file&amp;quot;)

## query data from Big Query dependent on user input
query &amp;lt;- bqr_query(&amp;quot;big-query-database&amp;quot;, &amp;quot;my_data&amp;quot;,
                   &amp;quot;SELECT userID, lifetimeValue FROM [my_dataset]&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Hopefully this has shown where some efficiencies could be made in your own digital analysis. For me, the reduction of computer servers to atoms of work has expanded the horizons on what is possible: applications such as sending big calculations to the cloud if taking too long locally; being able to send clients entire clusters of computers with a data application ready and working; and having customised R environments for every occasion, such as R workshops.&lt;/p&gt;

&lt;p&gt;For the future, I hope to introduce Spark clusters via &lt;a href=&#34;https://cloud.google.com/dataproc/&#34;&gt;Google Dataproc&lt;/a&gt;, giving the ability to use machine learning directly on a dataset without needing to download locally; scheduled scripts that launch servers as needed; and working with Google&amp;rsquo;s newly launched &lt;a href=&#34;https://cloud.google.com/ml/&#34;&gt;machine learning APIs&lt;/a&gt; that dovetail into the Google Cloud.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Efficient anti-sampling with the Google Analytics Reporting API</title>
      <link>https://code.markedmondson.me/anti-sampling-google-analytics-api/</link>
      <pubDate>Fri, 05 Aug 2016 23:03:57 +0100</pubDate>
      
      <guid>https://code.markedmondson.me/anti-sampling-google-analytics-api/</guid>
      <description>

&lt;p&gt;Avoiding sampling is one of the most common reasons people start using the Google Analytics API.  This blog lays out some pseudo-code to do so in an efficient manner, avoiding too many unnecessary API calls.  The approach is used in the v4 calls for the R package &lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/v4.html&#34;&gt;&lt;code&gt;googleAnalyticsR&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;avoiding-the-daily-walk&#34;&gt;Avoiding the daily walk&lt;/h2&gt;

&lt;p&gt;The most common approach to mitigate sampling is to break down the API calls into one call per day.  This has some problems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Its inefficient.&lt;/strong&gt;  If you have 80% sampling or 10% sampling, you use the same number of API calls.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It takes a long time.&lt;/strong&gt;  A year long fetch is 365 calls of 5+ seconds that can equate to a 30mins+ wait.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It doesn’t always work.&lt;/strong&gt; If you have so many sessions its sampled for one day, you will still have sampling, albeit at a lower rate.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;anti-sampling-based-on-session-size&#34;&gt;Anti-sampling based on session size&lt;/h2&gt;

&lt;p&gt;Google Analytics sampling works as &lt;a href=&#34;https://support.google.com/analytics/answer/2637192&#34;&gt;outlined in this Google article&lt;/a&gt;.  The main points are that if your API call covers a date range greater than set session limits, it will return a sampled call.&lt;/p&gt;

&lt;p&gt;The session limits vary according to if you are using Google Analytics 360 and other unknown factors in a sampling algorithm.  Fortunately, this information is available in the API responses via the &lt;code&gt;samplesReadCounts&lt;/code&gt; and &lt;code&gt;samplingSpaceSizes&lt;/code&gt; meta data.  See the &lt;a href=&#34;https://developers.google.com/analytics/devguides/reporting/core/v4/rest/v4/reports/batchGet#ReportData&#34;&gt;v4 API reference&lt;/a&gt; for their definitions.&lt;/p&gt;

&lt;p&gt;These values change per API call, so the general strategy is to make two exploratory API calls first to get the sampling information and the number of sessions over the desired date period, then use that information to construct batches of calls over date ranges that are small enough to avoid sampling, but large enough to not waste API calls.&lt;/p&gt;

&lt;p&gt;The two exploratory API calls to find the meta data are more than made up for once you have saved calls in the actual data fetch.&lt;/p&gt;

&lt;h2 id=&#34;how-it-works-in-practice-80-quicker-data&#34;&gt;How it works in practice - 80%+ quicker data&lt;/h2&gt;

&lt;p&gt;Following this approach, I have found a huge improvement in time spent for sampled calls, making it much more useable in say dynamic dashboards where waiting 30 mins for data is not an option.&lt;/p&gt;

&lt;p&gt;An example response from the &lt;code&gt;googleAnalyticsR&lt;/code&gt; library is shown below - for a month&amp;rsquo;s worth of unsampled data  that would have taken 30 API calls via a daily walk, I get the same in 5 (2 to find batch sizes, 3 to get the data).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; library(googleAnalyticsR)
&amp;gt; ga_auth()
&amp;gt; ga_data &amp;lt;- 
    google_analytics_4(id, 
                       date_range = c(&amp;quot;2016-01-01&amp;quot;,
                                      &amp;quot;2016-02-01&amp;quot;), 
                       metrics = c(&amp;quot;sessions&amp;quot;,
                                   &amp;quot;bounceRate&amp;quot;), 
                       dimensions = c(&amp;quot;date&amp;quot;,
                                      &amp;quot;landingPagePath&amp;quot;,
                                      &amp;quot;source&amp;quot;), 
                       anti_sample = TRUE)
                                
anti_sample set to TRUE. Mitigating sampling via multiple API calls.
Finding how much sampling in data request...
Data is sampled, based on 54.06% of visits.
Downloaded [10] rows from a total of [76796].
Finding number of sessions for anti-sample calculations...
Downloaded [32] rows from a total of [32].
Calculated [3] batches are needed to download [113316] rows unsampled.
Anti-sample call covering 10 days: 2016-01-01, 2016-01-10
Downloaded [38354] rows from a total of [38354].
Anti-sample call covering 20 days: 2016-01-11, 2016-01-30
Downloaded [68475] rows from a total of [68475].
Anti-sample call covering 2 days: 2016-01-31, 2016-02-01
Downloaded [6487] rows from a total of [6487].
Finished unsampled data request, total rows [113316]
Successfully avoided sampling
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The time saved gets even greater the longer the time period you request.&lt;/p&gt;

&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;As with daily walk anti-sample techniques, user metrics and unique users are linked to the date range you are querying, so this technique will not match the numbers as if you queried over the whole date range.&lt;/p&gt;

&lt;p&gt;The sampling session limit is also applied at a web property level, not View for non-GA360 accounts, so its best to use this on a Raw data View, as filters will cause the session calculations be incorrect.&lt;/p&gt;

&lt;h2 id=&#34;example-pseudo-code&#34;&gt;Example pseudo-code&lt;/h2&gt;

&lt;p&gt;R code that implements the above is &lt;a href=&#34;https://github.com/MarkEdmondson1234/googleAnalyticsR/blob/master/R/anti_sample.R&#34;&gt;available here&lt;/a&gt;, but the pseudo-code below is intended for you to port to different languages:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Get the unsampled data
test_call = get_ga_api(full_date_range, 
                       metrics = your_metrics, 
                       dimensions = your_dimensions)

// # Read the sample meta data

// read_counts is the number of sessions before sampling starts
// I make it 10% smaller to ensure its small enough as
// it seems a bit flakey
read_counts = meta_data(test_call, &amp;quot;sampledReadCounts&amp;quot;)
read_counts = read_counts * 0.9

// space_size is total amount of sessions sampling was used for
space_size = meta_data(test_call, &amp;quot;samplingSpaceSizes&amp;quot;)

// dividing by each gives the % of sampling in the API call
samplingPercent = read_counts / space_size

// if there is no sample meta data, its not sampled. We&#39;re done!
if(read_counts = NULL or space_size = NULL):
  return(test_call)
  
// ..otherwise its sampled
// # get info for batch size calculations

// I found rowCount returned from a sampled call was not equal 
// to an unsampled call, so I add 20% to rowCount to adjust
rowCount = meta_data(test_call, &amp;quot;rowCount&amp;quot;)
rowCount = rowCount * 1.2

// get the number of sessions per day
date_sessions = get_ga_api(full_date_range, 
                           metric = &amp;quot;sessions&amp;quot;, 
                           dimensions = &amp;quot;date&amp;quot;)

// get the cumulative number of sessions over the year
date_sessions.cumulative = cumsum(date_sessions.sessions)

// modulus divide the cumulative sessions by the 
// sample read_counts.
date_sessions.sample_bucket = date_sessions.cumulative %% 
                                            read_counts

// get the new date ranges per sample_bucket group
new_date_ranges = get_min_and_max_date(date_sessions)

// new_date_ranges should now hold the smaller date ranges 
// for each batched API call

// # call GA api with new date ranges

total = empty_matrix

for i in new_date_ranges:

  if(new_date_ranges[i].start == new_date_ranges[i].end):
    // only one day so split calls into hourly
    // see below for do_hourly() explanation
    batch_call = do_hourly(new_date_range.start, 
                           metrics = your_metrics, 
                           dimensions = your_dimensions)
  else:
    // multi-day batching
    batch_call = get_ga_api(new_date_ranges[i], 
                            metrics = your_metrics, 
                            dimensions = your_dimensions)
                            
  total = total + batch_call
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;per-hour-fetching-do-hourly&#34;&gt;Per hour fetching do_hourly()&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;do_hourly()&lt;/code&gt; function is very similar to the above code for daily, but with a fetch to examine the session distribution per hour.  I only call it when necessary since it is a lot more API calls and its an edge case.&lt;/p&gt;

&lt;p&gt;If you need anti-sampling for sub-hourly, then you should really be looking at using the &lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/big-query.html&#34;&gt;BigQuery Google Analytics 360 exports&lt;/a&gt;. :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SEO keyword research using searchConsoleR and googleAnalyticsR</title>
      <link>https://code.markedmondson.me/search-console-google-analytics-r-keyword-research/</link>
      <pubDate>Tue, 21 Jun 2016 23:03:57 +0100</pubDate>
      
      <guid>https://code.markedmondson.me/search-console-google-analytics-r-keyword-research/</guid>
      <description>

&lt;p&gt;In this blog we look at a method to estimate where to prioritise your SEO resources, estimating which keywords will give the greatest increase in revenue if you could improve their Google rank.&lt;/p&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&#34;https://data-seo.com/&#34;&gt;Vincent at data-seo.com&lt;/a&gt; who proof read and corrected some errors in the first draft&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Data comes from &lt;a href=&#34;https://www.google.com/webmasters/tools/home&#34;&gt;Google Search Console&lt;/a&gt; and &lt;a href=&#34;https://www.google.com/webmasters/tools/home?hl=en&#34;&gt;Google Analytics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Search Console is used to provide the keywords in these days post &lt;a href=&#34;http://www.notprovidedcount.com/&#34;&gt;(not provided)&lt;/a&gt;.  We then link the two data sets by using the URLs as a key, and estimate how much each keyword made in revenue by splitting them in the same proportion as the traffic they have sent to the page.&lt;/p&gt;

&lt;p&gt;This approach assumes each keyword converts at the same rate once on the page, and will work better with some websites more than others - the best results I have seen are those websites with a lot of content on seperate URLs, such that they capture long-tail queries.  This is because the amount of keywords per URL is small, but with enough volume to make the estimates trustworthy.&lt;/p&gt;

&lt;p&gt;We also try to incorporate magins of error in the results.  This avoids situations where one click on position 40 gets a massive weighting in potential revenue, which in reality could have been a freak event.&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;The method produced a targeted keyword list of 226 from an original seed list of ~21,000.  The top 30 revenue targets are shown in the plot below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://code.markedmondson.me/images/see-potential.png&#34; alt=&#34;seo-forecast-estimate&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Read below on how the plot was generated and what the figures mean.&lt;/p&gt;

&lt;h2 id=&#34;getting-the-data&#34;&gt;Getting the data&lt;/h2&gt;

&lt;p&gt;Google Analytics recently provided more integration between the Search Console imports and the web session data, of which &lt;a href=&#34;http://online-behavior.com/analytics/search-console&#34;&gt;Daniel Waisberg has an excellent walk-through&lt;/a&gt;, which could mean you can skip the first part of this script.&lt;/p&gt;

&lt;p&gt;However, there are circumstances where the integration won&amp;rsquo;t work, such as when the URLs in Google Analytics are a different format to Search Console - with the below script you have control on how to link the URLs, by formatting them to look the same.&lt;/p&gt;

&lt;p&gt;Data is provided via my &lt;a href=&#34;https://cran.r-project.org/web/packages/searchConsoleR/index.html&#34;&gt;searchConsoleR&lt;/a&gt; and &lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/&#34;&gt;googleAnalyticsR&lt;/a&gt; R packages.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(searchConsoleR)
library(googleAnalyticsR)

## authentication with both GA and SC
options(googleAuthR.scopes.selected = 
  c(&amp;quot;https://www.googleapis.com/auth/webmasters&amp;quot;,
    &amp;quot;https://www.googleapis.com/auth/analytics&amp;quot;,
    &amp;quot;https://www.googleapis.com/auth/analytics.readonly&amp;quot;))
    
googleAuthR::gar_auth()

## replace with your GA ID
ga_id &amp;lt;- 1111111 

## date range to fetch
start &amp;lt;- as.character(Sys.Date() - 93)
end &amp;lt;- &amp;quot;2016-06-01&amp;quot;

## Using new GA v4 API
## GAv4 filters
google_seo &amp;lt;- 
  filter_clause_ga4(list(dim_filter(&amp;quot;medium&amp;quot;, 
                                     &amp;quot;EXACT&amp;quot;, 
                                     &amp;quot;organic&amp;quot;),
                          dim_filter(&amp;quot;source&amp;quot;, 
                                     &amp;quot;EXACT&amp;quot;, 
                                     &amp;quot;google&amp;quot;)),
                     operator = &amp;quot;AND&amp;quot;)

## Getting the GA data
gadata &amp;lt;-
  google_analytics_4(ga_id,
                     date_range = c(start,end),
                     metrics = c(&amp;quot;sessions&amp;quot;,
                                 &amp;quot;transactions&amp;quot;,
                                 &amp;quot;transactionRevenue&amp;quot;),
                     dimensions = c(&amp;quot;landingPagePath&amp;quot;),
                     dim_filters = google_seo,
                     order = order_type(&amp;quot;transactions&amp;quot;, 
                                        sort_order = &amp;quot;DESC&amp;quot;, 
                                        orderType = &amp;quot;VALUE&amp;quot;),
                     max = 20000)

## Getting the Search Console data
## The development version &amp;gt;0.2.0.9000 lets you get more than 5000 rows
scdata &amp;lt;- search_analytics(&amp;quot;http://www.example.co.uk&amp;quot;, 
                           startDate = start, endDate = end,
                           dimensions = c(&amp;quot;page&amp;quot;,&amp;quot;query&amp;quot;),
                           rowLimit = 20000)

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;transforming-the-data&#34;&gt;Transforming the data&lt;/h2&gt;

&lt;p&gt;First we change the Search Console URLs into the same format as Google Analytics.  In this example, the hostname is appended to the GA URLs already (a reason why the native support won&amp;rsquo;t work), but you may also need to append the hostname to the GA URLs via &lt;code&gt;paste0(&amp;quot;www.yourdomain.com&amp;quot;, gadata$page)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We also get the search page result (e.g. Page 1 = 1-10, Page 2 = 11-20) as it may be useful.&lt;/p&gt;

&lt;p&gt;For the split of revenue later, the last call calculates the % of clicks going to each URL.&lt;/p&gt;

&lt;h3 id=&#34;search-console-data-transformation&#34;&gt;Search Console data transformation&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)

## get urls in same format

## this will differ from website to website, 
## but in most cases you will need to add the domain to the GA urls:
## gadata$page &amp;lt;- paste0(&amp;quot;www.yourdomain.com&amp;quot;, gadata$landingPagePath)
## gadata has urls www.example.com/pagePath
## scdata has urls in http://www.example.com/pagePath
scdata$page2 &amp;lt;- gsub(&amp;quot;http://&amp;quot;,&amp;quot;&amp;quot;, scdata$page)

## get SERP
scdata$serp &amp;lt;- cut(scdata$position, 
                   breaks = seq(1, 100, 10), 
                   labels = as.character(1:9),
                   include.lowest = TRUE, 
                   ordered_result = TRUE)

## % of SEO traffic to each page per keyword
scdata &amp;lt;- scdata %&amp;gt;% 
  group_by(page2) %&amp;gt;% 
  mutate(clickP = clicks / sum(clicks)) %&amp;gt;%
  ungroup()

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;merging-with-google-analytics&#34;&gt;Merging with Google Analytics&lt;/h3&gt;

&lt;p&gt;Now we merge the data, and calculate the estimates of revenue, transactions and sessions.&lt;/p&gt;

&lt;p&gt;Why estimate sessions when we already have them?  This is how we assess how accurate this approach is - if the clicks is roughly the same as the estimated sessions, we can go further.&lt;/p&gt;

&lt;p&gt;The accuracy metric is assessed as the ratio between the estiamted sessions and the clicks, minus 1.  This will be 0 when 100% accuracy, and then the further away from 0 the figure is the less we trust the results.&lt;/p&gt;

&lt;p&gt;We also round the avergae position to the nearest whole number.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)

## join data on page
joined_data &amp;lt;- gadata %&amp;gt;% 
  left_join(scdata, by = c(landingPagePath = &amp;quot;page2&amp;quot;)) %&amp;gt;%
  mutate(transactionsEst = clickP*transactions,
         revenueEst = clickP*transactionRevenue,
         sessionEst = clickP*sessions,
         accuracyEst = ((sessionEst / clicks) - 1),
         positionRound = round(position))

## we only want clicks over 0, and get rid of a few columns.
tidy_data &amp;lt;- joined_data %&amp;gt;% 
  filter(clicks &amp;gt; 0) %&amp;gt;% 
  select(-page, -sessions, -transactions, -transactionRevenue) 

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;is-it-reliable&#34;&gt;Is it reliable?&lt;/h3&gt;

&lt;p&gt;A histogram of the accuracy estimate shows we consistently over estimate but the clicks and estimated sessions are within a magnitude:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://code.markedmondson.me/images/histogram1.png&#34; alt=&#34;keyword-sessio-click-estimate-histogram&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Most of the session estimates were intrestingly around 1.3 times more than the clicks.  This may be because Search Console clicks act more like Google SEO users, but any other ideas please say in comments!&lt;/p&gt;

&lt;p&gt;From above I discarded all rows with an accuracy &amp;gt; 10 as unreliable, although you may want to be stricter in your criteria.  All figures are to be taken with a pinch of salt with this many assumptions, but if the relative performance looked ok then I feel there is still enough to get some action from the data.&lt;/p&gt;

&lt;h2 id=&#34;creating-the-seo-forecasts&#34;&gt;Creating the SEO forecasts&lt;/h2&gt;

&lt;p&gt;We now use the data to create a click curve table, with estimates on the CTR for each position, and the confidence in those results.&lt;/p&gt;

&lt;p&gt;I first attempted some models on making predictions of click curves for a website, but didn&amp;rsquo;t find any general satisifactory regressions.&lt;/p&gt;

&lt;p&gt;The diagram below uses a weighted &lt;code&gt;loess&lt;/code&gt; within &lt;code&gt;ggplot2&lt;/code&gt; which is good to show trend but not for making predictions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://code.markedmondson.me/images/ctr_scatter.png&#34; alt=&#34;ctr-scatter-plot&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;a-click-curve-to-use&#34;&gt;A click curve to use&lt;/h3&gt;

&lt;p&gt;However, 99% of the time we will only be concerned with the top 10, so it wasn&amp;rsquo;t too taxing to calculate the click through rates per website based on the data they had:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)

click_curve &amp;lt;- tidy_data %&amp;gt;% 
  group_by(positionRound) %&amp;gt;% 
  summarise(CTRmean = sum(clicks) / sum(impressions),
            n = n(),
            click.sum = sum(clicks),
            impressions.sum = sum(impressions),
            sd = sd(ctr),
            E = poisson.test(click.sum)$conf.int[2] / poisson.test(impressions.sum)$conf.int[1],
            lower = CTRmean - E/2,
            upper = CTRmean + E/2) %&amp;gt;% ungroup()

## add % increase to position 1
## could also include other positions
click_curve &amp;lt;- click_curve %&amp;gt;% 
  mutate(CTR1 = CTRmean[1] / CTRmean,
         CTR1.upper = upper[1] / upper,
         CTR1.lower = lower[1] / lower)


&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://code.markedmondson.me/images/ctr-curve.png&#34; alt=&#34;ctr-curve-seo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;These CTR rates are then used to predict how much more traffic/revenue etc. a keyword could get if they moved up to position 1.&lt;/p&gt;

&lt;h3 id=&#34;how-valuable-is-a-keyword-if-position-1&#34;&gt;How valuable is a keyword if position 1?&lt;/h3&gt;

&lt;p&gt;Once happy with the click curve, we now apply it to the original data, and work out estimates on SEO revenue for each keyword if they were at position 1.&lt;/p&gt;

&lt;p&gt;I trust results more if they have had more than 10 clicks, and the accuracy estimate figure is within 10 of 0. I would play around with this limits a little yourself to get something you can work with.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)
## combine with data

predict_click &amp;lt;- tidy_data1 %&amp;gt;% 
  mutate(positionRound = round(position)) %&amp;gt;%
  left_join(click_curve, by=c(positionRound = &amp;quot;positionRound&amp;quot;)) %&amp;gt;%
  mutate(revenueEst1 = revenueEst * CTR1,
         transEst1 = transactionsEst * CTR1,
         clickEst1 = clicks * CTR1,
         sessionsEst1 = sessionEst * CTR1,
         revenueEst1.lower = revenueEst * CTR1.lower,
         revenueEst1.upper = revenueEst * CTR1.upper,
         revenueEst1.change = revenueEst1 - revenueEst)

estimates &amp;lt;- predict_click %&amp;gt;% 
  select(landingPagePath, query, clicks, impressions, 
         ctr, position, serp, revenueEst, revenueEst1, 
         revenueEst1.change, revenueEst1.lower, revenueEst1.upper, 
         accuracyEst) %&amp;gt;%
  arrange(desc(revenueEst1)) %&amp;gt;% 
  dplyr::filter(abs(accuracyEst) &amp;lt; 10, 
                revenueEst1.change &amp;gt; 0, 
                clicks &amp;gt; 10)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;estimates&lt;/code&gt; now in this example holds 226 rows sorted in order or how much revenue they are to make if position #1 in Google. This is from an original keyword list of 21437, which is at least a way to narrow down to important keywords.&lt;/p&gt;

&lt;h2 id=&#34;plotting-the-data&#34;&gt;Plotting the data&lt;/h2&gt;

&lt;p&gt;All that remains to to present the data: limiting the keywords to the top 30 lets you present like below.&lt;/p&gt;

&lt;p&gt;The bars show the range of the estimate, as you can see its quite wide but lets you be more realistic in your expectations.&lt;/p&gt;

&lt;p&gt;The number in the middle of the bar is the current position, with the revenue at the x axis and keyword on the y.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://code.markedmondson.me/images/see-potential.png&#34; alt=&#34;seo-forecast-estimate&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;ggplot2-code&#34;&gt;ggplot2 code&lt;/h3&gt;

&lt;p&gt;To create the plots in this post, please see the ggplot2 code below.  Feel free to modify for your own needs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggplot2)

## CTR per position
ctr_plot &amp;lt;- ggplot(tidy_data, aes(x = position, 
                                  y = ctr
                                  ))
ctr_plot &amp;lt;- ctr_plot + theme_minimal()
ctr_plot &amp;lt;- ctr_plot + coord_cartesian(xlim = c(1,30), 
                                       ylim = c(0, 0.5))
ctr_plot &amp;lt;- ctr_plot + geom_point(aes(alpha = log(clicks),
                                      color = serp, 
                                      size = clicks))
ctr_plot &amp;lt;- ctr_plot + geom_smooth(aes(weight = clicks), 
                                   size = 0.2)
ctr_plot + scale_y_continuous(labels = scales::percent)
ctr_plot

hh &amp;lt;- ggplot(click_curve, aes(positionRound, CTRmean)) 
hh &amp;lt;- hh + theme_minimal()
hh &amp;lt;- hh + geom_line(linetype = 2) + coord_cartesian(xlim = c(1, 30), 
                                                     ylim = c(0,0.5))
hh &amp;lt;- hh + geom_ribbon(aes(positionRound, ymin = lower, ymax = upper), 
                       alpha = 0.2, 
                       fill = &amp;quot;orange&amp;quot;)
hh &amp;lt;- hh + scale_y_continuous(labels = scales::percent)
hh &amp;lt;- hh + geom_point() 
hh &amp;lt;- hh + geom_label(aes(label = scales::percent(CTRmean)))
hh


est_plot &amp;lt;- ggplot(estimates[1:30,], 
                   aes(reorder(query, revenueEst1), 
                       revenueEst1, 
                       ymax = revenueEst1.upper, 
                       ymin =  revenueEst1.lower))
est_plot &amp;lt;- est_plot + theme_minimal() + coord_flip()

est_plot &amp;lt;- est_plot + geom_crossbar(aes(fill = cut(accuracyEst, 
                                                    3, 
                                                    labels = c(&amp;quot;Good&amp;quot;,
                                                               &amp;quot;Ok&amp;quot;,
                                                               &amp;quot;Poor&amp;quot;))
                                                               ), 
                                     alpha = 0.7, 
                                     show.legend = FALSE)
                                     
est_plot &amp;lt;- est_plot + scale_x_discrete(name = &amp;quot;Query&amp;quot;)
est_plot &amp;lt;- est_plot + scale_y_continuous(name = &amp;quot;Estimated SEO Revenue Increase for Google #1&amp;quot;, 
                                          labels = scales::dollar_format(prefix = &amp;quot;£&amp;quot;))
est_plot &amp;lt;- est_plot + geom_label(aes(label = round(position)), 
                                  hjust = &amp;quot;center&amp;quot;)
est_plot &amp;lt;- est_plot + ggtitle(&amp;quot;SEO Potential Revenue (Current position)&amp;quot;)
est_plot

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>