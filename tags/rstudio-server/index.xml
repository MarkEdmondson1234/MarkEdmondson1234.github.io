<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rstudio Server on Mark Edmondson</title>
    <link>http://code.markedmondson.me/tags/rstudio-server/index.xml</link>
    <description>Recent content in Rstudio Server on Mark Edmondson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-GB</language>
    <copyright>Powered by [Hugo](//gohugo.io). Theme by [PPOffice](http://github.com/ppoffice).</copyright>
    <atom:link href="http://code.markedmondson.me/tags/rstudio-server/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A digital analytics workflow through the Google Cloud using R</title>
      <link>http://code.markedmondson.me/a-digital-analytics-workflow-through-the-google-cloud-using-r</link>
      <pubDate>Mon, 10 Oct 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/a-digital-analytics-workflow-through-the-google-cloud-using-r</guid>
      <description>

&lt;p&gt;There are now several packages built upon the &lt;code&gt;googleAuthR&lt;/code&gt; framework which are helpful to a digital analyst who uses R, so this post looks to demonstrate how they all work together.  If you&amp;rsquo;re new to R, and would like to know how it helps with your digital analytics, &lt;a href=&#34;http://analyticsdemystified.com/blog/tim-wilson/&#34;&gt;Tim Wilson&lt;/a&gt; and I ran a workshop last month aimed at getting a digital analyst up and running.  The course material is online at &lt;a href=&#34;http://www.dartistics.com/&#34;&gt;www.dartistics.com&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;diagram&#34;&gt;Diagram&lt;/h2&gt;

&lt;p&gt;A top level overview is shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/r-infrastructure.png&#34; alt=&#34;R-infrastructure-digital-analytics&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The diagram shows &lt;a href=&#34;http://code.markedmondson.me/googleAuthR/&#34;&gt;&lt;code&gt;googleAuthR&lt;/code&gt;&lt;/a&gt; packages, other packages, servers and APIs all of which interact to turn data into actionable analytics.&lt;/p&gt;

&lt;p&gt;The most recent addition is &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt;&lt;/a&gt; which has helped make great savings in the time it takes to set up servers.  I have in the past blogged about &lt;a href=&#34;http://code.markedmondson.me/setting-up-scheduled-R-scripts-for-an-analytics-team/&#34;&gt;setting up R servers in the Google Cloud&lt;/a&gt;, but it was still more dev-ops than I really wanted to be doing.  Now, I can do setups similar to those I have written about in a couple of lines of R.&lt;/p&gt;

&lt;h2 id=&#34;data-workflow&#34;&gt;Data workflow&lt;/h2&gt;

&lt;p&gt;A suggested workflow for working with data is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Infrastructure&lt;/em&gt; - Creating a computer to run your analysis.  This can be either your own laptop, or a server in the cloud.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Collection&lt;/em&gt; - Downloading your raw data from one or multiple sources, such as APIs or your CRM database.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Transformation&lt;/em&gt; - Turning your raw data into useful information via ETL, modelling or statistics.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Storage&lt;/em&gt; - Storing the information you have created so that it can be automatically updated and used.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Output&lt;/em&gt; - Displaying or using the information into an end user application.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The components of the diagram can be combined into the workflow above.  You can swap out various bits for your own needs, but its possible to use R for all of these steps.&lt;/p&gt;

&lt;p&gt;You could do all of this with an Excel workbook on your laptop.  However, as data analysis becomes more complicated, it makes sense to start breaking out the components into more specialised tools, since Excel will start to strain when you increase the data volume or want reproducibility.&lt;/p&gt;

&lt;h3 id=&#34;infrastructure&#34;&gt;Infrastructure&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt;&lt;/a&gt; uses the Google Clouds&amp;rsquo; virtual machine instances to create servers, with specific support for R.&lt;/p&gt;

&lt;p&gt;It uses &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker containers&lt;/a&gt; to make launching the servers and applications you need quick and simple, without you needing to know too much dev-ops to get started.&lt;/p&gt;

&lt;p&gt;Due to Docker, the applications created can be more easily transferred into other IT environments, such as within internal client intranets or AWS.&lt;/p&gt;

&lt;p&gt;To help with a digital analytics workflow, &lt;code&gt;googleComputeEngineR&lt;/code&gt; includes templates for the below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RStudio Server&lt;/strong&gt; - an R IDE you can work with from your browser. The server edition means you can access it from anywhere, and can always ensure the correct packages are installed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shiny&lt;/strong&gt; - A server to run your Shiny apps that display your data in dashboards or applications end users can work with.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenCPU&lt;/strong&gt; - A server that turns your R code into a JSON API.  Used for turning R output into a format web development teams can use straight within a website.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For instance, launching an RStudio server is as simple as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleComputeEngineR)
vm &amp;lt;- gce_vm_template(&amp;quot;rstudio&amp;quot;, 
                      name = &amp;quot;rstudio-server&amp;quot;, 
                      predefined_type = &amp;quot;f1-micro&amp;quot;, 
                      username = &amp;quot;mark&amp;quot;, 
                      password = &amp;quot;mark1234&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The instance will launch within a few minutes and give you a URL you can then login with.&lt;/p&gt;

&lt;h3 id=&#34;collection&#34;&gt;Collection&lt;/h3&gt;

&lt;p&gt;Once you are logged in to your RStudio Server, you can use all of R&amp;rsquo;s power to download and work with your data.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;googleAuthR&lt;/code&gt; packages can all be authenticated under the same OAuth2 token, to simplify access.&lt;/p&gt;

&lt;p&gt;Other packages useful to digital analytics include APIs such as &lt;a href=&#34;https://github.com/randyzwitch/RSiteCatalyst&#34;&gt;&lt;code&gt;RSiteCatalyst&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;twitteR&lt;/code&gt;.  A full list of digital analytics R packages can be found in the &lt;a href=&#34;https://cran.r-project.org/web/views/WebTechnologies.html&#34;&gt;web technologies section of CRAN Task Views&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another option is the R package &lt;a href=&#34;https://github.com/jennybc/googlesheets&#34;&gt;&lt;code&gt;googlesheets&lt;/code&gt;&lt;/a&gt; by Jenny Bryan, which could either be used as a data source or as a data storage for reports, to be processed onwards later.&lt;/p&gt;

&lt;p&gt;The below example R script fetches data from Google Analytics, SEO data from Google Search Console and CRM data from BigQuery.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleAnalyticsR)
library(searchConsoleR)
library(bigQueryR)
library(googleAuthR)

## authenticate with all services 
gar_auth_service(&amp;quot;auth.json&amp;quot;)

## get search console data
seo_data &amp;lt;- search_analytics(&amp;quot;http://example.com&amp;quot;, 
                             &amp;quot;2015-07-01&amp;quot;, &amp;quot;2015-07-31&amp;quot;, 
                              c(&amp;quot;date&amp;quot;, &amp;quot;query&amp;quot;, &amp;quot;page&amp;quot;))

## get google analytics data
ga_data &amp;lt;- google_analytics_4(1235678,
                              c(&amp;quot;2015-07-01&amp;quot;, &amp;quot;2015-07-31&amp;quot;),
                              metrics = c(&amp;quot;users&amp;quot;),
                              dimensions = c(&amp;quot;date&amp;quot;, &amp;quot;userID&amp;quot; , &amp;quot;landingPagePath&amp;quot;, &amp;quot;campaign&amp;quot;))

## get CRM data from BigQuery
crm_data &amp;lt;- bqr_query(&amp;quot;big-query-database&amp;quot;, &amp;quot;my_data&amp;quot;,
                      &amp;quot;SELECT userID, lifetimeValue FROM [my_dataset]&amp;quot;)
                              
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;transformation&#34;&gt;Transformation&lt;/h3&gt;

&lt;p&gt;This ground is well covered by existing R packages.  My suggestion here is to embrace the &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyverse/index.html&#34;&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/a&gt; and use that to create and generate your information.&lt;/p&gt;

&lt;p&gt;Applications include anomaly detection, measurement of causal effect, clustering and forecasting. Hadley Wickham&amp;rsquo;s book &lt;a href=&#34;http://r4ds.had.co.nz/index.html&#34;&gt;&amp;ldquo;R for Data Science&amp;rdquo;&lt;/a&gt; is a recent compendium of knowledge on this topic, which includes this suggested work flow:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/tidy-r.png&#34; alt=&#34;tidyr&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;storage&#34;&gt;Storage&lt;/h3&gt;

&lt;p&gt;Once you have your data in the format you want, you often need to keep it somewhere it is easily accessible for other systems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/storage/&#34;&gt;Google Cloud Storage&lt;/a&gt; is a cheap, reliable method of storing any type of data object so that its always available for further use, and is heavily used within Google Cloud applications as a central data store.  I use it for storing &lt;code&gt;RData&lt;/code&gt; files or storing &lt;code&gt;csv&lt;/code&gt; files with a public link that is emailed to users when available.  It is accessible in R via the &lt;a href=&#34;http://code.markedmondson.me/googleCloudStorageR/&#34;&gt;&lt;code&gt;googleCloudStorageR&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;

&lt;p&gt;For database style access, &lt;a href=&#34;https://cloud.google.com/bigquery/&#34;&gt;BigQuery&lt;/a&gt; can be queried from many data services, including data visualisation platforms such as Google&amp;rsquo;s Data Studio or Tableau.  BigQuery offers incredibly fast analytical queries for TBs of data, accessible via the &lt;a href=&#34;http://code.markedmondson.me/bigQueryR/&#34;&gt;&lt;code&gt;bigQueryR&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;

&lt;p&gt;An example of uploading data is below - again only one authentication is needed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleCloudStorageR)
library(bigQueryR)

## authenticate with all services 
gar_auth_service(&amp;quot;auth.json&amp;quot;)

## upload to Big Query
bqr_upload_data(&amp;quot;projectID&amp;quot;, &amp;quot;datasetID&amp;quot;, &amp;quot;tableID&amp;quot;,
                my_data_for_sql_queries)

## upload to Google Cloud Storage
gcs_upload(&amp;quot;my_rdata_file&amp;quot;)


&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;output&#34;&gt;Output&lt;/h3&gt;

&lt;p&gt;Finally, outputs include &lt;a href=&#34;http://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt; apps, &lt;a href=&#34;http://rmarkdown.rstudio.com/&#34;&gt;RMarkdown&lt;/a&gt;, a &lt;a href=&#34;https://gist.github.com/MarkEdmondson1234/ddcac436cbdfd4557639522573bfc7b6&#34;&gt;scheduled email&lt;/a&gt; or an R API call using &lt;a href=&#34;https://www.opencpu.org/&#34;&gt;OpenCPU&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All &lt;code&gt;googleAuthR&lt;/code&gt; functions are Shiny and RMarkdown compatible for user authentication - this means a user can login themselves and access their own data whilst using the logic of your app to gain insight, without you needing access to their data at all.  An example of an RMarkdown app taking advantage of this is the demo of the &lt;a href=&#34;https://github.com/MarkEdmondson1234/gentelellaShiny&#34;&gt;GentelellaShiny GA dashboard&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/gentellaShinyGA.png&#34; alt=&#34;gentella-shiny&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can launch OpenCPU and Shiny servers just as easily as RStudio Server via &lt;code&gt;googleComputeEngineR&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleComputeEngineR)

## creates a Shiny server
vm2 &amp;lt;- gce_vm_template(&amp;quot;shiny&amp;quot;, 
                      name = &amp;quot;shiny-server&amp;quot;, 
                      predefined_type = &amp;quot;f1-micro&amp;quot;, 
                      username = &amp;quot;mark&amp;quot;, 
                      password = &amp;quot;mark1234&amp;quot;)

## creates an OpenCPU server                      
vm3 &amp;lt;- gce_vm_template(&amp;quot;opencpu&amp;quot;, 
                      name = &amp;quot;opencpu-server&amp;quot;, 
                      predefined_type = &amp;quot;f1-micro&amp;quot;, 
                      username = &amp;quot;mark&amp;quot;, 
                      password = &amp;quot;mark1234&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Shiny Apps or RMarkdown HTML files can then be uploaded for display to the end user.  If the server needs more power, simply save the container and relaunch with a bigger RAM or CPU.&lt;/p&gt;

&lt;p&gt;OpenCPU is the technology demonstrated in my recent EARL London talk on using R to &lt;a href=&#34;http://code.markedmondson.me/predictClickOpenCPU/supercharge.html&#34;&gt;forecast HTML prefetching and deploying through Google Tag Manager&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Your Shiny, RMarkdown or OpenCPU functions can download data via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleCloudStorageR)
library(bigQueryR)

## authenticate with all services 
gar_auth_service(&amp;quot;auth.json&amp;quot;)

## download Google Cloud Storage
my_data &amp;lt;- gce_get_object(&amp;quot;my_rdata_file&amp;quot;)

## query data from Big Query dependent on user input
query &amp;lt;- bqr_query(&amp;quot;big-query-database&amp;quot;, &amp;quot;my_data&amp;quot;,
                   &amp;quot;SELECT userID, lifetimeValue FROM [my_dataset]&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Hopefully this has shown where some efficiencies could be made in your own digital analysis. For me, the reduction of computer servers to atoms of work has expanded the horizons on what is possible: applications such as sending big calculations to the cloud if taking too long locally; being able to send clients entire clusters of computers with a data application ready and working; and having customised R environments for every occasion, such as R workshops.&lt;/p&gt;

&lt;p&gt;For the future, I hope to introduce Spark clusters via &lt;a href=&#34;https://cloud.google.com/dataproc/&#34;&gt;Google Dataproc&lt;/a&gt;, giving the ability to use machine learning directly on a dataset without needing to download locally; scheduled scripts that launch servers as needed; and working with Google&amp;rsquo;s newly launched &lt;a href=&#34;https://cloud.google.com/ml/&#34;&gt;machine learning APIs&lt;/a&gt; that dovetail into the Google Cloud.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scheduling R scripts for a team using RStudio Server, Docker, Github and Google Compute Engine</title>
      <link>http://code.markedmondson.me/scheduling-r-scripts-for-a-team-using-rstudio-server-docker-github-and-google-compute-engine</link>
      <pubDate>Thu, 21 Apr 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/scheduling-r-scripts-for-a-team-using-rstudio-server-docker-github-and-google-compute-engine</guid>
      <description>

&lt;p&gt;&lt;em&gt;edit 20th November, 2016 - now everything in this post is abstracted away and available in the googleComputeEngineR package - I would say its a lot easier to use that.  Here is a post on getting started with it. &lt;a href=&#34;http://code.markedmondson.me/launch-rstudio-server-google-cloud-in-two-lines-r/&#34;&gt;http://code.markedmondson.me/launch-rstudio-server-google-cloud-in-two-lines-r/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This blog will give you steps that allows you to run on Google Compute Engine a server that has these features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RStudio Server instance with multiple login.&lt;/li&gt;
&lt;li&gt;Apache to host a welcome webpage.&lt;/li&gt;
&lt;li&gt;Restart the server to securely load private Github R packages / Docker images.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;A shared cron folder that runs every day at 0630 that user&amp;rsquo;s can put their own scripts into.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is inspired by conversations with &lt;a href=&#34;https://twitter.com/chipoglesby&#34;&gt;&lt;code&gt;@Chipoglesby&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/RichardFergie&#34;&gt;&lt;code&gt;@RichardFergie&lt;/code&gt;&lt;/a&gt; on twitter about this kind of setup, and was useful to me as a way to organise my thoughts on the subject ;).  If you have any suggestions on improvements please tweet to me at &lt;a href=&#34;https://twitter.com/HoloMarkeD&#34;&gt;&lt;code&gt;@HoloMarkeD&lt;/code&gt;&lt;/a&gt; or email via the blog.&lt;/p&gt;

&lt;h2 id=&#34;motivation-in-search-of-a-4-day-work-week&#34;&gt;Motivation: In search of a 4-day work week&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ve just started a new job at &lt;a href=&#34;http://iihnordic.dk/&#34;&gt;IIH Nordic&lt;/a&gt; where a part of my duties is to look for ways to automate the boring tasks that come up in a digital web analytics team.&lt;/p&gt;

&lt;p&gt;IIH Nordic have an aim to introduce a 4-day work week within a couple of years, so the motivation is to find out how to save 20% of the current work schedule without harming productivity.  Downloading data and creating reports is a big time-sink that looks ripe for optimisation.&lt;/p&gt;

&lt;h3 id=&#34;introducing-the-wonders-of-r-to-a-team&#34;&gt;Introducing the wonders of R to a team&lt;/h3&gt;

&lt;p&gt;The current strategy is to introduce the team to R and train everyone up in running simple scripts that download the data they need for their reports.  We will tackle data transformation/ETL, statistics and visualisation/presentation later, but the first goal is to solve the &amp;ldquo;How can I get data&amp;rdquo; problem.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m well placed to help due to writing &lt;a href=&#34;https://github.com/MarkEdmondson1234/googleAuthR&#34;&gt;&lt;code&gt;googleAuthR&lt;/code&gt;&lt;/a&gt;, so have played with R libraries with simple authentication and functions to download from &lt;a href=&#34;https://github.com/MarkEdmondson1234/googleAnalyticsR_public&#34;&gt;&lt;code&gt;Google Analytics&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/MarkEdmondson1234/searchConsoleR&#34;&gt;&lt;code&gt;Search Console&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/MarkEdmondson1234/bigQueryR&#34;&gt;&lt;code&gt;BigQuery&lt;/code&gt;&lt;/a&gt;, as well as using Randy Zwitch&amp;rsquo;s &lt;a href=&#34;https://github.com/randyzwitch/RSiteCatalyst&#34;&gt;&lt;code&gt;Adobe Analytics&lt;/code&gt;&lt;/a&gt; package.  These libraries alone cover 90% of the data sources we need.&lt;/p&gt;

&lt;h3 id=&#34;why-use-rstudio-server-in-the-cloud&#34;&gt;Why use RStudio Server in the cloud?&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;ve had a few internal workshops now and the team look comfortable writing the R-scripts, but during the process I noticed a few issues that are solved using RStudio Server.  Using it means:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Everyone is working on the same versions and libraries.&lt;/li&gt;
&lt;li&gt;Scripts can be logged for quality and errors.&lt;/li&gt;
&lt;li&gt;Scheduled scripts can be run from the server not someones local PC.&lt;/li&gt;
&lt;li&gt;Training material can be made available on same server.&lt;/li&gt;
&lt;li&gt;Web accessible private login for remote working.&lt;/li&gt;
&lt;li&gt;Can use R from an iPad or Google Chromebook!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;setting-up-rstudio-server-on-google-cloud-compute-gce&#34;&gt;Setting up RStudio Server on Google Cloud Compute (GCE)&lt;/h2&gt;

&lt;p&gt;I have a old blog post on the &lt;a href=&#34;http://markedmondson.me/run-r-rstudio-and-opencpu-on-google-compute-engine-free-vm-image&#34;&gt;installation of RStudio Server on GCE&lt;/a&gt;, but this will update that since technology and my understanding has improved.&lt;/p&gt;

&lt;p&gt;A brief overview of the components are below:&lt;/p&gt;

&lt;h3 id=&#34;docker&#34;&gt;Docker&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt; is a virtual machine-lite that can run on anything, which means that the set-up is very transferable to other operating systems like Windows or OSX.&lt;/p&gt;

&lt;p&gt;It offers easier set-up via pre-created Docker images, and is embraced and &lt;a href=&#34;https://cloud.google.com/compute/docs/containers/container_vms&#34;&gt;well supported by Google&lt;/a&gt; via its &lt;a href=&#34;https://cloud.google.com/container-registry/docs/&#34;&gt;container registry&lt;/a&gt;, giving you unlimited private repositories, unlike Docker Hub that gives you only one.  It provides reproducibility and scale.&lt;/p&gt;

&lt;h3 id=&#34;github&#34;&gt;Github&lt;/h3&gt;

&lt;p&gt;Github is the go-to place to review and share code, and offers web-hooks that means it can push updates when you update a repository.  Every coder should use a version control system so its good practice to introduce it to the team, and it also allows installation of experimental and private R packages via &lt;code&gt;devtools::install_github&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;Google also has support for using Github via its &lt;a href=&#34;https://cloud.google.com/source-repositories/docs/setting-up-repositories&#34;&gt;Cloud Source Repositories&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;google-compute-engine&#34;&gt;Google Compute Engine&lt;/h3&gt;

&lt;p&gt;The up-and-coming pretender to AWS&amp;rsquo;s cloud computing crown, the Google Cloud is a natural choice for me as it offers integrations for Google Analytics and BigQuery.  The server images it offers have a great API and user interface, and it offers very quick I/O and restart times on Google&amp;rsquo;s world-class infrastructure, and the Docker container support as mentioned makes things more simple to scale in the future.&lt;/p&gt;

&lt;h2 id=&#34;setup-steps&#34;&gt;Setup steps&lt;/h2&gt;

&lt;p&gt;Now we get to the actual commands you use to get things up and running.  It will give you:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RStudio Server instance with multiple login.&lt;/li&gt;
&lt;li&gt;Apache to host a welcome webpage.&lt;/li&gt;
&lt;li&gt;Restart the server to load your private Github R packages if you have any.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;A shared cron folder that runs every day at 0430 that user&amp;rsquo;s can put their own scripts into.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In general I use the command line from a local terminal, but all actions can also be carried out within the &lt;a href=&#34;https://console.cloud.google.com/compute/instances&#34;&gt;Compute Engine web interface&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;create-the-google-cloud-engine&#34;&gt;Create the Google Cloud Engine&lt;/h3&gt;

&lt;p&gt;Here is a reference for &lt;a href=&#34;https://cloud.google.com/compute/docs/containers/vm-image/&#34;&gt;GCE docker enabled VMs&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download and install the &lt;a href=&#34;https://cloud.google.com/sdk/&#34;&gt;Google Cloud SDK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Authorize and set-up billing for a Google Project&lt;/li&gt;
&lt;li&gt;Start up your terminal and set your project and zone:&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;  gcloud auth login
  
  gcloud config set project your-project-name
  
  gcloud config set compute/region europe-west1
  
  gcloud config set compute/zone europe-west1-b
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Issue this &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/reference/compute/instances/create&#34;&gt;gcloud create command&lt;/a&gt; to start up a docker enabled instance:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;edit 18th September 2016 to update to new images as per &lt;a href=&#34;https://cloud.google.com/compute/docs/containers/vm-image/&#34;&gt;https://cloud.google.com/compute/docs/containers/vm-image/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud compute instances create your-r-server-name \
  --image-family gci-stable \
  --image-project google-containers \
  --tags http-server
  --scopes https://www.googleapis.com/auth/devstorage.read_write \
  --machine-type n1-standard-1

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unlike previously, this new (from Aug 2016) container ready VM comes with Google Cloud Storage scopes already set, so you can use private docker repos.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;--tags&lt;/code&gt; flag sets the default http firewall rules to apply to this instance so we can reach it via the internet on port 80.&lt;/p&gt;

&lt;p&gt;You should now be able to see your instance has launched in the &lt;a href=&#34;https://console.cloud.google.com/compute/instances&#34;&gt;Google Compute Dashboard&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;create-persistent-disks&#34;&gt;Create persistent disks&lt;/h3&gt;

&lt;p&gt;Anything new in the Docker container will disappear on a restart if you also don&amp;rsquo;t commit and push the image, so to avoid that data and scripts are linked to the container via the &lt;code&gt;--volumes&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;We also link to persistent disk rather than the VM&amp;rsquo;s own, which has the advantage of being able to connect (read-only) to multiple servers at the same time, should you need it.  This also means if you need a more powerful server, you can safely create one knowing you will have the same data and scripts available.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud compute disks create --size=200GB my-data-disk
gcloud compute instances attach-disk your-r-server-name --disk my-data-disk
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;initial-configuration-on-google-compute-engine&#34;&gt;Initial Configuration on Google Compute Engine&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Login to GCE&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;  gcloud compute ssh your-r-server-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are now making commands from within the GCE VM.&lt;/p&gt;

&lt;p&gt;If you have any problems, use the web interface to login via the Cloud Shell.  Sometimes you had to add your username to the instance, and then login with that user like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;  gcloud compute ssh user@your-r-server-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See &lt;a href=&#34;https://cloud.google.com/compute/docs/troubleshooting#ssherrors&#34;&gt;here&lt;/a&gt; for more diagnostics help.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Format and mount the persistent disk&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;See instructions &lt;a href=&#34;https://cloud.google.com/compute/docs/disks/persistent-disks&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;```sh
sudo mkfs.ext4 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/disk/by-id/google-persistent-disk-1

##make mount point
sudo mkdir /mnt/data/
sudo mount -o discard,defaults /dev/disk/by-id/google-persistent-disk-1 /mnt/data/

## user folders
sudo mkdir /mnt/data/home/

## custom packages
sudo mkdir /mnt/data/R/

##permissions
sudo chmod a+w /mnt/data

## make it mount automatically each time
echo &#39;/dev/disk/by-id/google-persistent-disk-1 /mnt/data ext4 discard,defaults 1 1&#39; | sudo tee -a /etc/fstab
```
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Install and Configure Apache&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This gives you a user-friendly webpage pointing to the RStudio login, and I also use it as a place to put training material such as RMarkdown documents.&lt;/p&gt;

&lt;p&gt;I suppose this could also be done via another Docker container if you have a more complicated Apache setup to use.&lt;/p&gt;

&lt;p&gt;The ProxyPassMatch line is needed for the Shiny engine thats within RStudio Server to work.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo apt-get update
sudo apt-get -y install apache2

## Need proxy and proxy_http to create nice URLs, proxy_wstunnel for Shiny
sudo a2enmod proxy proxy_http proxy_wstunnel

## this may be 000-default.conf rather than 000-default 
## depending on version of apache
echo &#39;&amp;lt;VirtualHost *:80&amp;gt;
        ServerAdmin your@email.com

        DocumentRoot /var/www

        ProxyPassMatch ^/rstudio/p/([0-9]+)/(websocket|.*/websocket)/$ ws://localhost:8787/p/$1/$2/
        ProxyPass /rstudio/ http://localhost:8787/
        ProxyPassReverse /rstudio/ http://localhost:8787/
        RedirectMatch permanent ^/rstudio$ /rstudio/

        ErrorLog ${APACHE_LOG_DIR}/error.log

        LogLevel warn

        CustomLog ${APACHE_LOG_DIR}/access.log combined

&amp;lt;/VirtualHost&amp;gt;&#39; | sudo tee /etc/apache2/sites-enabled/000-default

sudo service apache2 restart

## A startup HTML page for you to customise
echo &#39;&amp;lt;!doctype html&amp;gt;&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;RStudio on Google Cloud Compute&amp;lt;/h1&amp;gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;./rstudio/&amp;quot;&amp;gt;RStudio Server&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;&#39; | sudo tee /var/www/index.html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should now be able to see your server running Apache.&lt;/p&gt;

&lt;p&gt;I then upload a website to &lt;code&gt;/var/www/&lt;/code&gt; via the &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/reference/compute/copy-files&#34;&gt;&lt;code&gt;gcloud copy-files&lt;/code&gt; command&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;An example, assuming your website is in your local folder &lt;code&gt;~/dev/website/www/&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo gcloud --project your-project-name compute copy-files ~/dev/website/www/ your-r-server-name:/var/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is what we have at IIH Nordic:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/rstudio-server-webpage.png&#34; alt=&#34;batching_example&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;a-download-folder&#34;&gt;A download folder&lt;/h4&gt;

&lt;p&gt;Now, you may want to have a dropbox style folder for the data your scripts are running, say scheduled data downloads.  There are a few ways to skin this cat, such as uploading to cloud storage in your script, but the simplest way for me was to use the Apache functionality to create a logged in download area.&lt;/p&gt;

&lt;p&gt;For this, you need to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create a folder on your data disk where scripts will dump their data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir /mnt/data/downloads
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a symlink to an Apache web server folder&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ln -s /mnt/data/downloads /var/www/downloads
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[optional] Style the folder with CSS so it looks nice using say &lt;a href=&#34;http://adamwhitcroft.com/apaxy/&#34;&gt;Apaxy&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Update your Apache config to have logged in access to the folder.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The extra Apache config is below.  It requires installation of &lt;code&gt;sudo a2enmod headers&lt;/code&gt; and a restart.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;### In download folder, auto-download .txt,.csv, and .pdf files
### Protect with a password
&amp;lt;Directory /var/www/downloads/&amp;gt;
      &amp;lt;FilesMatch &amp;quot;.+\.(txt|csv|pdf)$&amp;quot;&amp;gt;
          ForceType application/octet-stream
          Header set Content-Disposition attachment
      &amp;lt;/FilesMatch&amp;gt;
      AuthType Basic
      AuthName &amp;quot;Enter password&amp;quot;
      AuthUserFile /etc/apache2/.htpasswd
      Require valid-user
      Order allow,deny
      Allow from all
      Options Indexes FollowSymLinks
      AllowOverride All
&amp;lt;/Directory&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;download-and-run-the-rstudio-server-docker-image&#34;&gt;Download and Run the RStudio Server Docker image&lt;/h3&gt;

&lt;p&gt;We first download a pre-prepared RStudio Docker container created by the &lt;a href=&#34;http://dirk.eddelbuettel.com/blog/2014/10/23/&#34;&gt;Rocker&lt;/a&gt; team led by Dirk Eddelbuettel. This is what our custom image will be based upon.  Here we download a variant that also loads &lt;a href=&#34;https://hub.docker.com/r/rocker/hadleyverse/&#34;&gt;RStudio and all of Hadley&amp;rsquo;s packages&lt;/a&gt; to give us a great base to work from.&lt;/p&gt;

&lt;p&gt;Most of the below is gleaned from the &lt;a href=&#34;https://github.com/rocker-org/rocker/wiki&#34;&gt;Rocker Wiki&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The below docker command runs the docker image - if that image is not available it will look for the image on Docker hub and download. This will take a long time as it downloads everything, perhaps time to put the kettle on.&lt;/p&gt;

&lt;p&gt;Subsequent times will load quickly from local version.&lt;/p&gt;

&lt;p&gt;We run it with a custom username and password we want, as this will be exposed to the web and we don&amp;rsquo;t want the defaults to be exposed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## Run the docker image with RStudio and Hadley Universe
sudo docker run --name rstudio-server -d -p 8787:8787 \
     -e USER=YOUR_USERNAME -e PASSWORD=YOUR_PW \
     -v /mnt/data/:/home/ \
     rocker/hadleyverse
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;configure-rstudio-server-docker-image&#34;&gt;Configure RStudio Server Docker image&lt;/h3&gt;

&lt;p&gt;Now it could be that you are done from here - you should have a working RStudio interface available on the IP of your container (&lt;a href=&#34;http://your-vm-ip-address/rstudio/auth-sign-in&#34;&gt;http://your-vm-ip-address/rstudio/auth-sign-in&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;But we will configure it a bit more, adding users, more packages, and scheduled jobs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you have a lot of configurations then it is better to create your own &lt;a href=&#34;https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/&#34;&gt;DOCKERFILE&lt;/a&gt; and build the image yourself.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We log in to the running docker container here:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo docker exec -it rstudio-server bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a good one to remember for when you are doing future customisation.&lt;/p&gt;

&lt;p&gt;You are now in the Docker container.&lt;/p&gt;

&lt;p&gt;Install stuff then CTRL-D to come out again to commit and push your changes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## Make users that will create a directory on the data disk
adduser mark

## [Optional] Install packages
## install as sudo to ensure all user&#39;s have access
sudo su - -c &amp;quot;R -e \&amp;quot;install.packages(&#39;abc&#39;, repos=&#39;http://cran.rstudio.com/&#39;)\&amp;quot;&amp;quot;

## [Optional] Install libraries from Github packages
sudo su - -c &amp;quot;R -e \&amp;quot;devtools::install_github(&#39;MarkEdmondson1234/bigQueryR&#39;)\&amp;quot;&amp;quot;

## [Optional] Install libraries from private Github packages
sudo su - -c &amp;quot;R -e \&amp;quot;devtools::install_github(&#39;MarkEdmondson1234/privatePackge&#39;, auth_token=YOUR_GITHUB_PAT)\&amp;quot;&amp;quot;
````

### Configure scheduling via CRON

A big reason to have a server is for the team to schedule their data fetching scripts.  We achieve this by running CRON within the Docker container (to ensure all packages are installed) and then providing a link to a folder that runs the script when they need it.

First we install CRON in the container:

```sh
## Download and install cron
sudo apt-get update
sudo apt-get -y install cron nano
sudo service cron start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then schedule scripts via RScript to run daily.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## annoying nano bug in Docker
export TERM=xterm

## open up the cron editor (select 2)
sudo crontab -e

## add this to the bottom of file
## runs script at 0420 every day
# m h  dom mon dow   command
20 4 * * * /home/cron/r-cron.R &amp;gt;/home/cron/cronlog.log 2&amp;gt;&amp;amp;1

## CTRL-X and Y to save changes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;r-cron.R&lt;/code&gt; script needs to have &lt;code&gt;#!/usr/bin/Rscript&lt;/code&gt; at the top to run correctly.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#!/usr/bin/Rscript
cat(&amp;quot;Cron script started: &amp;quot;, date())

....do R stuff

cat(&amp;quot;Cron script stopped: &amp;quot;, date())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Repeat for all scripts you want to run.&lt;/p&gt;

&lt;h3 id=&#34;sending-emails&#34;&gt;Sending emails&lt;/h3&gt;

&lt;p&gt;Its useful to send an email once a script has run successfully (or not), one that uses Mailrun is below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Email a user a report is ready
#&#39;
#&#39; Requires an account at Mailgun: https://mailgun.com
#&#39; Pre-verification can only send to a whitelist of emails you configure
#&#39;
#&#39; @param email Email to send to
#&#39; @param mail_message Any extra info
#&#39;
#&#39; @return TRUE if successful email sent
#&#39; @import httr
#&#39; @export
sendEmail &amp;lt;- function(email = &amp;quot;XXXXX@you.com&amp;quot;,
                      mail_message = &amp;quot;Hello&amp;quot;){

  url &amp;lt;- &amp;quot;https://api.mailgun.net/v3/sandbox5f2XXXXXXXa.mailgun.org/messages&amp;quot;
  ## username:password so api_key is all after the api:
  api_key &amp;lt;- &amp;quot;key-c5957XXXXXXXXXXXbb9cf8ce&amp;quot;
  the_body &amp;lt;-
    list(
      from=&amp;quot;Mailgun Sandbox &amp;lt;postmaster@sandbox5XXXXXXXXa.mailgun.org&amp;gt;&amp;quot;,
      to=email,
      subject=&amp;quot;Mailgun from R&amp;quot;,
      text=mailmessage,
    )

  req &amp;lt;- httr::POST(url,
                    httr::authenticate(&amp;quot;api&amp;quot;, api_key),
                    encode = &amp;quot;form&amp;quot;,
                    body = the_body)

  httr::stop_for_status(req)
  
  TRUE

}

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;scheduling-packages&#34;&gt;Scheduling packages&lt;/h3&gt;

&lt;p&gt;There is also an R package that manages cron, &lt;a href=&#34;https://github.com/bnosac/cronR&#34;&gt;cronR&lt;/a&gt; , which now has an RStudio Server addin, which looks like a good option.&lt;/p&gt;

&lt;h2 id=&#34;pushing-the-docker-changes&#34;&gt;Pushing the Docker changes&lt;/h2&gt;

&lt;p&gt;We now commit and push changes to the &lt;a href=&#34;https://cloud.google.com/container-registry/docs/&#34;&gt;Google Docker Hub&lt;/a&gt;.  The Docker command docs are &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/commit/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## CTRL-D to come out of the docker container again

## to get the container id e.g. c3f279d17e0a
sudo docker ps 

## commit with a message
sudo docker commit -a &amp;quot;Mark&amp;quot; -m &amp;quot;Added R stuff&amp;quot; \
    CONTAINER_ID yourname/your-new-docker-image

## list your new image with the old
sudo docker images

## tag the image with the location
sudo docker tag yourname/your-new-docker-image \
                gcr.io/your-project-id/your-new-docker-image

## push to Google Docker registry
sudo gcloud docker push \
     gcr.io/your-project-id/your-new-docker-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You could now pull this image using:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo gcloud docker pull \
  gcr.io/your-project-id/your-new-image-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;but this will be taken care of in the startup behaviour below.&lt;/p&gt;

&lt;p&gt;Remember to commit any changes each time you change the configuration of RStudio.&lt;/p&gt;

&lt;h2 id=&#34;setup-restart-behaviour&#34;&gt;Setup restart behaviour&lt;/h2&gt;

&lt;p&gt;Now we want to configure the above to happen everytime the VM starts up.  I use a startup script for pulling the latest docker image and updating any packages or data from github so to refresh I just need to restart the server.&lt;/p&gt;

&lt;h3 id=&#34;download-latest-custom-packages-and-data&#34;&gt;Download latest custom packages and data&lt;/h3&gt;

&lt;p&gt;In the custom metadata for the VM, we need the field &lt;code&gt;startup-script&lt;/code&gt; and then optional other metadata.&lt;/p&gt;

&lt;p&gt;The metadata is kept seperate away from your running containers, but available via the Google metadata commands.  This can be used for things like passwords and security settings you would prefer not to be shipped in a Docker container, and is easier to manage - just edit the metadata keys.&lt;/p&gt;

&lt;p&gt;Create one metadata field per bash script variable - examples below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;github-user: your-githubname

github-repo: your-github-repo

github-data: your-github-data

github-pat: your-github-pat
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This startup script loads the metadata above and downloads custom R packages and data files from github.&lt;/p&gt;

&lt;p&gt;You can save this locally as &lt;code&gt;startup.sh&lt;/code&gt; and upload via &lt;code&gt;gcloud&lt;/code&gt; or paste it into the interface into a metadata field called &lt;code&gt;startup-script&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;#!/bin/bash
GH_USER=$(curl http://metadata/computeMetadata/v1/instance/attributes/github-user -H &amp;quot;Metadata-Flavor: Google&amp;quot;)
GH_REPO=$(curl http://metadata/computeMetadata/v1/instance/attributes/github-repo -H &amp;quot;Metadata-Flavor: Google&amp;quot;)
GH_DATA=$(curl http://metadata/computeMetadata/v1/instance/attributes/github-data -H &amp;quot;Metadata-Flavor: Google&amp;quot;)
GH_PAT=$(curl http://metadata/computeMetadata/v1/instance/attributes/github-pat -H &amp;quot;Metadata-Flavor: Google&amp;quot;)

## Run Docker image
docker run --name rstudio-server \
       -d -p 8787:8787 \
       -e USER=ADMIN_USERNAME \
       -e PASSWORD=ADMIN_PW \
       -v /mnt/data/:/home/ \
       gcr.io/your-project-name/your-image-name

## Data files: update from github
cd /mnt/data/user_name/project_name/data
git pull &#39;https://&#39;$GH_USER&#39;:&#39;$GH_PAT&#39;@github.com/&#39;$GH_USER&#39;/&#39;$GH_DATA&#39;.git&#39;

## Private packages 
# 1. pull from github
cd /mnt/data/R/privatePackageName
git pull &#39;https://&#39;$GH_USER&#39;:&#39;$GH_PAT&#39;@github.com/&#39;$GH_USER&#39;/&#39;$GH_REPO&#39;.git&#39;

# 2. Install the package from the local file you just updated from Git
sudo docker exec -it rstudio-server \
     sudo su - -c &amp;quot;R -e \&amp;quot;devtools::install(&#39;/home/R/localPackageName/&#39;)\&amp;quot;&amp;quot;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can debug your startup script by connecting to your instance and viewing &lt;code&gt;cat /var/log/startupscript.log&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You could also have a &lt;code&gt;shutdown-script&lt;/code&gt; that would execute before any shutdown.  Check out the &lt;a href=&#34;https://cloud.google.com/compute/docs/metadata&#34;&gt;list of metadata&lt;/a&gt; you can pass into scripts.&lt;/p&gt;

&lt;h2 id=&#34;launch&#34;&gt;Launch&lt;/h2&gt;

&lt;p&gt;Now we relaunch the newly configured VM from your local computer, to test the startup script works.  This will copy over all the configurations from the RStudio server above.&lt;/p&gt;

&lt;p&gt;Save the &lt;code&gt;startup.sh&lt;/code&gt; script to a file on your local machine, cd into the same folder and add the metadata:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud compute instances add-metadata your-r-server-name \
  --metadata-from-file startup-script=startup.sh \
  --metadata github-user=YOUR-USER,github-repo=YOUR_REPO,github-data=YOUR_DATA,github-pat=YOUR_PAT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now reset, cross fingers:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## reset
gcloud compute instances reset your-r-server-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a couple of minutes everything should now be running as configured before.&lt;/p&gt;

&lt;p&gt;If you want to stop or start the server again, use the below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## stop (no billing) but not deleted
gcloud compute instances stop your-r-server-name

## start up a stopped server
gcloud compute instances start your-r-server-name
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;the-future&#34;&gt;The future&lt;/h2&gt;

&lt;p&gt;This should put you in a great position to support R-scripts to the team, but also in a scalable way where starting up faster and bigger machines is just a case of updating configuration files.&lt;/p&gt;

&lt;p&gt;I would like to start up containers using the container manifest syntax but couldn&amp;rsquo;t get it to work for me yet, but for just one VM it means a few less lines in the start up script.&lt;/p&gt;

&lt;p&gt;We also have &lt;a href=&#34;http://iihnordic.dk/blog/posts/2016/marts/creating-a-content-recommendation-engine-using-r-opencpu-and-gtm&#34;&gt;OpenCPU&lt;/a&gt; and &lt;a href=&#34;https://www.rstudio.com/products/shiny/shiny-server/&#34;&gt;Shiny Server&lt;/a&gt; in the Google project, as we move into providing data output such as visualisations, APIs and dashboards.  They are setup in a similar fashion, just swap out the Docker image for the appropriate version you need.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
