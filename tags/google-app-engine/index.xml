<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Google App Engine on Mark Edmondson</title>
    <link>http://code.markedmondson.me/tags/google-app-engine/index.xml</link>
    <description>Recent content in Google App Engine on Mark Edmondson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-GB</language>
    <copyright>Powered by [Hugo](//gohugo.io). Theme by [PPOffice](http://github.com/ppoffice).</copyright>
    <atom:link href="http://code.markedmondson.me/tags/google-app-engine/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Four Ways to Schedule R scripts on Google Cloud Platform</title>
      <link>http://code.markedmondson.me/4-ways-schedule-r-scripts-on-google-cloud-platform</link>
      <pubDate>Sun, 09 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>http://code.markedmondson.me/4-ways-schedule-r-scripts-on-google-cloud-platform</guid>
      <description>&lt;p&gt;A common question I come across is how to automate scheduling of R scripts downloading data. This post goes through some options that I have played around with, which I’ve mostly used for downloading API data such as Google Analytics using the Google Cloud platform, but the same principles could apply for AWS or Azure.&lt;/p&gt;
&lt;!--more--&gt;
&lt;div id=&#34;scheduling-scripts-advice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scheduling scripts advice&lt;/h2&gt;
&lt;p&gt;But first, some notes on the scripts you are scheduling, that I’ve picked up.&lt;/p&gt;
&lt;div id=&#34;dont-save-data-to-the-scheduling-server&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Don’t save data to the scheduling server&lt;/h3&gt;
&lt;p&gt;I would suggest to not save or use data in the same place you are doing the scheduling. Use a service like BigQuery (&lt;code&gt;bigQueryR&lt;/code&gt;) or googleCloudStorageR (&lt;code&gt;googleCloudStorageR&lt;/code&gt;) to first load any necessary data, do your work then save it out again. This may be a bit more complicated to set up, but will save you tears if the VM or service goes down - you still have your data.&lt;/p&gt;
&lt;p&gt;To help with this, on Google Cloud you can authenticate with the same details you used to launch a VM to authenticate with the storage services above (as all are covered under the &lt;code&gt;http://www.googleapis.com/auth/cloud-services&lt;/code&gt; scope) - you can access this auth when on a GCE VM in R via &lt;code&gt;googleAuthR::gar_gce_auth()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;An example skeleton script is shown below that may be something you are scheduling.&lt;/p&gt;
&lt;p&gt;It downloads authentication files, does an API call, then saves it up to the cloud again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleAuthR)
library(googleCloudStorageR)

gcs_global_bucket(&amp;quot;my-bucket&amp;quot;)
## auth on the VM
options(googleAuthR.scopes.selected = &amp;quot;https://www.googleapis.com/auth/cloud-platform&amp;quot;)
gar_gce_auth()

## use the GCS auth to download the auth files for your API
auth_file &amp;lt;- &amp;quot;auth/my_auth_file.json&amp;quot;
gcs_get_object(auth_file, saveToDisk = TRUE)

## now auth with the file you just download
gar_auth_service(auth_file)

## do your work with APIs etc.
.....

## upload results back up to GCS (or BigQuery, etc.)
gcs_upload(my_results, name = &amp;quot;results/my_results.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;set-up-a-schedule-for-logs-too&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Set up a schedule for logs too&lt;/h3&gt;
&lt;p&gt;Logs are important for scheduled jobs, so you have some idea on whats happened when things go wrong. To help with scheduling debugging, most &lt;code&gt;googleAuthR&lt;/code&gt; packages now have a timestamp on their output messages.&lt;/p&gt;
&lt;p&gt;You can send the output of your scripts to log files, if using cron and RScript it looks something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;RScript /your-r-script.R &amp;gt; your-r-script.log&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;…where &lt;code&gt;&amp;gt;&lt;/code&gt; sends the output to the new file.&lt;/p&gt;
&lt;p&gt;Over time though, this can get big and (sigh) fill up your disk so you can’t log in to the VM (speaking from experience here!) so I now set up another scheduled job that every week takes the logs and uploads to GCS, then deletes the current ones.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;consider-using-docker-for-environments&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Consider using Docker for environments&lt;/h3&gt;
&lt;p&gt;Several of the methods below use &lt;a href=&#34;https://www.docker.com/&#34;&gt;&lt;code&gt;Docker&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The reasons for that is Docker provides a nice reprodueable way to define exactly what packages and dependencies you need for your script to run, which can run on top of any type of infrastructure as &lt;code&gt;Docker&lt;/code&gt; has quickly become a cloud standard.&lt;/p&gt;
&lt;p&gt;For instance, migrating from Google Cloud to AWS is much easier if both can be deployed using Docker, and below Docker is instrumental in allowing you to run on multiple solutions.&lt;/p&gt;
&lt;p&gt;Bear in mind that when a Docker container relaunches it won’t save any data, so any non-saved state will be lost (you should make a new container if you need it to contain data), but you’re not saving your data to the docker container anyway, aren’t you?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;scheduling-options---pros-and-cons&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scheduling options - Pros and cons&lt;/h2&gt;
&lt;p&gt;Here is an overview of the pros and cons of the options presented in more detail below:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;29%&#34; /&gt;
&lt;col width=&#34;35%&#34; /&gt;
&lt;col width=&#34;35%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Pros&lt;/th&gt;
&lt;th&gt;Cons&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1. &lt;code&gt;cronR&lt;/code&gt; Addin on &lt;code&gt;RStudio Server&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Simple and quick&lt;/td&gt;
&lt;td&gt;Not so robust, need to log into server to make changes, versioning packages.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2. &lt;code&gt;gce_vm_scheduler&lt;/code&gt; and &lt;code&gt;Dockerfiles&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Robust and can launch from local R session, support versioning&lt;/td&gt;
&lt;td&gt;Need to build Dockerfiles, all scripts on one VM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;3. Master &amp;amp; Slave VM&lt;/td&gt;
&lt;td&gt;Tailor a fresh VM for each script, cheaper&lt;/td&gt;
&lt;td&gt;Need to build Dockerfiles, more complicated VM setup.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;4. Google AppEngine with flexible containers&lt;/td&gt;
&lt;td&gt;Managed platform&lt;/td&gt;
&lt;td&gt;Need to turn script into web responses, more complicated setup&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;cronr-plus-rstudio-server&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1 - cronR plus RStudio Server&lt;/h2&gt;
&lt;p&gt;This is the simplest and the one to start with.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Start up an RStudio Server instance&lt;/li&gt;
&lt;li&gt;Install &lt;a href=&#34;https://github.com/bnosac/cronR&#34;&gt;&lt;code&gt;cronR&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Upload your R script&lt;/li&gt;
&lt;li&gt;Schedule your script using &lt;code&gt;cronR&lt;/code&gt; RStudio addin&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With &lt;code&gt;googleComputeEngineR&lt;/code&gt; and the new &lt;code&gt;gcer-public&lt;/code&gt; project containing public images that include one with &lt;code&gt;cronR&lt;/code&gt; already installed, this is as simple as the few lines of code below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleComputeEngineR)

## get the tag for prebuilt Docker image with googleAuthRverse, cronR and tidyverse
tag &amp;lt;- gce_tag_container(&amp;quot;google-auth-r-cron-tidy&amp;quot;, project = &amp;quot;gcer-public&amp;quot;)
# gcr.io/gcer-public/google-auth-r-cron-tidy

## start a custom Rstudio instance
vm &amp;lt;- gce_vm(name = &amp;quot;my-rstudio&amp;quot;,
              predefined_type = &amp;quot;n1-highmem-8&amp;quot;,
              template = &amp;quot;rstudio&amp;quot;,
              dynamic_image = tag,
              username = &amp;quot;me&amp;quot;, password = &amp;quot;mypassword&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait for it to launch and give you an IP, then log in, upload a script and configure the schedule via the &lt;code&gt;cronR&lt;/code&gt; addin.&lt;/p&gt;
&lt;p&gt;Some more detail about this workflow can be found at these &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/rstudio-team.html&#34;&gt;custom RStudio example workflows&lt;/a&gt; on the &lt;code&gt;googleComputeEngineR&lt;/code&gt; website.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gce_vm_scheduler-and-dockerfiles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2- &lt;em&gt;gce_vm_scheduler&lt;/em&gt; and &lt;em&gt;Dockerfiles&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;This method I prefer to the above since it lets you create the exact environment (e.g. package versions, dependencies) to run your script in, that you can trail dev and production versions with. It also works locally without needing to log into the server each time to deploy a script.&lt;/p&gt;
&lt;div id=&#34;handy-tools-for-docker---containerit-and-build-triggers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Handy tools for Docker - containerit and Build Triggers&lt;/h3&gt;
&lt;p&gt;Here we introduce Docker images, which may have been more a technical barrier for some before (but worth knowing, I think)&lt;/p&gt;
&lt;div id=&#34;containerit&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;containerit&lt;/h4&gt;
&lt;p&gt;Things are much easier now though, as we have the magic new R package &lt;a href=&#34;http://o2r.info/2017/05/30/containerit-package/&#34;&gt;&lt;code&gt;containerit&lt;/code&gt;&lt;/a&gt; which can generate these Docker files for you - just send &lt;code&gt;containerit::dockerfile()&lt;/code&gt; around the script file.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build-triggers&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Build Triggers&lt;/h4&gt;
&lt;p&gt;Along with auto-generating Dockerfiles, for Google Cloud in particular we now also have &lt;a href=&#34;https://cloud.google.com/container-builder/docs/how-to/build-triggers&#34;&gt;Build Triggers&lt;/a&gt; which automates building the Docker image for you.&lt;/p&gt;
&lt;p&gt;Just make the Dockerfile, then set up a trigger for when you push that file up to GitHub - you can see the ones used to create the public R resources here in the &lt;a href=&#34;https://github.com/cloudyr/googleComputeEngineR/tree/master/inst/dockerfiles&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt; repo&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;recipe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Recipe&lt;/h3&gt;
&lt;p&gt;Putting it all together then, documentation of this workflow for &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/scheduled-rscripts.html&#34;&gt;scheduling R scripts is found here&lt;/a&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If you don’t already have one, start up a scheduler VM using &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/gce_vm_scheduler.html&#34;&gt;&lt;code&gt;gce_vm_scheduler&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Create a Dockerfile either manually or using &lt;code&gt;containerit&lt;/code&gt; that will run your script upon execution&lt;/li&gt;
&lt;li&gt;Upload the Dockerfile to a git repo (private or public)&lt;/li&gt;
&lt;li&gt;Setup a build trigger for that Dockerfile&lt;/li&gt;
&lt;li&gt;Once built, set a script to schedule within that Dockerfile with &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/gce_schedule_docker.html&#34;&gt;&lt;code&gt;gce_schedule_docker&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is still in beta at time of writing but should be stable by the time &lt;code&gt;googlecomputeEngineR&lt;/code&gt; hits CRAN &lt;code&gt;0.2.0&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;master-and-slave-vms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3 - Master and Slave VMs&lt;/h2&gt;
&lt;p&gt;Some scripts take more resources than others, and since you are using VMs already you can have more control over what specifications of VM to launch based on the script you want to run.&lt;/p&gt;
&lt;p&gt;This means you can have a cheap scheduler server, that launch biggers VMs for the duration of the job. As GCP charges per minute, this can save you money over having a schedule server that is as big as what your most expensive script needs running 24/7.&lt;/p&gt;
&lt;p&gt;This method is largely like the scheduled scripts above, except in this case the scheduled script is also launching VMs to run the job upon.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;googleCloudStorageR::gcs_source&lt;/code&gt; you can run an R script straight from where it is hosted upon GCS, meaning all data, authentication files and scripts can be kept seperate from the computation. An example master script is shown below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## intended to be run on a small instance via cron
## use this script to launch other VMs with more expensive tasks
library(googleComputeEngineR)
library(googleCloudStorageR)
gce_global_project(&amp;quot;my-project&amp;quot;)
gce_global_zone(&amp;quot;europe-west1-b&amp;quot;)
gcs_global_bucket(&amp;quot;your-gcs-bucket&amp;quot;)

## auth to same project we&amp;#39;re on
googleAuthR::gar_gce_auth()

## launch the premade VM
vm &amp;lt;- gce_vm(&amp;quot;slave-1&amp;quot;)

## set SSH to use &amp;#39;master&amp;#39; username as configured before
vm &amp;lt;- gce_ssh_setup(vm, username = &amp;quot;master&amp;quot;, ssh_overwrite = TRUE)

## run the script on the VM that will source from GCS
runme &amp;lt;- &amp;quot;Rscript -e \&amp;quot;googleAuthR::gar_gce_auth();googleCloudStorageR::gcs_source(&amp;#39;download.R&amp;#39;, bucket = &amp;#39;your-gcs-bucket&amp;#39;)\&amp;quot;&amp;quot;
out &amp;lt;- docker_cmd(vm, 
                  cmd = &amp;quot;exec&amp;quot;, 
                  args = c(&amp;quot;rstudio&amp;quot;, runme), 
                  wait = TRUE)

## once finished, stop the VM
gce_vm_stop(vm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More detail is again available at the &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/scheduled-rscripts.html#master-slave-scheduler&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;google-app-engine-with-flexible-custom-runtimes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4 - Google App Engine with flexible custom runtimes&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/appengine/docs/&#34;&gt;Google App Engine&lt;/a&gt; has always had schedule options, but only for its supported languages of Python, Java, PHP etc. Now with the &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/&#34;&gt;introduction of flexible containers&lt;/a&gt;, any Docker container running any language (including R) can also be run.&lt;/p&gt;
&lt;p&gt;This is potentially the best solution since it runs upon a 100% managed platform, meaning you don’t need to worry about servers at all, and it takes care of things like server maintence, logging etc.&lt;/p&gt;
&lt;div id=&#34;setting-up-your-script-for-app-engine&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting up your script for App Engine&lt;/h3&gt;
&lt;p&gt;There are some &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/build&#34;&gt;requirements for the container&lt;/a&gt; that need configuring so it can run:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can not use &lt;code&gt;googleAuthR::gar_gce_auth()&lt;/code&gt; so will need to upload the auth token within the Dockerfile.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;AppEngine expects a web service to be listening on port 8080, so your schedule script needs to be triggered via HTTP requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For authentication, I use the system environment arguments (i.e. those usually set in &lt;code&gt;.Renviron&lt;/code&gt;) that &lt;code&gt;googleAuthR&lt;/code&gt; packages use for auto-authentication. Put the auth file (such as JSON or a &lt;code&gt;.httr-oauth&lt;/code&gt; file) into the deployment folder, then point to its location via specifying in the &lt;code&gt;app.yaml&lt;/code&gt;. Details below.&lt;/p&gt;
&lt;p&gt;To solve the need for being a webservice on port 8080 (which is then proxied to normal webports 80/443), &lt;a href=&#34;https://www.rplumber.io/&#34;&gt;&lt;code&gt;plumber&lt;/code&gt;&lt;/a&gt; is a great service by Jeff Allen of RStudio, which already comes with its own Docker solution. You can then modify that &lt;code&gt;Dockerfile&lt;/code&gt; slightly so that it works on App Engine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;recipe-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Recipe&lt;/h3&gt;
&lt;p&gt;To then schedule your R script on app engine, follow the guide below, first making sure you have setup the &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/&#34;&gt;gcloud CLI&lt;/a&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create a Google Appengine project in the US region (only region that supports flexible containers at the moment)&lt;/li&gt;
&lt;li&gt;Create a scheduled script e.g. &lt;code&gt;schedule.R&lt;/code&gt; - you can use auth from environment files specified in &lt;code&gt;app.yaml&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Make an API out of the script by using &lt;code&gt;plumber&lt;/code&gt; - example:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleAuthR)         ## authentication
library(googleCloudStorageR)  ## google cloud storage
library(readr)                ## 
## gcs auto authenticated via environment file 
## pointed to via sys.env GCS_AUTH_FILE

#* @get /demoR
demoScheduleAPI &amp;lt;- function(){
  
  ## download or do something
  something &amp;lt;- tryCatch({
      gcs_get_object(&amp;quot;schedule/test.csv&amp;quot;, 
                     bucket = &amp;quot;mark-edmondson-public-files&amp;quot;)
    }, error = function(ex) {
      NULL
    })
      
  something_else &amp;lt;- data.frame(X1 = 1,
                               time = Sys.time(), 
                               blah = paste(sample(letters, 10, replace = TRUE), collapse = &amp;quot;&amp;quot;))
  something &amp;lt;- rbind(something, something_else)
  
  tmp &amp;lt;- tempfile(fileext = &amp;quot;.csv&amp;quot;)
  on.exit(unlink(tmp))
  write.csv(something, file = tmp, row.names = FALSE)
  ## upload something
  gcs_upload(tmp, 
             bucket = &amp;quot;mark-edmondson-public-files&amp;quot;, 
             name = &amp;quot;schedule/test.csv&amp;quot;)
  
  message(&amp;quot;Done&amp;quot;, Sys.time())
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create Dockerfile. If using &lt;code&gt;containerit&lt;/code&gt; then replace FROM with &lt;code&gt;trestletech/plumber&lt;/code&gt; and add the below lines to use correct AppEngine port:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(containerit)

dockerfile &amp;lt;- dockerfile(&amp;quot;schedule.R&amp;quot;, copy = &amp;quot;script_dir&amp;quot;, soft = TRUE)
write(dockerfile, file = &amp;quot;Dockerfile&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then change/add these lines to the created Dockerfile:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;EXPOSE 8080
ENTRYPOINT [&amp;quot;R&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;pr &amp;lt;- plumber::plumb(commandArgs()[4]); pr$run(host=&amp;#39;0.0.0.0&amp;#39;, port=8080)&amp;quot;]
CMD [&amp;quot;schedule.R&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Example final Dockerfile below. This doesn’t need to be built in say a build trigger as its built upon app engine deployment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FROM trestletech/plumber
LABEL maintainer=&amp;quot;mark&amp;quot;
RUN export DEBIAN_FRONTEND=noninteractive; apt-get -y update \
 &amp;amp;&amp;amp; apt-get install -y libcairo2-dev \
    libcurl4-openssl-dev \
    libgmp-dev \
    libpng-dev \
    libssl-dev \
    libxml2-dev \
    make \
    pandoc \
    pandoc-citeproc \
    zlib1g-dev
RUN [&amp;quot;install2.r&amp;quot;, &amp;quot;-r &amp;#39;https://cloud.r-project.org&amp;#39;&amp;quot;, &amp;quot;readr&amp;quot;, &amp;quot;googleCloudStorageR&amp;quot;, &amp;quot;Rcpp&amp;quot;, &amp;quot;digest&amp;quot;, &amp;quot;crayon&amp;quot;, &amp;quot;withr&amp;quot;, &amp;quot;mime&amp;quot;, &amp;quot;R6&amp;quot;, &amp;quot;jsonlite&amp;quot;, &amp;quot;xtable&amp;quot;, &amp;quot;magrittr&amp;quot;, &amp;quot;httr&amp;quot;, &amp;quot;curl&amp;quot;, &amp;quot;testthat&amp;quot;, &amp;quot;devtools&amp;quot;, &amp;quot;hms&amp;quot;, &amp;quot;shiny&amp;quot;, &amp;quot;httpuv&amp;quot;, &amp;quot;memoise&amp;quot;, &amp;quot;htmltools&amp;quot;, &amp;quot;openssl&amp;quot;, &amp;quot;tibble&amp;quot;, &amp;quot;remotes&amp;quot;]
RUN [&amp;quot;installGithub.r&amp;quot;, &amp;quot;MarkEdmondson1234/googleAuthR@7917351&amp;quot;, &amp;quot;hadley/rlang@ff87439&amp;quot;]
WORKDIR /payload/
COPY [&amp;quot;.&amp;quot;, &amp;quot;./&amp;quot;]

EXPOSE 8080
ENTRYPOINT [&amp;quot;R&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;pr &amp;lt;- plumber::plumb(commandArgs()[4]); pr$run(host=&amp;#39;0.0.0.0&amp;#39;, port=8080)&amp;quot;]
CMD [&amp;quot;schedule.R&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Specify &lt;code&gt;app.yaml&lt;/code&gt; for flexible containers as &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/&#34;&gt;detailed here&lt;/a&gt;. Add any environment vars such as auth files, that will be included in same deployment folder.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;runtime: custom
env: flex

env_variables:
  GCS_AUTH_FILE: auth.json&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Specify &lt;code&gt;cron.yaml&lt;/code&gt; for the schedule needed:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;cron:
- description: &amp;quot;test cron&amp;quot;
  url: /demoR
  schedule: every 1 hours&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;You should now have these files in the deployment folder:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;app.yaml&lt;/code&gt; - configuration of general app settings&lt;/li&gt;
&lt;li&gt;&lt;code&gt;auth.json&lt;/code&gt; - an authentication file specified in env arguments or app.yaml&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cron.yaml&lt;/code&gt; - specification of when your scheduling is&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Dockerfile&lt;/code&gt; - specification of the environment&lt;/li&gt;
&lt;li&gt;&lt;code&gt;schedule.R&lt;/code&gt; - the plumber version of your script containing your endpoints&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Open the terminal in that folder, and deploy via &lt;code&gt;gcloud app deploy --project your-project&lt;/code&gt; and the cron schedule via &lt;code&gt;gcloud app deploy cron.yaml --project your-project&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It will take a while (up to 10 mins) the first time.&lt;/p&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The App Engine should then be deployed on &lt;a href=&#34;https://your-project.appspot.com/&#34; class=&#34;uri&#34;&gt;https://your-project.appspot.com/&lt;/a&gt; - every &lt;code&gt;GET&lt;/code&gt; request to &lt;a href=&#34;https://your-project.appspot.com/demoR&#34; class=&#34;uri&#34;&gt;https://your-project.appspot.com/demoR&lt;/a&gt; (or other endpoints you have specified in R script) will run the R code. The cron example above will run every hour to this endpoint.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Logs for the instance are found &lt;a href=&#34;https://console.cloud.google.com/logs/viewer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This approach is the most flexible, and offers a fully managed platform for your scripts. Scheduled scripts are only the beginning, since deploying such actually gives you a way to run R scripts in response to any HTTP request from any language - triggers could also include if someone updates a spreadsheet, adds a file to a folder, pushes to GitHub etc. which opens up a lot of exciting possibilities. You can also scale it up to become a fully functioning R API.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Hopefully this has given you an idea on your options for R on Google Cloud regarding scheduling. If you have some other easier workflows or suggestions for improvements please put them in the comments below!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Real-time forecasting dashboard with Google Tag Manager, Google Cloud and R Shiny - Part two</title>
      <link>http://code.markedmondson.me/real-time-GTM-google-cloud-r-shiny-2/</link>
      <pubDate>Sun, 22 Jan 2017 14:20:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/real-time-GTM-google-cloud-r-shiny-2/</guid>
      <description>

&lt;p&gt;In part two of this two part series we walk through the steps to stream data from a &lt;a href=&#34;https://www.google.com/analytics/tag-manager/&#34;&gt;Google Tag Manager&lt;/a&gt; (GTM) implementation into a &lt;a href=&#34;https://cloud.google.com/appengine/&#34;&gt;Google App Engine&lt;/a&gt; (GAE) web app, which then adds data to a &lt;a href=&#34;https://cloud.google.com/bigquery/&#34;&gt;BigQuery&lt;/a&gt; table via BigQuery&amp;rsquo;s data streaming capability.  In part two, we go into how to query that table in realtime from R, make a forecast using R, then visualise it in &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt; and the JavaScript visualisation library &lt;a href=&#34;http://jkunst.com/highcharter/&#34;&gt;Highcharts&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Read &lt;a href=&#34;http://code.markedmondson.me/real-time-GTM-google-cloud-r-shiny-1/&#34;&gt;part one here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The project combines several languages where their advantages lie: Python for its interaction with Google APIs and its quick start creating your own API on App Engine, SQL to query the BigQuery data itself, R for its forecasting libraries and the reactive Shiny framework, and JavaScript for the visualisation and data capture at the Google Tag Manager end.&lt;/p&gt;

&lt;h2 id=&#34;thanks&#34;&gt;Thanks&lt;/h2&gt;

&lt;p&gt;This project wouldn&amp;rsquo;t have been possible without the help of the excellent work gone beforehand by &lt;a href=&#34;https://www.analyticspros.com/blog/data-science/streaming-prebid-data-google-bigquery/&#34;&gt;Luke Cempre&amp;rsquo;s post on AnalyticsPros&lt;/a&gt; for streaming data from Google Tag Manager to BigQuery, and &lt;a href=&#34;http://jkunst.com/&#34;&gt;Joshua Kunst&lt;/a&gt; for his help with the Highcharts JavaScript.&lt;/p&gt;

&lt;h2 id=&#34;data-flows&#34;&gt;Data flows&lt;/h2&gt;

&lt;p&gt;There are two data flows in this project.  The first adds data to BigQuery:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;GTM collects data into its dataLayer from a web visit.&lt;/li&gt;
&lt;li&gt;A custom HTML tag in GTM collects the data you want to stream then calls an App Engine URL with its data payload.&lt;/li&gt;
&lt;li&gt;The app engine URL is sent to a queue to add the data to BigQuery.&lt;/li&gt;
&lt;li&gt;The data plus a timestamp is put into a BigQuery row.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then to read the data:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The Shiny app calls Big Query every X seconds.&lt;/li&gt;
&lt;li&gt;The data is aggregated&lt;/li&gt;
&lt;li&gt;A forecast is made with the updated data.&lt;/li&gt;
&lt;li&gt;The Highcharts visualisation reads the changing dataset, and updates the visualisation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This blog will cover the second. A demo &lt;a href=&#34;https://github.com/MarkEdmondson1234/realtimeShiny&#34;&gt;Github repo for the Shiny app is available here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;calling-data-your-options&#34;&gt;Calling data: your options&lt;/h2&gt;

&lt;p&gt;The Google App Engine app on Github includes functions to both read and write data from BigQuery.  You can either call the data via the app engine app, which in turn reads the data via the Python BigQuery library, or if you are using a platform that supports reading the data from BigQuery then you can use that directly.&lt;/p&gt;

&lt;p&gt;In most cases, you will be better off with the latter, as you will be cutting out the middle man.  In some cases the app engine will time out so if you are using it you should make sure your app can handle null results.  But it is useful to have, for those platforms that do not have Big query SDKs, such as some visualisation BI tools.&lt;/p&gt;

&lt;h3 id=&#34;option-1-google-app-engine-reading-realtime-data-from-bigquery&#34;&gt;Option 1 - Google App Engine: Reading realtime data from BigQuery&lt;/h3&gt;

&lt;p&gt;The full code for reading and writing data is available at the &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream&#34;&gt;supporting Github repo here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The first blog in this series went through its data input, we now look at the data output.  In production this may be separated out into a different app, but for brevity its here in the same application.&lt;/p&gt;

&lt;p&gt;We first define some environmental variables in the &lt;code&gt;app.yaml&lt;/code&gt; setup file, with the dataset and table and a secret code word:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#[START env]
env_variables:
  DATASET_ID: tests
  TABLE_ID: realtime_markedmondsonme
  SECRET_SALT: change_this_to_something_unique
#[END env]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first function below then queries the BigQuery table we defined in the environmental variables, and turns it into JSON.  By default it will get the last row, or you can pass in the &lt;code&gt;limit&lt;/code&gt; argument to get more rows, or your own &lt;code&gt;q&lt;/code&gt; argument with custom SQL to query the table directly:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# queries and turns into JSON
def get_data(q, limit = 1):
	datasetId = os.environ[&#39;DATASET_ID&#39;]
	tableId   = os.environ[&#39;TABLE_ID&#39;]

	if len(q) &amp;gt; 0:
		query = q % (datasetId, tableId)
	else:
		query = &#39;SELECT * FROM %s.%s ORDER BY ts DESC LIMIT %s&#39; % (datasetId, tableId, limit)

	bqdata = sync_query(query)

	return json.dumps(bqdata)
	
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This next class is called when the &amp;ldquo;get data&amp;rdquo; URL is requested.  A lot of headers are set to ensure no browser caching is done which we don&amp;rsquo;t want since this is a realtime feed.&lt;/p&gt;

&lt;p&gt;For security, we also test via a &lt;code&gt;hash&lt;/code&gt; parameter to make sure its an authorised request, and decide how much data to return via the &lt;code&gt;limit&lt;/code&gt; parameter.&lt;/p&gt;

&lt;p&gt;Finally we call the function above and write that out to the URL response.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class QueryTable(webapp2.RequestHandler):

	def get(self):

    # no caching
		self.response.headers.add_header(&amp;quot;Access-Control-Allow-Origin&amp;quot;, &amp;quot;*&amp;quot;)
		self.response.headers.add_header(&amp;quot;Pragma&amp;quot;, &amp;quot;no-cache&amp;quot;)
		self.response.headers.add_header(&amp;quot;Cache-Control&amp;quot;, &amp;quot;no-cache, no-store, must-revalidate, pre-check=0, post-check=0&amp;quot;)
		self.response.headers.add_header(&amp;quot;Expires&amp;quot;, &amp;quot;Thu, 01 Dec 1994 16:00:00&amp;quot;)
		self.response.headers.add_header(&amp;quot;Content-Type&amp;quot;, &amp;quot;application/json&amp;quot;)

		q      = cgi.escape(self.request.get(&amp;quot;q&amp;quot;))
		myhash = cgi.escape(self.request.get(&amp;quot;hash&amp;quot;))
		limit  = cgi.escape(self.request.get(&amp;quot;limit&amp;quot;))

		salt = os.environ[&#39;SECRET_SALT&#39;]
		test = hashlib.sha224(q+salt).hexdigest()

		if(test != myhash):
			logging.debug(&#39;Expected hash: {}&#39;.format(test))
			logging.error(&amp;quot;Incorrect hash&amp;quot;)
			return

		if len(limit) == 0:
			limit = 1

		self.response.out.write(get_data(q, limit))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To find the hash, you can run this line and copy the result:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
q = &amp;quot;&amp;quot; # change to the query you use directly if you donøt use the default
salt = os.environ[&#39;SECRET_SALT&#39;] # the secret passphrase
test = hashlib.sha224(q+salt).hexdigest()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;or easier is to just run one call, then check the logs for the hash in the debug messages:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://code.markedmondson.me/images/secrethash.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;full-app-engine-script&#34;&gt;Full App Engine Script&lt;/h3&gt;

&lt;p&gt;The full script uploaded is available in the Github repository here:  &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream/blob/master/main.py&#34;&gt;main.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;With this script you then need some configuration files for the app and upload it to your Google Project.  A guide on how to deploy this is and more is available from the &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream/blob/master/README.md&#34;&gt;Github repository README&lt;/a&gt;, but once done the app will be available at &lt;code&gt;https://YOUR-PROJECT-ID.appspot.com&lt;/code&gt; and you will call the &lt;code&gt;/bq-streamer&lt;/code&gt; and &lt;code&gt;/bq-get&lt;/code&gt; URLs to send and get data.&lt;/p&gt;

&lt;h3 id=&#34;option-2-calling-bigquery-directly&#34;&gt;Option 2 - Calling BigQuery directly&lt;/h3&gt;

&lt;p&gt;This is the preferred method, as it will be on average 5-10 seconds quicker to get your results, and avoid timeouts.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m using Shiny as it fits into existing work, but you may prefer to use some of the other &lt;a href=&#34;https://cloud.google.com/bigquery/docs/reference/libraries&#34;&gt;BigQuery SDKs out there&lt;/a&gt;. I&amp;rsquo;ll be using &lt;a href=&#34;https://github.com/cloudyr/bigQueryR&#34;&gt;Github version of bigQueryR&lt;/a&gt; &lt;code&gt;&amp;gt; 0.2.0.9000&lt;/code&gt; as it has support for non-cached queries that are needed to see the tables update in realtime.&lt;/p&gt;

&lt;p&gt;A demo &lt;a href=&#34;https://github.com/MarkEdmondson1234/realtimeShiny&#34;&gt;Github repo for the Shiny app is available here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this case, the function in R that is in the Shiny &lt;code&gt;server.R&lt;/code&gt; is below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(bigQueryR)


do_bq &amp;lt;- function(limit){

  ## authenticate offline first, upload the .httr-oauth token
  bqr_auth()
  q &amp;lt;- sprintf(&amp;quot;SELECT * FROM [big-query-r:tests.realtime_markedmondsonme] ORDER BY ts DESC LIMIT %s&amp;quot;, 
               limit)
  
  bqr_query(projectId = &amp;quot;big-query-r&amp;quot;, 
            datasetId = &amp;quot;tests&amp;quot;, 
            query = q, 
            useQueryCache = FALSE)  
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;bqr_auth()&lt;/code&gt; if first run offline to generate the authentication token, which is then uploaded with the app.  Alternatively you can use the &lt;a href=&#34;http://code.markedmondson.me/googleAuthR/reference/gar_auth_service.html&#34;&gt;JSON service auth&lt;/a&gt; or if deploying on Google Compute Engine the automatic Google Cloud auth features of googleAuthR (&lt;code&gt;googleAuthR::gar_gce_auth()&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;The query itself selects all the columns, and orders by the timestamp we supplied in the input post. The function has a parameter so you can select how many rows to collect, which we use later.&lt;/p&gt;

&lt;p&gt;Note the use of &lt;code&gt;useQueryCache = FALSE&lt;/code&gt; to ensure you always get the freshest results.  If this wasn&amp;rsquo;t selected queries of the same type will return the first result they queried, which is no good for these purposes.&lt;/p&gt;

&lt;h2 id=&#34;reactivepoll-the-shiny-realtime-function&#34;&gt;reactivePoll - the Shiny realtime function&lt;/h2&gt;

&lt;p&gt;For realtime applications, &lt;code&gt;shiny::reactivePoll()&lt;/code&gt; is a function that periodically checks a datasource for changes.&lt;/p&gt;

&lt;p&gt;Now, what constitutes &amp;ldquo;realtime&amp;rdquo; is debatable here - for my applications I really only need an update every ~30 seconds.  Practically the Shiny output dims when updating with data, so for periods less than say 10 seconds it may not be the best approach for you - updating directly via JavaScript libraries may be better, and rely on say OpenCPU to provide the forecasting or another JS library.&lt;/p&gt;

&lt;p&gt;However, for my purposes I just need something better than the Google Analytics 4-hour lag in data (for GA360) and this suits well, particularly as you can apply a whole host of R data functions to the output.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;reactivePoll&lt;/code&gt; needs to be supplied with two functions: one to check if the data has changed, the other to make the complete fetch once a change is detected.  For these we just check if the timestamp of the last entry has changed, and if so, then fetch the last 1000 results to make the prediction:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## get 1000 rows of data
get_bq &amp;lt;- function(){

  message(&amp;quot;Getting new data...&amp;quot;)
  check &amp;lt;- do_bq(1000)
  
  rt &amp;lt;- as.data.frame(check, stringsAsFactors = FALSE)
  names(rt) &amp;lt;- c(&amp;quot;pageURL&amp;quot;,&amp;quot;Referrer&amp;quot;,&amp;quot;ts&amp;quot;)
  
  ## turn string into JS timestamp
  rt$timestamp &amp;lt;- as.POSIXct(as.numeric(as.character(rt$ts)), origin=&amp;quot;1970-01-01&amp;quot;)
  
  rt
}

## get 1 row of data, output its timestamp
check_bq &amp;lt;- function(){
  
  check &amp;lt;- do_bq(1)
  
  message(&amp;quot;Checking....check$ts: &amp;quot;, check$ts)
  check$ts
  
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is then called in the Shiny server function like so, in this case every 5 seconds:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;shinyServer(function(input, output, session) {
  
  ## checks every 5 seconds for changes
  realtime_data &amp;lt;- reactivePoll(5000, 
                                session, 
                                checkFunc = check_bq, 
                                valueFunc = get_bq)

  ### ... do stuff with realtime_data() ...

}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;transforming-data&#34;&gt;Transforming data&lt;/h2&gt;

&lt;p&gt;We then need to make the forecast, and put the data into the correct format it can be used in the chosen visualisation library, &lt;a href=&#34;http://jkunst.com/highcharter/&#34;&gt;highcharter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The next function takes the output of the &lt;code&gt;realtime_data()&lt;/code&gt; function, aggregates per hour (my lowly blog hasn&amp;rsquo;t enough data to make it worth doing per minute, but YNMV), turns the aggregation into time series objects suitable for the forecast and highcharts functions, then outputs a list.&lt;/p&gt;

&lt;p&gt;In this case I have chosen a very simple forecast function using all the defaults of &lt;code&gt;forecast::forecast()&lt;/code&gt; but this should be tweaked to your particular use cases, such as taking more account of seasonality and so forth.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;transform_rt &amp;lt;- function(rt){
  ## aggregate per hour
  rt_agg &amp;lt;- rt %&amp;gt;% 
    mutate(hour = format(timestamp, format = &amp;quot;%Y-%m-%d %H:00&amp;quot;)) %&amp;gt;% 
    count(hour)
  
  rt_agg$hour &amp;lt;- as.POSIXct(rt_agg$hour, origin=&amp;quot;1970-01-01&amp;quot;)
  
  # ## the number of hits per timestamp
  rt_xts &amp;lt;- xts::xts(rt_agg$n, frequency = 24, order.by = rt_agg$hour)
  rt_ts &amp;lt;- ts(rt_agg$n, frequency = 24)
  
  list(forecast = forecast::forecast(rt_ts, h = 12),
       xts = rt_xts)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All that remains now is to apply this transformation to new data as it appears (e.g. for each new visit, the hourly aggregate for the last hour increases, and the forecast updates)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  plot_data &amp;lt;- reactive({
    
    req(realtime_data())
    rt &amp;lt;- realtime_data()
    message(&amp;quot;plot_data()&amp;quot;)
    ## aggregate
    transform_rt(rt)
    
  })
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;output-to-highcharts&#34;&gt;Output to Highcharts&lt;/h2&gt;

&lt;p&gt;The final output to Highcharts has been tweaked a bit to get the prediction intervals and so forth:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  output$hc &amp;lt;- renderHighchart({
    
    req(plot_data())
    ## forcast values object
    fc &amp;lt;- plot_data()$forecast
    
    ## original data
    raw_data &amp;lt;- plot_data()$xts
    
    # plot last 48 hrs only, although forecast accounts for all data
    raw_data &amp;lt;- tail(raw_data, 48)
    raw_x_date &amp;lt;- as.numeric(index(raw_data)) * 1000
    
    ## start time in JS time
    forecast_x_start &amp;lt;- as.numeric(index(raw_data)[length(raw_data)])*1000
    ## each hour after that in seconds, 
    forecast_x_sequence &amp;lt;- seq(3600000, by = 3600000, length.out = 12)
    ## everything * 1000 to get to Javascript time
    forecast_times &amp;lt;- as.numeric(forecast_x_start + forecast_x_sequence)
    
    forecast_values &amp;lt;- as.numeric(fc$mean)
    
    hc &amp;lt;- highchart() %&amp;gt;%
      hc_chart(zoomType = &amp;quot;x&amp;quot;) %&amp;gt;%
      hc_xAxis(type = &amp;quot;datetime&amp;quot;) %&amp;gt;% 
      hc_add_series(type = &amp;quot;line&amp;quot;,
                    name = &amp;quot;data&amp;quot;,
                    data = list_parse2(data.frame(date = raw_x_date, 
                                                  value = raw_data))) %&amp;gt;%
      hc_add_series(type = &amp;quot;arearange&amp;quot;, 
                    name = &amp;quot;80%&amp;quot;,
                    fillOpacity = 0.3,
                    data = list_parse2(data.frame(date = forecast_times,
                                                  upper = as.numeric(fc$upper[,1]),
                                                  lower = as.numeric(fc$lower[,1])))) %&amp;gt;%
      hc_add_series(type = &amp;quot;arearange&amp;quot;, 
                    name = &amp;quot;95%&amp;quot;,
                    fillOpacity = 0.3,
                    data = list_parse2(data.frame(date = forecast_times,
                                                  upper = as.numeric(fc$upper[,2]),
                                                  lower = as.numeric(fc$lower[,2])))) %&amp;gt;% 
      hc_add_series(type = &amp;quot;line&amp;quot;,
                    name = &amp;quot;forecast&amp;quot;,
                    data = list_parse2(data.frame(date = forecast_times, 
                                                  value = forecast_values)))
    
    hc
    
  })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can then be displayed now in a very simple &lt;code&gt;ui.R&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(shiny)
library(highcharter)

shinyUI(fluidPage(
  titlePanel(&amp;quot;Realtime Shiny Dashboard from BigQuery&amp;quot;),
  highchartOutput(&amp;quot;hc&amp;quot;)
  )
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;to be tweaked and put into a template as needed.&lt;/p&gt;

&lt;p&gt;The gif doesn&amp;rsquo;t quite do it justice, but you get the idea:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/realtime_forcast.gif&#34; alt=&#34;Update as visits registered to the blog&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;improvements&#34;&gt;Improvements&lt;/h2&gt;

&lt;p&gt;Ideally I&amp;rsquo;d like to avoid the Shiny grey-out when new data is fetched and the graph redraw - I fiddled a bit trying to get the JavaScript to take data from an R table and pull it in directly put that didn&amp;rsquo;t work out - I may update it if its figured out later.&lt;/p&gt;

&lt;p&gt;However, as I said above for my application I needed an update only every 60 seconds so it wasn&amp;rsquo;t worth too much trouble over.  But if say you needed (and who really &lt;em&gt;needs&lt;/em&gt; this?) a smooth update every 5 seconds, the grey out would be too often to be useable.&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;The full app then can be tested easily as its realtime :-)&lt;/p&gt;

&lt;p&gt;As I visit my blog it sends data from Google Tag Manager to BigQuery; that tables is queried every 5 seconds from the Shiny app to see if any new visits have occured; if they have the full data set is downloaded; a new forecast is made and output to the Highcharts.&lt;/p&gt;

&lt;p&gt;Whatever your application, the biggest thing I got from trying this project was it was a lot easier than I expected, which I credit the BigQuery platform for, so give it a go and let me know how it goes for you. Improve on the base I have made here, and I&amp;rsquo;d be really interested in the applications beyond reporting you may use it for - real time traffic predictions that modify bids being one example.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Real-time forecasting dashboard with Google Tag Manager, Google Cloud and R Shiny - Part one</title>
      <link>http://code.markedmondson.me/real-time-GTM-google-cloud-r-shiny-1/</link>
      <pubDate>Thu, 12 Jan 2017 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/real-time-GTM-google-cloud-r-shiny-1/</guid>
      <description>

&lt;p&gt;In part one of this two part series we walk through the steps to stream data from a &lt;a href=&#34;https://www.google.com/analytics/tag-manager/&#34;&gt;Google Tag Manager&lt;/a&gt; (GTM) implementation into a &lt;a href=&#34;https://cloud.google.com/appengine/&#34;&gt;Google App Engine&lt;/a&gt; (GAE) web app, which then adds data to a &lt;a href=&#34;https://cloud.google.com/bigquery/&#34;&gt;BigQuery&lt;/a&gt; table via BigQuery&amp;rsquo;s data streaming capability.  In part two, we go into how to query that table in realtime from R, make a forecast using R, then visualise it in &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt; and the JavaScript visualisation library &lt;a href=&#34;http://jkunst.com/highcharter/&#34;&gt;Highcharts&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Read &lt;a href=&#34;http://code.markedmondson.me/real-time-GTM-google-cloud-r-shiny-2/&#34;&gt;part two here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The project combines several languages where their advantages lie: Python for its interaction with Google APIs and its quick start creating your own API on App Engine, SQL to query the BigQuery data itself, R for its forecasting libraries and the reactive Shiny framework, and JavaScript for the visualisation and data capture at the Google Tag Manager end.&lt;/p&gt;

&lt;h2 id=&#34;thanks&#34;&gt;Thanks&lt;/h2&gt;

&lt;p&gt;This project wouldn&amp;rsquo;t have been possible without the help of the excellent work gone beforehand by &lt;a href=&#34;https://www.analyticspros.com/blog/data-science/streaming-prebid-data-google-bigquery/&#34;&gt;Luke Cempre&amp;rsquo;s post on AnalyticsPros&lt;/a&gt; for streaming data from Google Tag Manager to BigQuery, and &lt;a href=&#34;http://jkunst.com/&#34;&gt;Joshua Kunst&lt;/a&gt; for his help with the Highcharts JavaScript.&lt;/p&gt;

&lt;h2 id=&#34;data-flows&#34;&gt;Data flows&lt;/h2&gt;

&lt;p&gt;There are two data flows in this project.  The first adds data to BigQuery:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;GTM collects data into its dataLayer from a web visit.&lt;/li&gt;
&lt;li&gt;A custom HTML tag in GTM collects the data you want to stream then calls an App Engine URL with its data payload.&lt;/li&gt;
&lt;li&gt;The app engine URL is sent to a queue to add the data to BigQuery.&lt;/li&gt;
&lt;li&gt;The data plus a timestamp is put into a BigQuery row.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then to read the data:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The Shiny app calls Big Query every X seconds.&lt;/li&gt;
&lt;li&gt;The data is aggregated&lt;/li&gt;
&lt;li&gt;A forecast is made with the updated data.&lt;/li&gt;
&lt;li&gt;The Highcharts visualisation reads the changing dataset, and updates the visualisation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This blog will cover the first, putting data into BigQuery. The code for the finished app is available on Github here:  &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream&#34;&gt;https://github.com/MarkEdmondson1234/ga-bq-stream&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;bigquery-configuration&#34;&gt;BigQuery configuration&lt;/h2&gt;

&lt;p&gt;Starting with BigQuery, you need to create a project, dataset and a table where the data will stream to.  The script we will use on App Engine assumes you have one field called &amp;ldquo;ts&amp;rdquo; which will hold a timestamp, other than that add the fields you will add in the Google Tag Manager script.&lt;/p&gt;

&lt;p&gt;Select &amp;ldquo;partitioned&amp;rdquo; table when creating, which is useful if holding more than one days worth of data.&lt;/p&gt;

&lt;p&gt;A demo is shown below, where the &lt;code&gt;ts&lt;/code&gt; field is joined by the page URL and referrer for that page:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/BQconfig.png&#34; alt=&#34;bqconfig&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;google-app-engine&#34;&gt;Google App Engine&lt;/h2&gt;

&lt;p&gt;Next we get to the meat with the Google App Engine app.&lt;/p&gt;

&lt;p&gt;There is a guide on how to install and configure the app there too on its &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream/blob/master/README.md&#34;&gt;README&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this example the app both reads and writes the data to BigQuery, but in production this should be separated out to avoid hitting quotas.&lt;/p&gt;

&lt;p&gt;App Engine is useful in providing a way to run a script (in this case Python) whenever a URL is called, and also providing the infrastructure that lets you scale those hits from a free small amount to billions if you pay up.&lt;/p&gt;

&lt;p&gt;In essence we upload a Python script and tell App Engine to run the script when certain URL endpoints are called, and then we shall call that URL from Google Tag Manager with the data we want to stream.&lt;/p&gt;

&lt;p&gt;We now walk through the important functions of the app:&lt;/p&gt;

&lt;h3 id=&#34;adding-data-to-bigquery&#34;&gt;Adding data to BigQuery&lt;/h3&gt;

&lt;p&gt;You can read more about &lt;a href=&#34;https://cloud.google.com/bigquery/streaming-data-into-bigquery&#34;&gt;streaming data into BigQuery here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The first function is modified from the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/bigquery/cloud-client/stream_data.py&#34;&gt;python BigQuery examples&lt;/a&gt; and takes care of authentication, loading the JSON sent to the app into a Python list and sending to BigQuery:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def stream_data(dataset_name, table_name, json_data, time_stamp = time.time()):
    bigquery_client = bigquery.Client()
    dataset = bigquery_client.dataset(dataset_name)
    table = dataset.table(table_name)
    data = json_data

    data[&#39;ts&#39;] = time_stamp

    # Reload the table to get the schema.
    table.reload()

    ## get the names of schema
    schema = table.schema
    schema_names = [o.name for o in schema]

    logging.debug(&#39;BQ Schema: {}&#39;.format(schema_names))

    # from schema names get list of tuples of the values
    rows = [(data[x] for x in schema_names)]

    # Send data to bigquery, returning any errors
    errors = table.insert_data(rows, row_ids = str(uuid.uuid4()))

    if not errors:
    	logging.debug(&#39;Loaded 1 row into {}:{}&#39;.format(dataset_name, table_name))
    else:
        logging.error(errors)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next class reads the data from a GET or POST request to the URL we specify later, and puts the job into a task queue, along with the timestamp.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class MainHandler(webapp2.RequestHandler):

	## for debugging
	def get(self):
		## allows CORS
		self.response.headers.add_header(&amp;quot;Access-Control-Allow-Origin&amp;quot;, &amp;quot;*&amp;quot;)

		## get example.com?bq=blah
		b = self.request.get(&amp;quot;bq&amp;quot;)

		## send to async task URL
		task = taskqueue.add(url=&#39;/bq-task&#39;, params={&#39;bq&#39;: b, &#39;ts&#39;: str(time.time())})

	# use in prod
	def post(self):
		## allows CORS
		self.response.headers.add_header(&amp;quot;Access-Control-Allow-Origin&amp;quot;, &amp;quot;*&amp;quot;)

		## get example.com?bq=blah
		b = self.request.get(&amp;quot;bq&amp;quot;)

		## send to task URL
		task = taskqueue.add(url=&#39;/bq-task&#39;, params={&#39;bq&#39;: b, &#39;ts&#39;: str(time.time())})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The task queue then reads the JSON data and calls the function to send data into BigQuery.  App Engine task queues will rerun if any connection problems and act as a buffer, so you can configure them to suit the needs and volumes of your app.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class BqHandler(webapp2.RequestHandler):
	def post(self):

		## get example.com/bq-task?bq=blah
		b = self.request.get(&amp;quot;bq&amp;quot;)
		ts = self.request.get(&amp;quot;ts&amp;quot;)

		b = json.loads(b)

		logging.debug(&#39;json load: {}&#39;.format(b))

		if len(b) &amp;gt; 0:
			datasetId = os.environ[&#39;DATASET_ID&#39;]
			tableId   = os.environ[&#39;TABLE_ID&#39;]

			today = date.today().strftime(&amp;quot;%Y%m%d&amp;quot;)

			tableId = &amp;quot;%s$%s&amp;quot;%(tableId, today)

			stream_data(datasetId, tableId, b, ts)

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;full-app-engine-script&#34;&gt;Full App Engine Script&lt;/h3&gt;

&lt;p&gt;The full script uploaded is available in the Github repository here: &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream/blob/master/main.py&#34;&gt;main.py&lt;/a&gt; which also includes the read functions used in the next blogpost.&lt;/p&gt;

&lt;p&gt;With this script you then need some configuration files for the app and upload it to your Google Project.  A guide on how to deploy this is and more is available from the &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream/blob/master/README.md&#34;&gt;Github repository README&lt;/a&gt;, but once done the app will be available at &lt;code&gt;https://YOUR-PROJECT-ID.appspot.com&lt;/code&gt; and you will call the &lt;code&gt;/bq-streamer&lt;/code&gt; and &lt;code&gt;/bq-get&lt;/code&gt; URLs to send and get data.&lt;/p&gt;

&lt;h2 id=&#34;google-tag-manager&#34;&gt;Google Tag Manager&lt;/h2&gt;

&lt;p&gt;With the app ready, we now move to sending it data via Google Tag Manager.  This is relatively simple, since we just need to decide which data to add to the endpoint URL:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;&amp;lt;script&amp;gt;

  var bqArray = {};
        
  // put the variables you want realtime here      
  bqArray[&amp;quot;fieldname&amp;quot;] = &amp;quot;{{dataLayer}}&amp;quot;;
  bqArray[&amp;quot;fieldname2&amp;quot;] = &amp;quot;{{dataLayer2}}&amp;quot;;
  	
  jQuery.post(&amp;quot;https://YOUR-PROJECT-ID.appspot.com/bq-streamer&amp;quot;, {&amp;quot;bq&amp;quot;:JSON.stringify(bqArray)});

&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The script assumes you have jQuery defined on your website, if you haven&amp;rsquo;t you will need to load it either on the page or hack it a bit by loading above the script via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;&amp;lt;script src=&amp;quot;//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For my blog example, here is a screenshot from GTM all configured:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/GTMconfig.png&#34; alt=&#34;gtm&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The app engine supports GET or POST hits, GET is useful for testing in the browser yourself but its better to POST in production as it supports more data.&lt;/p&gt;

&lt;p&gt;Add this as a custom HTML tag and deploy on a trigger that occurs after the data you want to collect is there.  Thats pretty much it.&lt;/p&gt;

&lt;p&gt;Once the tag is published, make sure you have deployed the App Engine app and you are using the exact same field names as the BigQuery table.&lt;/p&gt;

&lt;h1 id=&#34;checking-its-all-working&#34;&gt;Checking its all working&lt;/h1&gt;

&lt;p&gt;You should then be able to start seeing hits in the App Engine logs and in BigQuery.  By default the BQ queries in the UI cache the results, so don&amp;rsquo;t forget to turn those off, but then as new hits are made to the GTM container you should be able to refresh and see the results in BigQuery within a few seconds.  Here is the example from my blog:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/itsalive.png&#34; alt=&#34;its-alive&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And thats it!  You could now query this table from various solutions such as Tableau or Data Studio, but in part two of this post I&amp;rsquo;ll go in to how to query this table from an R Shiny application, updating a forecast and displaying using the Highcharts library.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>