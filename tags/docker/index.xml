<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Docker on Mark Edmondson</title>
    <link>http://code.markedmondson.me/tags/docker/index.xml</link>
    <description>Recent content in Docker on Mark Edmondson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-GB</language>
    <copyright>Powered by [Hugo](//gohugo.io). Theme by [PPOffice](http://github.com/ppoffice).</copyright>
    <atom:link href="http://code.markedmondson.me/tags/docker/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>R on Kubernetes - serverless Shiny, R APIs and scheduled scripts</title>
      <link>http://code.markedmondson.me/r-on-kubernetes-serverless-shiny-r-apis-and-scheduled-scripts</link>
      <pubDate>Wed, 02 May 2018 00:00:00 +0000</pubDate>
      
      <guid>http://code.markedmondson.me/r-on-kubernetes-serverless-shiny-r-apis-and-scheduled-scripts</guid>
      <description>&lt;h2 id=&#34;why-run-r-on-kubernetes&#34;&gt;Why run R on Kubernetes?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt; is a free and open-source utility to run jobs within a computer cluster.  It abstracts away the servers the jobs are running on so you need only worry about the code to run.  It has features such as &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/&#34;&gt;scheduling&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler&#34;&gt;auto-scaling&lt;/a&gt;, and auto-healing to replace nodes if they breakdown.&lt;/p&gt;

&lt;p&gt;If you only need to run R on a single machine, then its probably a bit OTT to use Kubernetes, but if you are starting to work with multiple Docker containers and/or VMs it gets more and more attractive to have a way to easily orchestrate them.&lt;/p&gt;

&lt;p&gt;Kubernetes works via &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt; containers, so if you are already familiar with using Docker for abstracting away code environments, it should be a short step up to abstracting away the computers those Docker containers run upon.
&lt;/p&gt;

&lt;p&gt;R is a good mix with Docker since it provides a way to have stable production environments in a landscape of shifting dependencies given R&amp;rsquo;s fast changing open source foundation.  There is a growing ecosystem to help R users with Docker including the &lt;a href=&#34;https://www.rocker-project.org/&#34;&gt;Rocker Project&lt;/a&gt;, &lt;a href=&#34;http://o2r.info/2017/05/30/containerit-package/&#34;&gt;containerit&lt;/a&gt; and &lt;a href=&#34;https://github.com/wch/harbor&#34;&gt;harbor&lt;/a&gt;, and Rhys Jackson has also crafted a Kubernetes client for R in &lt;a href=&#34;https://github.com/RhysJackson/googleKubernetesR&#34;&gt;googleKubernetesR&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If your R scripts are deployed in Docker containers, you can use them in Kubernetes.  This includes applications such as R powered APIs using OpenCPU or plumber, Shiny apps, batch R jobs that can scale horizontally over many CPUs, or scheduled analysis.&lt;/p&gt;

&lt;p&gt;What you gain is reliable, flexible, production ready R applications, that will scale, run on many cloud providers including Google, and once set up easy to deploy - in most cases pushing to GitHub can be the trigger to serve your new code.&lt;/p&gt;

&lt;h3 id=&#34;alternatives&#34;&gt;Alternatives&lt;/h3&gt;

&lt;p&gt;This article deploys to Google Kubernetes Engine (GKE), but there are many other methods to cover some of Kubernetes functionality, that you may prefer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;gke_icon.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Kubernetes itself can run on all the cloud providers aside Google Cloud Platform, as well as your own servers.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/&#34;&gt;Google App Engine Flexible&lt;/a&gt; is a more managed platform that takes care of some details, but you lose control of others.  I wrote about this previously for running R APIs and a demo is in this &lt;a href=&#34;https://github.com/MarkEdmondson1234/serverless-R-API-appengine&#34;&gt;GitHub repo&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.docker.com/compose/&#34;&gt;Docker Compose&lt;/a&gt; is an equivalent service that only runs on one host, &lt;a href=&#34;https://docs.docker.com/engine/swarm/&#34;&gt;Docker Swarm&lt;/a&gt; runs on multiple hosts and most comparible to Kubernetes.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/services/container-service/&#34;&gt;Azure Container Service&lt;/a&gt; is a more managed Kubernetes platform than GKE&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Regardless which one you use, the principles are similar in that they all build on Docker containers, and one of the advantages of Kubernetes is you can move to other providers a lot more easily, since Docker/Kubernetes is fast becoming the standard for cloud developments.&lt;/p&gt;

&lt;h3 id=&#34;setup-background&#34;&gt;Setup background&lt;/h3&gt;

&lt;p&gt;The below is a setup I have used for the above applications, and may be able to help if you are looking for similar functionality.&lt;/p&gt;

&lt;p&gt;I ran into several gotchas whilst developing it, so hopefully it will help you avoid some of those. Most vexing for me was finding a way to serve different R scripts on one kubernetes cluster on different URL paths (e.g. &lt;code&gt;/r-script1&lt;/code&gt;, &lt;code&gt;/shinyapp2&lt;/code&gt;, &lt;code&gt;/r-api-3&lt;/code&gt;, etc) - many thanks to Rhys&amp;rsquo; help with that.  The answer was that (at the time of writing) the default Google Kubernetes Engine doesn&amp;rsquo;t support rewriting URLs, so instead its better to install another application on your cluster to take care of the above URL serving (termed &lt;code&gt;ingress fanout&lt;/code&gt;), namely using &lt;code&gt;nginx ingress&lt;/code&gt; instead of the default GKE ingress.&lt;/p&gt;

&lt;p&gt;This article then demonstrates:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Creating a Kubernetes cluster for your R applications&lt;/li&gt;
&lt;li&gt;Installing Helm (and Tiller) to aid with a nginx ingress installation&lt;/li&gt;
&lt;li&gt;Deploying R containers, with examples for Shiny, an R API and an R scheduled script&lt;/li&gt;
&lt;li&gt;Serving up those R containers on nice URLs&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;These were used to help develop the below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/tutorials/http-balancer&#34;&gt;Setting up HTTP load balancing with Ingress&lt;/a&gt; Google tutorial&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/community/tutorials/nginx-ingress-gke&#34;&gt;Ingress with nginx&lt;/a&gt; modified with &lt;a href=&#34;https://docs.bitnami.com/kubernetes/how-to/configure-rbac-in-your-kubernetes-cluster/&#34;&gt;this&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;My &lt;a href=&#34;https://stackoverflow.com/questions/48452556/setting-up-a-kuberentes-cluster-with-http-load-balancing-ingress-for-rstudio-and&#34;&gt;StackOverflow question&lt;/a&gt; on http load balancing&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/reference/kubectl/overview/&#34;&gt;Kubernetes reference documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.bitnami.com/kubernetes/how-to/secure-kubernetes-services-with-ingress-tls-letsencrypt/&#34;&gt;Kubernetes with nginx, TLS and LetsEncrypt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;part-1-creating-an-r-cluster&#34;&gt;Part 1 - Creating an R cluster&lt;/h2&gt;

&lt;p&gt;Follow the &lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-container-cluster&#34;&gt;setup steps to authenticate with gcloud and kuberctl&lt;/a&gt; then create your cluster via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud container clusters create r-cluster --num-nodes=3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;or if using &lt;code&gt;googleKubernetesR&lt;/code&gt; :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleKubernetesR)

## create a 3 node cluster called r-cluster with defaults
createCluster(projectId = gcp_get_project(), zone = &amp;quot;europe-west3-b&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You&amp;rsquo;ll need to wait a bit as it provisions all the VMs.&lt;/p&gt;

&lt;p&gt;Most of the below will use the terminal/shell for working rather than R, but in the future a lot of this may be possible via &lt;code&gt;googleKubernetesR&lt;/code&gt; within an R session.&lt;/p&gt;

&lt;p&gt;Set up your shell to get the credentials for Kuberctl:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud container clusters get-credentials r-cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And we are all set to start setting up this cluster for R jobs!&lt;/p&gt;

&lt;p&gt;Here is a screenshot from the web UI of what it should look like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;r-cluster.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;part-2-install-the-nginx-ingress-controller&#34;&gt;Part 2 - Install the nginx ingress controller&lt;/h2&gt;

&lt;p&gt;The first recommended setup is to enable an ingress controller.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;ingress_controller.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Image from &lt;a href=&#34;https://www.nginx.com/products/nginx/kubernetes-ingress-controller/&#34;&gt;nginx&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This is a pod that directs internet traffic to the right R container we set up later.&lt;/p&gt;

&lt;p&gt;We install the ingress controller via &lt;a href=&#34;https://helm.sh/&#34;&gt;Helm&lt;/a&gt; and its counterpart Tiller on the cluster.&lt;/p&gt;

&lt;p&gt;Helm is a package manager for Kubernetes.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You can skip installing Helm/Tiller/ingress controller if you don&amp;rsquo;t need to have R apps on a URL, for instance if its just scheduled cron jobs you want on your cluster, but it is recommended if you want to serve up APIs and Shiny.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The below is mostly copied straight from &lt;a href=&#34;https://docs.bitnami.com/kubernetes/how-to/configure-rbac-in-your-kubernetes-cluster/&#34;&gt;Bitnami&lt;/a&gt;&amp;rsquo;s article.&lt;/p&gt;

&lt;h3 id=&#34;install-helm&#34;&gt;Install Helm&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;The below assumes RBAC rules are enabled, which is true from Kubernetes version 1.8 onwards.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl -o get_helm.sh https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get
chmod +x get_helm.sh
./get_helm.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;install-tiller-on-cluster&#34;&gt;Install Tiller on cluster&lt;/h3&gt;

&lt;h4 id=&#34;step-1-create-the-tiller-service-account&#34;&gt;Step 1: Create the Tiller service account&lt;/h4&gt;

&lt;p&gt;Create a tiller-serviceaccount.yaml file using kubectl:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl create serviceaccount tiller --namespace kube-system
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;step-2-bind-the-tiller-service-account-to-the-cluster-admin-role&#34;&gt;Step 2: Bind the Tiller service account to the cluster-admin role&lt;/h4&gt;

&lt;p&gt;Create a &lt;code&gt;tiller-clusterrolebinding.yaml&lt;/code&gt; file with the following contents:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;      kind: ClusterRoleBinding
      apiVersion: rbac.authorization.k8s.io/v1beta1
      metadata:
        name: tiller-clusterrolebinding
      subjects:
      - kind: ServiceAccount
        name: tiller
        namespace: kube-system
      roleRef:
        kind: ClusterRole
        name: cluster-admin
        apiGroup: &amp;quot;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Deploy the ClusterRoleBinding:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl create -f tiller-clusterrolebinding.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;step-3-update-the-existing-tiller-deployment&#34;&gt;Step 3: Update the existing Tiller deployment&lt;/h4&gt;

&lt;p&gt;Update the existing tiller-deploy deployment with the Service Account you created earlier:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;helm init --service-account tiller --upgrade
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wait a few seconds for the Tiller server to be redeployed.&lt;/p&gt;

&lt;h4 id=&#34;step-4-test-the-new-helm&#34;&gt;Step 4: Test the new Helm&lt;/h4&gt;

&lt;p&gt;All being well, you should be able to execute this command without errors:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;helm ls
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s it! You have configured Helm in your Kubernetes cluster.&lt;/p&gt;

&lt;h4 id=&#34;step-5-deploy-ngnix-ingress-controller&#34;&gt;Step 5: Deploy ngnix Ingress Controller&lt;/h4&gt;

&lt;p&gt;Now you can deploy the &lt;a href=&#34;https://kubernetes.github.io/ingress-nginx/&#34;&gt;nginx ingress controller&lt;/a&gt; using Helm to take care of details:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;helm install --name nginx-ingress stable/nginx-ingress --set rbac.create=true
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should see your IP when you issue:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl get service nginx-ingress-controller
#&amp;gt; NAME                       CLUSTER-IP    EXTERNAL-IP   PORT(S)                      AGE
#&amp;gt; nginx-ingress-controller   10.7.253.89   &amp;lt;pending&amp;gt;     80:32713/TCP,443:31061/TCP   23s
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;part-3-deploying-r-workloads&#34;&gt;Part 3 - Deploying R Workloads!&lt;/h2&gt;

&lt;p&gt;Now the cluster is all setup for you to deploy your R containers.  You can install other containers in any other language too, such as normal HTML webapps or utilties.&lt;/p&gt;

&lt;p&gt;We deploy them using the command &lt;code&gt;kubectl run&lt;/code&gt;, and go through some examples below.&lt;/p&gt;

&lt;h3 id=&#34;a-shiny-apps&#34;&gt;A: Shiny apps&lt;/h3&gt;

&lt;p&gt;Below we deploy two Shiny apps, in this case the &lt;a href=&#34;https://github.com/cloudyr/googleComputeEngineR/tree/master/inst/dockerfiles/shiny-googleAuthRdemo&#34;&gt;&lt;code&gt;googleAuthR&lt;/code&gt; demo app&lt;/a&gt; that is configured to run on port 3838, in a &lt;code&gt;/shiny/&lt;/code&gt; folder, and a &lt;a href=&#34;https://github.com/flaviobarros/shiny-wordcloud&#34;&gt;wordcloud Shiny app by Flavio Barros&lt;/a&gt; that has had its Dockerfile configured to run on port 80 in the root folder, to demonstrate how to handle different Docker deployments.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl run shiny1 --image gcr.io/gcer-public/shiny-googleauthrdemo:latest --port 3838
kubectl run shiny2 --image=flaviobarros/shiny-wordcloud --port=80
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;expose-containers-as-node-ports&#34;&gt;Expose containers as node ports&lt;/h4&gt;

&lt;p&gt;At the moment we are only selecting which services the cluster will run - we won&amp;rsquo;t be able to see them yet until we expose the apps and configure the ingress rules&lt;/p&gt;

&lt;p&gt;These will be pointed to via the nginx ingress controller.&lt;/p&gt;

&lt;p&gt;We first open up their ports as nodeports, so the ingress can see them:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl expose deployment shiny1 --target-port=3838  --type=NodePort
kubectl expose deployment shiny2 --target-port=80  --type=NodePort
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;create-ingress-with-ngninx&#34;&gt;Create Ingress with ngninx&lt;/h4&gt;

&lt;p&gt;This lets you be able to call the different services from one ip address.&lt;/p&gt;

&lt;p&gt;Save this as &lt;code&gt;r-ingress-nginx.yaml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: r-ingress-nginx
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: / 
    nginx.ingress.kubernetes.io/ssl-redirect: &amp;quot;false&amp;quot;
spec:
  rules:
  - http:
      paths:
      - path: /wordcloud/
      # app deployed to /wordcloud/
        backend:
          serviceName: shiny2
          servicePort: 80
      - path: /gar/
      # app deployed to /gar/shiny/
        backend:
          serviceName: shiny1
          servicePort: 3838

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Deploy via&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl apply -f r-ingress-nginx.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Verify and keep refreshing until it has an ip address assigned in the &lt;code&gt;ADDRESS&lt;/code&gt; field (5mins)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl get ingress r-ingress-nginx
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;see-your-deployed-containers&#34;&gt;See your deployed containers&lt;/h4&gt;

&lt;p&gt;The Nginx ingress controller takes care of any URL path not covered in the &lt;code&gt;r-ingress-nginx&lt;/code&gt; ingress.  By default this should give a &lt;code&gt;default backend - 404&lt;/code&gt; message.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/gar/shiny/&lt;/code&gt; is served by the googleAuthR demo app.  &lt;code&gt;/gar/&lt;/code&gt; covers the standard Shiny welcome screen, and &lt;code&gt;/shiny/&lt;/code&gt; is the folder that is configured in the Dockerfile&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/wordcloud/&lt;/code&gt; holds the Wordcloud Shiny app. In this case, the Shiny app is configured to appear in the root directory of the &lt;code&gt;/wordcloud/&lt;/code&gt; URL.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here are some screenshots:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;gar_app.png&#34; alt=&#34;googleAuthR app&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;wordcloud.png&#34; alt=&#34;Wordcloud app&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;default404.png&#34; alt=&#34;Default webpage if not defined in ingress&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Note it matters where you put the app in your Dockerfile.  The &lt;code&gt;nginx.ingress.kubernetes.io/rewrite-target: /&lt;/code&gt; line in the ingress lets you specify a URL that is relative to your specified URLs.&lt;/p&gt;

&lt;p&gt;If you want to have your app served on the absolute URL specified in the Dockerfile, see below for a no-redirect version.&lt;/p&gt;

&lt;h4 id=&#34;no-redirects&#34;&gt;No redirects&lt;/h4&gt;

&lt;p&gt;Without redirects work too, but the Shiny app needs to have the correct URL path configured in its Docker that matches the URLs you are specifying in the Ingress e.g. for our examples as the googleAuthR demo is in path &lt;code&gt;/shiny/&lt;/code&gt; and the Wordcloud demo in the root &lt;code&gt;/&lt;/code&gt;, the Ingress rules need to match those. Beware clashes where two Shiny apps have configured to the same URL path, e.g. root &lt;code&gt;/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Save this as &lt;code&gt;r-ingress-nginx-noredirect.yaml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: r-ingress-nginx
  annotations:
    kubernetes.io/ingress.class: nginx
    #nginx.ingress.kubernetes.io/rewrite-target: / # not used in this example
    nginx.ingress.kubernetes.io/ssl-redirect: &amp;quot;false&amp;quot;
spec:
  rules:
  - http:
      paths:
      - path: /
        backend:
          serviceName: shiny2
          servicePort: 80
      - path: /shiny/
        backend:
          serviceName: shiny1
          servicePort: 3838
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Deploy via&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl apply -f r-ingress-nginx-noredirect.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again, the Nginx ingress controller takes care of any URL path not covered in the &lt;code&gt;r-ingress-nginx&lt;/code&gt; ingress.&lt;/p&gt;

&lt;p&gt;However now:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/shiny/&lt;/code&gt; is served by the googleAuthR demo app.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/&lt;/code&gt; holds the Wordcloud Shiny app.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;b-r-apis-opencpu&#34;&gt;B: R APIs - OpenCPU&lt;/h3&gt;

&lt;p&gt;Kubernetes is well suited to serving R APIs as they can auto-scale to demand, covering peaks and troughs in demand.&lt;/p&gt;

&lt;p&gt;A demo API is shown below, using &lt;a href=&#34;http://code.markedmondson.me/predictClickOpenCPU/&#34;&gt;&lt;code&gt;openCPU&lt;/code&gt; for predicting website clicks&lt;/a&gt;, but the same principle allies if using &lt;code&gt;plumber&lt;/code&gt; or another framework by swapping out the Dockerfile to serve the API on a port.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This assumes you have configured the ingress controller above&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl run opencpu-demo --image gcr.io/gcer-public/opencpu-demo:latest --port 8004
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Expose the port as before:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl expose deployment opencpu-demo --target-port=8004  --type=NodePort
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we do another Ingress deployment for the R-API, in addition to the Shiny apps above.&lt;/p&gt;

&lt;p&gt;Save the below as &lt;code&gt;r-ingress-opencpu.yaml&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: r-ingress-nginx-ocpu
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: &amp;quot;false&amp;quot;
spec:
  rules:
  - http:
      paths:
      - path: /ocpu/
        backend:
          serviceName: opencpu-demo
          servicePort: 8004
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;and deploy via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl apply -f r-ingress-opencpu.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your ip address will be as shown as your &lt;code&gt;ADDRESS&lt;/code&gt; when you issue:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl get ingress r-ingress-nginx-ocpu
#&amp;gt; NAME                   HOSTS     ADDRESS         PORTS     AGE
#&amp;gt; r-ingress-nginx-ocpu   *         35.205.249.34   80        10m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note no rewriting this time as OpenCPU takes care of itself on &lt;code&gt;/ocpu/&lt;/code&gt; folder.&lt;/p&gt;

&lt;h3 id=&#34;use-your-opencpu-installation&#34;&gt;Use your OpenCPU installation&lt;/h3&gt;

&lt;p&gt;Assuming your IP is &lt;code&gt;1.2.3.4&lt;/code&gt;; we can see the OpenCPU is installed via &lt;code&gt;1.2.3.4/ocpu/info&lt;/code&gt; and the test page at &lt;code&gt;1.2.3.4/ocpu/test/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For our example, the custom package uploaded is available at &lt;code&gt;1.2.3.4/ocpu/library/predictClickOpenCPU&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;open-cpu-running.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We use the API for this package by creating a POST hit to API endpoint&lt;/p&gt;

&lt;p&gt;&lt;code&gt;1.2.3.4/ocpu/library/predictClickOpenCPU/R/predictMarkov/json&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;with the parameter &lt;code&gt;pageview_names=[&amp;quot;/example/96&amp;quot;,&amp;quot;/example/213&amp;quot;,&amp;quot;/example/107&amp;quot;]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Constructing the above in thwe terminal with &lt;code&gt;curl&lt;/code&gt;, we test the API is functioning:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;curl --data &#39;pageview_names=[&amp;quot;/example/96&amp;quot;,&amp;quot;/example/213&amp;quot;,&amp;quot;/example/107&amp;quot;]&#39; http://35.233.42.6/ocpu/library/predictClickOpenCPU/R/predictMarkov/json
#&amp;gt; {
#  &amp;quot;page&amp;quot;: [&amp;quot;/example/251&amp;quot;],
#  &amp;quot;probability&amp;quot;: [0.5657]
#&amp;gt; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Web API endpoints can also be used as webhooks, for instance to react and do something when something is pushed to GitHub for continuous integration purposes.&lt;/p&gt;

&lt;h3 id=&#34;c-scheduled-r-tasks-via-cronjobs&#34;&gt;C: Scheduled R - Tasks via cronJobs&lt;/h3&gt;

&lt;p&gt;Kuberenetes has a special option for deploying scheduled tasks, called cronJobs.&lt;/p&gt;

&lt;p&gt;When configuring your Dockerfile, you can choose so it runs entirely self-contained, perhaps calling an API and writing the result to a database.  For each new job you can make a new Dockerfile and build them on something like build triggers.&lt;/p&gt;

&lt;p&gt;In that case the Dockerfile should run with all configuration files included in the Docker build.  One advantage of running on GKE is that the job can be authenticated with the same project via &lt;a href=&#34;http://code.markedmondson.me/googleAuthR/reference/gar_gce_auth.html&#34;&gt;&lt;code&gt;googleAuthR::gar_gce_auth()&lt;/code&gt;&lt;/a&gt; so will have access to Google Cloud Storage, BigQuery etc.&lt;/p&gt;

&lt;p&gt;An alternative is to configure the Dockerfile to rely on a config file, and have that config file mounted as a volume in the pod - that isn&amp;rsquo;t covered here yet.&lt;/p&gt;

&lt;h4 id=&#34;creating-an-executable-r-dockerfile&#34;&gt;Creating an executable R Dockerfile&lt;/h4&gt;

&lt;p&gt;This is an example Dockerfile that installs dependcies and R packages, loads a local configuration file then runs a custom function when it is called upon by the cronJob:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM rocker/verse
MAINTAINER Mark Edmondson (r@sunholo.com)

# install R package dependencies
RUN apt-get update &amp;amp;&amp;amp; apt-get install -y \
    gcc gsl-bin libblas-dev \
    ## etc....
    ## clean up
    &amp;amp;&amp;amp; apt-get clean \ 
    &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/ \ 
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds

# any necessary environment arguments
ENV SECRET_ARG hellomum

# Install packages from CRAN and GitHub
RUN install2.r --error \ 
    -r &#39;http://cran.rstudio.com&#39; \
    googleAuthR googleAnalyticsR bigQueryR \
    &amp;amp;&amp;amp; Rscript -e &amp;quot;devtools::install_github(&#39;MarkEdmondson1234/youtubeAnalyticsR&#39;)&amp;quot; \
    &amp;amp;&amp;amp; Rscript -e &amp;quot;devtools::install_github(&#39;yourprivate/customPackage&#39;)&amp;quot; \
    ## clean up
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds

# my_config.yaml is in same folder as Dockerfile
COPY my_config.yaml my_config.yaml

## this runs when this Docker container is called
CMD [&amp;quot;Rscript&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;customPackage::do_something(&#39;my_config.yaml&#39;)&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;running-cronjobs&#34;&gt;Running cronJobs&lt;/h4&gt;

&lt;p&gt;For &lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/&#34;&gt;cronJobs&lt;/a&gt;, specify the &lt;code&gt;schedule&lt;/code&gt; argument, and we change the restart policy to only do so on failure.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl run scheduled-r --schedule=&amp;quot;23 4 * * *&amp;quot; --restart=OnFailure --image=gcr.io/xxxxxx/private_docker_image:latest
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That is getting a bit complicated for the terminal , so I prefer to deploy via the file method:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: scheduledr
spec:
  schedule: &amp;quot;23 4 * * *&amp;quot;
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: my_image
            image: gcr.io/xxxxxx/private_docker_image:latest
          restartPolicy: OnFailure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Save the file as say &lt;code&gt;cron-r.yaml&lt;/code&gt; and deploy via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl create -f ./cron-r.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should then see the job has been created via&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get cronjob scheduledr
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And in the Web UI under &lt;a href=&#34;https://console.cloud.google.com/kubernetes/workload?project=iih-tools-analytics&amp;amp;authuser=0&amp;amp;workload_list_tablesize=50&#34;&gt;Workloads&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;cronJob.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;part-4-going-further&#34;&gt;Part 4 - Going further&lt;/h2&gt;

&lt;p&gt;This article turned into a lot more words than I intended, so I&amp;rsquo;ll stop now, but see below on some more things I could have written about:&lt;/p&gt;

&lt;h3 id=&#34;updating-your-cluster&#34;&gt;Updating your cluster&lt;/h3&gt;

&lt;p&gt;Its a bit of configuration to get there, but once deployed since the services are running off Docker, and those in turn are built using build triggers that are from GitHub pushes, it means that changes to scripts can all be controlled via GitHub pushes and branches, which you are probably using anyway for version control.&lt;/p&gt;

&lt;p&gt;When you change a configuration file and want to deploy, use &lt;code&gt;kubectl apply&lt;/code&gt; instead of &lt;code&gt;kubectl create&lt;/code&gt; to change the settings.&lt;/p&gt;

&lt;p&gt;Its best practice to specify a dev and production branch to your GitHub Docker images, so you can safely test containers before you break your production.&lt;/p&gt;

&lt;h3 id=&#34;secrets-and-https-ssl-configuration&#34;&gt;Secrets and https / SSL configuration&lt;/h3&gt;

&lt;p&gt;Kubernetes supports more robust methods to guard your secrets that including them in the Dockerfiles - see &lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables&#34;&gt;here&lt;/a&gt; for proper treatment.&lt;/p&gt;

&lt;p&gt;Related to that is serving over https.  In this case I found Bitnami the easiest method to follow, using Helm again to &lt;a href=&#34;https://docs.bitnami.com/kubernetes/how-to/secure-kubernetes-services-with-ingress-tls-letsencrypt/&#34;&gt;configure TLS with LetsEncrypt and Kube-lego&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;auto-scaling&#34;&gt;Auto scaling&lt;/h3&gt;

&lt;p&gt;Launching the cluster with these flags enables auto-scaling&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud container clusters create r-cluster --num-nodes=0 --enable-autoscaling --min-nodes=0 --max-nodes=3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On Google Kubernetes Engine this scales down to 0, which should be cheaper than running a few CPUs running &lt;sup&gt;24&lt;/sup&gt;&amp;frasl;&lt;sub&gt;7&lt;/sub&gt;, and you can configure when new nodes are launched such as when it hits 80% CPU.&lt;/p&gt;

&lt;h3 id=&#34;parallel-tasks&#34;&gt;Parallel tasks&lt;/h3&gt;

&lt;p&gt;For big parallel processing tasks, Kubernetes is an option that can launch as many CPUs you need.&lt;/p&gt;

&lt;p&gt;However, you will need to configure your workload so that each service works on a different part of the data, so some form of central ledger is needed that keeps track of which rows on the database or files have been treated, and to remove them from the input files once completed.&lt;/p&gt;

&lt;p&gt;A strategy to handle this is to use message brokers that specify which work needs to be done. An example of using a &lt;a href=&#34;https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/&#34;&gt;message broker is here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;and-finally&#34;&gt;And finally&amp;hellip;&lt;/h3&gt;

&lt;p&gt;After all that, do remember to tear down your cluster and/or load balancers when finished with, as you are charged by minute of uptime per node.  Most tutorials include this at the end - you can either delete in the web UI but to be complete here:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Delete the Ingress: This deallocates the ephemeral external IP address and the load balancing resources associated to your application:&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;kubectl delete ingress your-ingress
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Delete the cluster: This deletes the compute nodes of your container cluster and other resources such as the Deployments in the cluster:&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud container clusters delete r-cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;When working with Kubernetes documentation you can tell that it is a fast changing developing product, which can be a bit frustrating when looking for solutions, but I anticipate the APIs to settle down in the future.&lt;/p&gt;

&lt;p&gt;This should hopefully serve as a good jumping off point for you to develop your own Kubernetes R cluster.  Having got Kubernetes under my belt I now feel fully tooled up for deploying R (and other applications) at scale, and in an exciting and developing framework that should stand in good sted for a few years to come.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Run RStudio Server on a Chromebook as a Cloud Native</title>
      <link>http://code.markedmondson.me/rstudio-server-chromebook/</link>
      <pubDate>Tue, 05 Sep 2017 13:55:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/rstudio-server-chromebook/</guid>
      <description>

&lt;p&gt;I recently got an &lt;a href=&#34;https://www.asus.com/us/Laptops/ASUS-Chromebook-Flip-C302CA/&#34;&gt;Asus Chromebook Flip&lt;/a&gt; with which I&amp;rsquo;m very happy, but it did make me realise that if a Chromebook was to replace my normal desktop as my primary workstation, my RStudio Server setup would need to be more cloud native than was available up until now.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;TL;DR - A how-to on making RStudio Server run on a Chromebook that automatically backs up data and configuration settings to Google Cloud Storage is on the &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/persistent-rstudio.html&#34;&gt;googleComputeEngineR website here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;broken-promises-of-the-cloud&#34;&gt;Broken promises of the cloud?&lt;/h2&gt;

&lt;p&gt;Whilst there are lots of other solutions around for hosting RStudio Server in the cloud, &lt;a href=&#34;http://code.markedmondson.me/setting-up-scheduled-R-scripts-for-an-analytics-team/&#34;&gt;including some of my own&lt;/a&gt;, I don&amp;rsquo;t believe they are a serious replacement for a desktop station running RStudio.&lt;/p&gt;

&lt;p&gt;To date I&amp;rsquo;ve treated RStudio Server as a temporary UI whilst setting up scheduled scripts and so forth, and for running workshops for students with pre-installed packages.  Yes, the convenience of running RStudio in a browser and being able to launch different configurations is great, but it wasn&amp;rsquo;t a total desktop replacement for the following reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;More expensive&lt;/em&gt; - running RStudio Server permanently in the cloud can be done for as little as 5 dollars a month, but for that you get a machine about as powerful as a Raspberry Pi.  For a proper workstation with decent RAM etc, you are looking at more like 30 dollars a month, which if you&amp;rsquo;re running for 2 years is around $700 that you could have spent on a laptop that can do more things.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;More risky&lt;/em&gt; - keeping the only copy of your data on a cloud server is not a good idea, as I learnt to my cost when a VM&amp;rsquo;s hard disk filled up. Unable to log in, a convoluted panic to increase the disk size occurred.  A physical laptop is a lot easier to troubleshoot if something goes awry.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Not flexible enough&lt;/em&gt; - if you run out of RAM or want things to compute quicker, you are going to need to transfer your data and boot up another VM with stronger specs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But isn&amp;rsquo;t the allure of cheaper, safer, more flexible computing the reason we are using the cloud in the first place?&lt;/p&gt;

&lt;h2 id=&#34;going-cloud-native&#34;&gt;Going Cloud Native&lt;/h2&gt;

&lt;p&gt;I believe the reason for the dichotomy between expectation and reality is due to not being &amp;ldquo;Cloud Native&amp;rdquo; enough, something I realised whilst attending this year&amp;rsquo;s &lt;a href=&#34;https://cloudnext.withgoogle.com/&#34;&gt;Google NEXT event&lt;/a&gt;.  There they described three phases of business cloud evolution:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Lift and Shift - moving your existing infrastructure into equivalent cloud versions&lt;/li&gt;
&lt;li&gt;Separation of compute and data - abstracting out the computing away from the data the computing is running on.  Essentially letting the cloud take on the roles of your hard disk vs your CPU.&lt;/li&gt;
&lt;li&gt;Platform-as-a-service (PaaS) - abstracting away the servers the computing and data are running on, so as to run on purely managed services such as BigQuery, Kubernetes and AppEngine&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;From the above, existing RStudio Server solutions are in the Lift and Shift vein, whilst what we may imagine to be The Cloud are in the more evolved options.&lt;/p&gt;

&lt;h2 id=&#34;docker-ftw&#34;&gt;Docker FTW&lt;/h2&gt;

&lt;p&gt;To truly take advantage of the cloud, I believe this means you must become familiar with containers, specifically &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt;.  Containers are a rapidly evolving standard that makes cloud evolution possible by allowing breakout of computation, and is being embraced by all cloud providers.&lt;/p&gt;

&lt;p&gt;Using Docker with R has other benefits.  Reproducibility of R code is enhanced when you can pin the exact environment to run code within, and addresses criticisms of R that it is not production ready: normally R&amp;rsquo;s open source and rapidly evolving nature means R code you wrote two years ago may not work with the most modern R package versions.&lt;/p&gt;

&lt;p&gt;I am in a lucky position to work on this as I have developed a niche creating Google API libraries in R.  Starting with my needs from digital marketing to wrap the Search Console and Google Analytics APIs, I then used that experience to move into the more general Cloud APIs such as Cloud Storage and Compute Engine, and now get support through my Google Developer Expert status network to experiment with the Google Cloud platform.&lt;/p&gt;

&lt;h2 id=&#34;a-persistent-rstudio-server&#34;&gt;A Persistent RStudio Server&lt;/h2&gt;

&lt;p&gt;Its with two of my R libraries, &lt;a href=&#34;http://code.markedmondson.me/googleCloudStorageR/&#34;&gt;googleCloudStorageR&lt;/a&gt; and &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/&#34;&gt;googleComputeEngineR&lt;/a&gt;, that I&amp;rsquo;ve put together something much closer to the cheap, resilient, and flexible version of the cloud I want to be using when running RStudio in the cloud.&lt;/p&gt;

&lt;p&gt;The role of a harddrive is delegated to Google Cloud Storage, whilst RStudio is served from within Docker containers.  With some new functions that are in the &lt;code&gt;.Rprofile&lt;/code&gt; of a custom RStudio Docker image, Google Cloud Storage is called to download on startup, or upload on exit, all the files to a specific bucket.  These files can include SSH and GitHub settings, or a project folder.  Backups are activated by putting a &lt;code&gt;_gcssave.yaml&lt;/code&gt; file in a folder, or via the VM&amp;rsquo;s metadata.&lt;/p&gt;

&lt;p&gt;What this all means is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RStudio Server runs within its own Docker container, and can be relaunched with custom package setups&lt;/li&gt;
&lt;li&gt;Data is persistent between Docker containers and cloud compute instances.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;I can turn off the RStudio Server to avoid charges, then turn it on again and start from where I left off without reconfiguring git etc.&lt;/li&gt;
&lt;li&gt;I can work locally in one RStudio project, then switch to the cloud to work on the same project.&lt;/li&gt;
&lt;li&gt;SSH keys and GitHub configurations are set up only once and then automatically available across Docker containers, cloud computers and local RStudio projects.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I have a bookmark to the Google Cloud console to startup/shutdown the instance:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/startup-rstudio.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Once launched, I log in and configurations are auto loaded by the &lt;code&gt;_gcssave.yaml&lt;/code&gt; configuration:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/startup-rstudio-persistent.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As this includes the home directory, so long as I login with the same username, and point to the same bucket, any RStudio launched (within a Docker/locally, whereever) don&amp;rsquo;t need to reconfigure Git - downloading a GitHub repo is as simple as copying the SSH GitHub URL&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/ssh-clone-github.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;hellip;and starting a new RStudio project:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/rstudio-github.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This saves me significant cost by letting me stop/start servers as and when I need them via the GCP Web UI. My data is safer than keeping it on my local harddrive, and switching to a bigger VM takes seconds since all data and code upload/download automatically.&lt;/p&gt;

&lt;p&gt;The details for the configuration setup is &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/persistent-rstudio.html&#34;&gt;here on the googleComputeEngineR website&lt;/a&gt;, for which you&amp;rsquo;ll need the latest development versions of &lt;code&gt;googleComputeEngineR&lt;/code&gt; and &lt;code&gt;googleCloudStorageR&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;rstudio-as-a-service&#34;&gt;RStudio-as-a-service?&lt;/h2&gt;

&lt;p&gt;Whats next?  Could this evolve further into a RStudio-as-a-service offering?  To qualify, we would need to not worry about starting or stopping servers at all, and scale under any load.&lt;/p&gt;

&lt;p&gt;Well, prompted by this blog post I had a good look.  It IS possible to deploy RStudio on App Engine and I got excited, but unfortunately the minimum number of permanent instances on a flexible App Engine app is 1, so all told for a solo analyst it is a more expensive solution than running a VM that you stop and start yourself.  However, if you had enough demand to pay for 1 VM 24 hours a day (~$30 a month),  it does start to make sense to scale on App Engine.  A setup that does that is on this GitHub repo &lt;a href=&#34;https://github.com/MarkEdmondson1234/appengine-rstudio&#34;&gt;running RStudio on App Engine&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/appengine-rstudio.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Using R on App Engine is possible due to App Engine&amp;rsquo;s new beta support for &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/&#34;&gt;flexible custom runtime containers&lt;/a&gt;.  Any language deployed through a Docker container will work, something I recently worked through in a proof of concept to deploy a &lt;a href=&#34;https://github.com/MarkEdmondson1234/serverless-R-API-appengine&#34;&gt;serverless R API using Plumber&lt;/a&gt;.  Shiny may be another beneficiary of App Engine, although at time of writing App Engine doesn&amp;rsquo;t support the necessary websockets.&lt;/p&gt;

&lt;p&gt;I suspect using &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt;, a container orchestrator upon Google Container Engine the above could be achieved, but I haven&amp;rsquo;t quite grok&amp;rsquo;d that yet so perhaps I&amp;rsquo;ll update this in the future.&lt;/p&gt;

&lt;p&gt;You can still benefit from PaaS within R if you are using services such as BigQuery.  There, analysis of large datasets is performed without you knowing how many shards of instances are being used to do so, and you can access the results within R via bigrquery/bigQueryR.  I did a presentation of &lt;a href=&#34;https://docs.google.com/presentation/d/1510xJzDuWgbLgoNY3Fs5-CGtMCJEYs5msaxIpINt03g/edit?usp=sharing&#34;&gt;BigQuery&amp;rsquo;s usefulness for analytics here&lt;/a&gt; for MeasureCamp Copenhagen.&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Of course, all this still doesn&amp;rsquo;t work if you do not have an internet connection. :)  But I&amp;rsquo;m now much more comfortable using my Chromebook to start and maintain R projects, and could imagine it being my main work station.&lt;/p&gt;

&lt;p&gt;Typical workflows include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Work on local RStudio&lt;/li&gt;
&lt;li&gt;create project _gcssave.yaml&lt;/li&gt;
&lt;li&gt;log off, auto-save to GCE&lt;/li&gt;
&lt;li&gt;Move to Chromebook&lt;/li&gt;
&lt;li&gt;Start up VM via the Google Cloud console&lt;/li&gt;
&lt;li&gt;create a new project with same name&lt;/li&gt;
&lt;li&gt;auto-load project&lt;/li&gt;
&lt;li&gt;Do code&lt;/li&gt;
&lt;li&gt;Shutdown (auto-save project)&lt;/li&gt;
&lt;li&gt;switch to local RStudio&lt;/li&gt;
&lt;li&gt;etc. etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;hellip;and:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Save project to (private) GitHub&lt;/li&gt;
&lt;li&gt;Move to Chromebook&lt;/li&gt;
&lt;li&gt;Start up VM via the Google Cloud console&lt;/li&gt;
&lt;li&gt;Open RStudio Server IP&lt;/li&gt;
&lt;li&gt;GitHub settings autoload&lt;/li&gt;
&lt;li&gt;Clone GitHub repo via New RStudio Projects&lt;/li&gt;
&lt;li&gt;Do code, commit, push, pull&lt;/li&gt;
&lt;li&gt;etc. etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;None of this would be possible without the skills of the &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;RStudio&lt;/a&gt; team, &lt;a href=&#34;https://cloud.google.com/&#34;&gt;Google Cloud&lt;/a&gt; and &lt;a href=&#34;https://hub.docker.com/u/rocker/&#34;&gt;Rocker&lt;/a&gt;, so a massive thanks to them.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;d imagine if you want to do this with another cloud provider they should have very similar services that you can build upon, let me know in the comments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Four Ways to Schedule R scripts on Google Cloud Platform</title>
      <link>http://code.markedmondson.me/4-ways-schedule-r-scripts-on-google-cloud-platform</link>
      <pubDate>Sun, 09 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>http://code.markedmondson.me/4-ways-schedule-r-scripts-on-google-cloud-platform</guid>
      <description>&lt;p&gt;A common question I come across is how to automate scheduling of R scripts downloading data. This post goes through some options that I have played around with, which I’ve mostly used for downloading API data such as Google Analytics using the Google Cloud platform, but the same principles could apply for AWS or Azure.&lt;/p&gt;
&lt;!--more--&gt;
&lt;div id=&#34;scheduling-scripts-advice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scheduling scripts advice&lt;/h2&gt;
&lt;p&gt;But first, some notes on the scripts you are scheduling, that I’ve picked up.&lt;/p&gt;
&lt;div id=&#34;dont-save-data-to-the-scheduling-server&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Don’t save data to the scheduling server&lt;/h3&gt;
&lt;p&gt;I would suggest to not save or use data in the same place you are doing the scheduling. Use a service like BigQuery (&lt;code&gt;bigQueryR&lt;/code&gt;) or googleCloudStorageR (&lt;code&gt;googleCloudStorageR&lt;/code&gt;) to first load any necessary data, do your work then save it out again. This may be a bit more complicated to set up, but will save you tears if the VM or service goes down - you still have your data.&lt;/p&gt;
&lt;p&gt;To help with this, on Google Cloud you can authenticate with the same details you used to launch a VM to authenticate with the storage services above (as all are covered under the &lt;code&gt;http://www.googleapis.com/auth/cloud-services&lt;/code&gt; scope) - you can access this auth when on a GCE VM in R via &lt;code&gt;googleAuthR::gar_gce_auth()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;An example skeleton script is shown below that may be something you are scheduling.&lt;/p&gt;
&lt;p&gt;It downloads authentication files, does an API call, then saves it up to the cloud again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleAuthR)
library(googleCloudStorageR)

gcs_global_bucket(&amp;quot;my-bucket&amp;quot;)
## auth on the VM
options(googleAuthR.scopes.selected = &amp;quot;https://www.googleapis.com/auth/cloud-platform&amp;quot;)
gar_gce_auth()

## use the GCS auth to download the auth files for your API
auth_file &amp;lt;- &amp;quot;auth/my_auth_file.json&amp;quot;
gcs_get_object(auth_file, saveToDisk = TRUE)

## now auth with the file you just download
gar_auth_service(auth_file)

## do your work with APIs etc.
.....

## upload results back up to GCS (or BigQuery, etc.)
gcs_upload(my_results, name = &amp;quot;results/my_results.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;set-up-a-schedule-for-logs-too&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Set up a schedule for logs too&lt;/h3&gt;
&lt;p&gt;Logs are important for scheduled jobs, so you have some idea on whats happened when things go wrong. To help with scheduling debugging, most &lt;code&gt;googleAuthR&lt;/code&gt; packages now have a timestamp on their output messages.&lt;/p&gt;
&lt;p&gt;You can send the output of your scripts to log files, if using cron and RScript it looks something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;RScript /your-r-script.R &amp;gt; your-r-script.log&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;…where &lt;code&gt;&amp;gt;&lt;/code&gt; sends the output to the new file.&lt;/p&gt;
&lt;p&gt;Over time though, this can get big and (sigh) fill up your disk so you can’t log in to the VM (speaking from experience here!) so I now set up another scheduled job that every week takes the logs and uploads to GCS, then deletes the current ones.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;consider-using-docker-for-environments&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Consider using Docker for environments&lt;/h3&gt;
&lt;p&gt;Several of the methods below use &lt;a href=&#34;https://www.docker.com/&#34;&gt;&lt;code&gt;Docker&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The reasons for that is Docker provides a nice reprodueable way to define exactly what packages and dependencies you need for your script to run, which can run on top of any type of infrastructure as &lt;code&gt;Docker&lt;/code&gt; has quickly become a cloud standard.&lt;/p&gt;
&lt;p&gt;For instance, migrating from Google Cloud to AWS is much easier if both can be deployed using Docker, and below Docker is instrumental in allowing you to run on multiple solutions.&lt;/p&gt;
&lt;p&gt;Bear in mind that when a Docker container relaunches it won’t save any data, so any non-saved state will be lost (you should make a new container if you need it to contain data), but you’re not saving your data to the docker container anyway, aren’t you?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;scheduling-options---pros-and-cons&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scheduling options - Pros and cons&lt;/h2&gt;
&lt;p&gt;Here is an overview of the pros and cons of the options presented in more detail below:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;29%&#34; /&gt;
&lt;col width=&#34;35%&#34; /&gt;
&lt;col width=&#34;35%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Pros&lt;/th&gt;
&lt;th&gt;Cons&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1. &lt;code&gt;cronR&lt;/code&gt; Addin on &lt;code&gt;RStudio Server&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Simple and quick&lt;/td&gt;
&lt;td&gt;Not so robust, need to log into server to make changes, versioning packages.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2. &lt;code&gt;gce_vm_scheduler&lt;/code&gt; and &lt;code&gt;Dockerfiles&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Robust and can launch from local R session, support versioning&lt;/td&gt;
&lt;td&gt;Need to build Dockerfiles, all scripts on one VM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;3. Master &amp;amp; Slave VM&lt;/td&gt;
&lt;td&gt;Tailor a fresh VM for each script, cheaper&lt;/td&gt;
&lt;td&gt;Need to build Dockerfiles, more complicated VM setup.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;4. Google AppEngine with flexible containers&lt;/td&gt;
&lt;td&gt;Managed platform&lt;/td&gt;
&lt;td&gt;Need to turn script into web responses, more complicated setup&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;cronr-plus-rstudio-server&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1 - cronR plus RStudio Server&lt;/h2&gt;
&lt;p&gt;This is the simplest and the one to start with.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Start up an RStudio Server instance&lt;/li&gt;
&lt;li&gt;Install &lt;a href=&#34;https://github.com/bnosac/cronR&#34;&gt;&lt;code&gt;cronR&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Upload your R script&lt;/li&gt;
&lt;li&gt;Schedule your script using &lt;code&gt;cronR&lt;/code&gt; RStudio addin&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With &lt;code&gt;googleComputeEngineR&lt;/code&gt; and the new &lt;code&gt;gcer-public&lt;/code&gt; project containing public images that include one with &lt;code&gt;cronR&lt;/code&gt; already installed, this is as simple as the few lines of code below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleComputeEngineR)

## get the tag for prebuilt Docker image with googleAuthRverse, cronR and tidyverse
tag &amp;lt;- gce_tag_container(&amp;quot;google-auth-r-cron-tidy&amp;quot;, project = &amp;quot;gcer-public&amp;quot;)
# gcr.io/gcer-public/google-auth-r-cron-tidy

## start a custom Rstudio instance
vm &amp;lt;- gce_vm(name = &amp;quot;my-rstudio&amp;quot;,
              predefined_type = &amp;quot;n1-highmem-8&amp;quot;,
              template = &amp;quot;rstudio&amp;quot;,
              dynamic_image = tag,
              username = &amp;quot;me&amp;quot;, password = &amp;quot;mypassword&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait for it to launch and give you an IP, then log in, upload a script and configure the schedule via the &lt;code&gt;cronR&lt;/code&gt; addin.&lt;/p&gt;
&lt;p&gt;Some more detail about this workflow can be found at these &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/rstudio-team.html&#34;&gt;custom RStudio example workflows&lt;/a&gt; on the &lt;code&gt;googleComputeEngineR&lt;/code&gt; website.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gce_vm_scheduler-and-dockerfiles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2- &lt;em&gt;gce_vm_scheduler&lt;/em&gt; and &lt;em&gt;Dockerfiles&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;This method I prefer to the above since it lets you create the exact environment (e.g. package versions, dependencies) to run your script in, that you can trail dev and production versions with. It also works locally without needing to log into the server each time to deploy a script.&lt;/p&gt;
&lt;div id=&#34;handy-tools-for-docker---containerit-and-build-triggers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Handy tools for Docker - containerit and Build Triggers&lt;/h3&gt;
&lt;p&gt;Here we introduce Docker images, which may have been more a technical barrier for some before (but worth knowing, I think)&lt;/p&gt;
&lt;div id=&#34;containerit&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;containerit&lt;/h4&gt;
&lt;p&gt;Things are much easier now though, as we have the magic new R package &lt;a href=&#34;http://o2r.info/2017/05/30/containerit-package/&#34;&gt;&lt;code&gt;containerit&lt;/code&gt;&lt;/a&gt; which can generate these Docker files for you - just send &lt;code&gt;containerit::dockerfile()&lt;/code&gt; around the script file.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build-triggers&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Build Triggers&lt;/h4&gt;
&lt;p&gt;Along with auto-generating Dockerfiles, for Google Cloud in particular we now also have &lt;a href=&#34;https://cloud.google.com/container-builder/docs/how-to/build-triggers&#34;&gt;Build Triggers&lt;/a&gt; which automates building the Docker image for you.&lt;/p&gt;
&lt;p&gt;Just make the Dockerfile, then set up a trigger for when you push that file up to GitHub - you can see the ones used to create the public R resources here in the &lt;a href=&#34;https://github.com/cloudyr/googleComputeEngineR/tree/master/inst/dockerfiles&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt; repo&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;recipe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Recipe&lt;/h3&gt;
&lt;p&gt;Putting it all together then, documentation of this workflow for &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/scheduled-rscripts.html&#34;&gt;scheduling R scripts is found here&lt;/a&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If you don’t already have one, start up a scheduler VM using &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/gce_vm_scheduler.html&#34;&gt;&lt;code&gt;gce_vm_scheduler&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Create a Dockerfile either manually or using &lt;code&gt;containerit&lt;/code&gt; that will run your script upon execution&lt;/li&gt;
&lt;li&gt;Upload the Dockerfile to a git repo (private or public)&lt;/li&gt;
&lt;li&gt;Setup a build trigger for that Dockerfile&lt;/li&gt;
&lt;li&gt;Once built, set a script to schedule within that Dockerfile with &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/gce_schedule_docker.html&#34;&gt;&lt;code&gt;gce_schedule_docker&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is still in beta at time of writing but should be stable by the time &lt;code&gt;googlecomputeEngineR&lt;/code&gt; hits CRAN &lt;code&gt;0.2.0&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;master-and-slave-vms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3 - Master and Slave VMs&lt;/h2&gt;
&lt;p&gt;Some scripts take more resources than others, and since you are using VMs already you can have more control over what specifications of VM to launch based on the script you want to run.&lt;/p&gt;
&lt;p&gt;This means you can have a cheap scheduler server, that launch biggers VMs for the duration of the job. As GCP charges per minute, this can save you money over having a schedule server that is as big as what your most expensive script needs running 24/7.&lt;/p&gt;
&lt;p&gt;This method is largely like the scheduled scripts above, except in this case the scheduled script is also launching VMs to run the job upon.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;googleCloudStorageR::gcs_source&lt;/code&gt; you can run an R script straight from where it is hosted upon GCS, meaning all data, authentication files and scripts can be kept seperate from the computation. An example master script is shown below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## intended to be run on a small instance via cron
## use this script to launch other VMs with more expensive tasks
library(googleComputeEngineR)
library(googleCloudStorageR)
gce_global_project(&amp;quot;my-project&amp;quot;)
gce_global_zone(&amp;quot;europe-west1-b&amp;quot;)
gcs_global_bucket(&amp;quot;your-gcs-bucket&amp;quot;)

## auth to same project we&amp;#39;re on
googleAuthR::gar_gce_auth()

## launch the premade VM
vm &amp;lt;- gce_vm(&amp;quot;slave-1&amp;quot;)

## set SSH to use &amp;#39;master&amp;#39; username as configured before
vm &amp;lt;- gce_ssh_setup(vm, username = &amp;quot;master&amp;quot;, ssh_overwrite = TRUE)

## run the script on the VM that will source from GCS
runme &amp;lt;- &amp;quot;Rscript -e \&amp;quot;googleAuthR::gar_gce_auth();googleCloudStorageR::gcs_source(&amp;#39;download.R&amp;#39;, bucket = &amp;#39;your-gcs-bucket&amp;#39;)\&amp;quot;&amp;quot;
out &amp;lt;- docker_cmd(vm, 
                  cmd = &amp;quot;exec&amp;quot;, 
                  args = c(&amp;quot;rstudio&amp;quot;, runme), 
                  wait = TRUE)

## once finished, stop the VM
gce_vm_stop(vm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More detail is again available at the &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/scheduled-rscripts.html#master-slave-scheduler&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;google-app-engine-with-flexible-custom-runtimes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4 - Google App Engine with flexible custom runtimes&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/appengine/docs/&#34;&gt;Google App Engine&lt;/a&gt; has always had schedule options, but only for its supported languages of Python, Java, PHP etc. Now with the &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/&#34;&gt;introduction of flexible containers&lt;/a&gt;, any Docker container running any language (including R) can also be run.&lt;/p&gt;
&lt;p&gt;This is potentially the best solution since it runs upon a 100% managed platform, meaning you don’t need to worry about servers at all, and it takes care of things like server maintence, logging etc.&lt;/p&gt;
&lt;div id=&#34;setting-up-your-script-for-app-engine&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting up your script for App Engine&lt;/h3&gt;
&lt;p&gt;There are some &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/build&#34;&gt;requirements for the container&lt;/a&gt; that need configuring so it can run:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can not use &lt;code&gt;googleAuthR::gar_gce_auth()&lt;/code&gt; so will need to upload the auth token within the Dockerfile.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;AppEngine expects a web service to be listening on port 8080, so your schedule script needs to be triggered via HTTP requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For authentication, I use the system environment arguments (i.e. those usually set in &lt;code&gt;.Renviron&lt;/code&gt;) that &lt;code&gt;googleAuthR&lt;/code&gt; packages use for auto-authentication. Put the auth file (such as JSON or a &lt;code&gt;.httr-oauth&lt;/code&gt; file) into the deployment folder, then point to its location via specifying in the &lt;code&gt;app.yaml&lt;/code&gt;. Details below.&lt;/p&gt;
&lt;p&gt;To solve the need for being a webservice on port 8080 (which is then proxied to normal webports 80/443), &lt;a href=&#34;https://www.rplumber.io/&#34;&gt;&lt;code&gt;plumber&lt;/code&gt;&lt;/a&gt; is a great service by Jeff Allen of RStudio, which already comes with its own Docker solution. You can then modify that &lt;code&gt;Dockerfile&lt;/code&gt; slightly so that it works on App Engine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;recipe-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Recipe&lt;/h3&gt;
&lt;p&gt;To then schedule your R script on app engine, follow the guide below, first making sure you have setup the &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/&#34;&gt;gcloud CLI&lt;/a&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create a Google Appengine project in the US region (only region that supports flexible containers at the moment)&lt;/li&gt;
&lt;li&gt;Create a scheduled script e.g. &lt;code&gt;schedule.R&lt;/code&gt; - you can use auth from environment files specified in &lt;code&gt;app.yaml&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Make an API out of the script by using &lt;code&gt;plumber&lt;/code&gt; - example:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleAuthR)         ## authentication
library(googleCloudStorageR)  ## google cloud storage
library(readr)                ## 
## gcs auto authenticated via environment file 
## pointed to via sys.env GCS_AUTH_FILE

#* @get /demoR
demoScheduleAPI &amp;lt;- function(){
  
  ## download or do something
  something &amp;lt;- tryCatch({
      gcs_get_object(&amp;quot;schedule/test.csv&amp;quot;, 
                     bucket = &amp;quot;mark-edmondson-public-files&amp;quot;)
    }, error = function(ex) {
      NULL
    })
      
  something_else &amp;lt;- data.frame(X1 = 1,
                               time = Sys.time(), 
                               blah = paste(sample(letters, 10, replace = TRUE), collapse = &amp;quot;&amp;quot;))
  something &amp;lt;- rbind(something, something_else)
  
  tmp &amp;lt;- tempfile(fileext = &amp;quot;.csv&amp;quot;)
  on.exit(unlink(tmp))
  write.csv(something, file = tmp, row.names = FALSE)
  ## upload something
  gcs_upload(tmp, 
             bucket = &amp;quot;mark-edmondson-public-files&amp;quot;, 
             name = &amp;quot;schedule/test.csv&amp;quot;)
  
  message(&amp;quot;Done&amp;quot;, Sys.time())
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create Dockerfile. If using &lt;code&gt;containerit&lt;/code&gt; then replace FROM with &lt;code&gt;trestletech/plumber&lt;/code&gt; and add the below lines to use correct AppEngine port:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(containerit)

dockerfile &amp;lt;- dockerfile(&amp;quot;schedule.R&amp;quot;, copy = &amp;quot;script_dir&amp;quot;, soft = TRUE)
write(dockerfile, file = &amp;quot;Dockerfile&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then change/add these lines to the created Dockerfile:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;EXPOSE 8080
ENTRYPOINT [&amp;quot;R&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;pr &amp;lt;- plumber::plumb(commandArgs()[4]); pr$run(host=&amp;#39;0.0.0.0&amp;#39;, port=8080)&amp;quot;]
CMD [&amp;quot;schedule.R&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Example final Dockerfile below. This doesn’t need to be built in say a build trigger as its built upon app engine deployment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FROM trestletech/plumber
LABEL maintainer=&amp;quot;mark&amp;quot;
RUN export DEBIAN_FRONTEND=noninteractive; apt-get -y update \
 &amp;amp;&amp;amp; apt-get install -y libcairo2-dev \
    libcurl4-openssl-dev \
    libgmp-dev \
    libpng-dev \
    libssl-dev \
    libxml2-dev \
    make \
    pandoc \
    pandoc-citeproc \
    zlib1g-dev
RUN [&amp;quot;install2.r&amp;quot;, &amp;quot;-r &amp;#39;https://cloud.r-project.org&amp;#39;&amp;quot;, &amp;quot;readr&amp;quot;, &amp;quot;googleCloudStorageR&amp;quot;, &amp;quot;Rcpp&amp;quot;, &amp;quot;digest&amp;quot;, &amp;quot;crayon&amp;quot;, &amp;quot;withr&amp;quot;, &amp;quot;mime&amp;quot;, &amp;quot;R6&amp;quot;, &amp;quot;jsonlite&amp;quot;, &amp;quot;xtable&amp;quot;, &amp;quot;magrittr&amp;quot;, &amp;quot;httr&amp;quot;, &amp;quot;curl&amp;quot;, &amp;quot;testthat&amp;quot;, &amp;quot;devtools&amp;quot;, &amp;quot;hms&amp;quot;, &amp;quot;shiny&amp;quot;, &amp;quot;httpuv&amp;quot;, &amp;quot;memoise&amp;quot;, &amp;quot;htmltools&amp;quot;, &amp;quot;openssl&amp;quot;, &amp;quot;tibble&amp;quot;, &amp;quot;remotes&amp;quot;]
RUN [&amp;quot;installGithub.r&amp;quot;, &amp;quot;MarkEdmondson1234/googleAuthR@7917351&amp;quot;, &amp;quot;hadley/rlang@ff87439&amp;quot;]
WORKDIR /payload/
COPY [&amp;quot;.&amp;quot;, &amp;quot;./&amp;quot;]

EXPOSE 8080
ENTRYPOINT [&amp;quot;R&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;pr &amp;lt;- plumber::plumb(commandArgs()[4]); pr$run(host=&amp;#39;0.0.0.0&amp;#39;, port=8080)&amp;quot;]
CMD [&amp;quot;schedule.R&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Specify &lt;code&gt;app.yaml&lt;/code&gt; for flexible containers as &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/&#34;&gt;detailed here&lt;/a&gt;. Add any environment vars such as auth files, that will be included in same deployment folder.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;runtime: custom
env: flex

env_variables:
  GCS_AUTH_FILE: auth.json&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Specify &lt;code&gt;cron.yaml&lt;/code&gt; for the schedule needed:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;cron:
- description: &amp;quot;test cron&amp;quot;
  url: /demoR
  schedule: every 1 hours&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;You should now have these files in the deployment folder:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;app.yaml&lt;/code&gt; - configuration of general app settings&lt;/li&gt;
&lt;li&gt;&lt;code&gt;auth.json&lt;/code&gt; - an authentication file specified in env arguments or app.yaml&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cron.yaml&lt;/code&gt; - specification of when your scheduling is&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Dockerfile&lt;/code&gt; - specification of the environment&lt;/li&gt;
&lt;li&gt;&lt;code&gt;schedule.R&lt;/code&gt; - the plumber version of your script containing your endpoints&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Open the terminal in that folder, and deploy via &lt;code&gt;gcloud app deploy --project your-project&lt;/code&gt; and the cron schedule via &lt;code&gt;gcloud app deploy cron.yaml --project your-project&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It will take a while (up to 10 mins) the first time.&lt;/p&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The App Engine should then be deployed on &lt;a href=&#34;https://your-project.appspot.com/&#34; class=&#34;uri&#34;&gt;https://your-project.appspot.com/&lt;/a&gt; - every &lt;code&gt;GET&lt;/code&gt; request to &lt;a href=&#34;https://your-project.appspot.com/demoR&#34; class=&#34;uri&#34;&gt;https://your-project.appspot.com/demoR&lt;/a&gt; (or other endpoints you have specified in R script) will run the R code. The cron example above will run every hour to this endpoint.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Logs for the instance are found &lt;a href=&#34;https://console.cloud.google.com/logs/viewer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This approach is the most flexible, and offers a fully managed platform for your scripts. Scheduled scripts are only the beginning, since deploying such actually gives you a way to run R scripts in response to any HTTP request from any language - triggers could also include if someone updates a spreadsheet, adds a file to a folder, pushes to GitHub etc. which opens up a lot of exciting possibilities. You can also scale it up to become a fully functioning R API.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Hopefully this has given you an idea on your options for R on Google Cloud regarding scheduling. If you have some other easier workflows or suggestions for improvements please put them in the comments below!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A digital analytics workflow through the Google Cloud using R</title>
      <link>http://code.markedmondson.me/digital-analytics-workflow-through-google-cloud/</link>
      <pubDate>Mon, 10 Oct 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/digital-analytics-workflow-through-google-cloud/</guid>
      <description>

&lt;p&gt;There are now several packages built upon the &lt;code&gt;googleAuthR&lt;/code&gt; framework which are helpful to a digital analyst who uses R, so this post looks to demonstrate how they all work together.  If you&amp;rsquo;re new to R, and would like to know how it helps with your digital analytics, &lt;a href=&#34;http://analyticsdemystified.com/blog/tim-wilson/&#34;&gt;Tim Wilson&lt;/a&gt; and I ran a workshop last month aimed at getting a digital analyst up and running.  The course material is online at &lt;a href=&#34;http://www.dartistics.com/&#34;&gt;www.dartistics.com&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;diagram&#34;&gt;Diagram&lt;/h2&gt;

&lt;p&gt;A top level overview is shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/r-infrastructure.png&#34; alt=&#34;R-infrastructure-digital-analytics&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The diagram shows &lt;a href=&#34;http://code.markedmondson.me/googleAuthR/&#34;&gt;&lt;code&gt;googleAuthR&lt;/code&gt;&lt;/a&gt; packages, other packages, servers and APIs all of which interact to turn data into actionable analytics.&lt;/p&gt;

&lt;p&gt;The most recent addition is &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt;&lt;/a&gt; which has helped make great savings in the time it takes to set up servers.  I have in the past blogged about &lt;a href=&#34;http://code.markedmondson.me/setting-up-scheduled-R-scripts-for-an-analytics-team/&#34;&gt;setting up R servers in the Google Cloud&lt;/a&gt;, but it was still more dev-ops than I really wanted to be doing.  Now, I can do setups similar to those I have written about in a couple of lines of R.&lt;/p&gt;

&lt;h2 id=&#34;data-workflow&#34;&gt;Data workflow&lt;/h2&gt;

&lt;p&gt;A suggested workflow for working with data is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Infrastructure&lt;/em&gt; - Creating a computer to run your analysis.  This can be either your own laptop, or a server in the cloud.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Collection&lt;/em&gt; - Downloading your raw data from one or multiple sources, such as APIs or your CRM database.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Transformation&lt;/em&gt; - Turning your raw data into useful information via ETL, modelling or statistics.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Storage&lt;/em&gt; - Storing the information you have created so that it can be automatically updated and used.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Output&lt;/em&gt; - Displaying or using the information into an end user application.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The components of the diagram can be combined into the workflow above.  You can swap out various bits for your own needs, but its possible to use R for all of these steps.&lt;/p&gt;

&lt;p&gt;You could do all of this with an Excel workbook on your laptop.  However, as data analysis becomes more complicated, it makes sense to start breaking out the components into more specialised tools, since Excel will start to strain when you increase the data volume or want reproducibility.&lt;/p&gt;

&lt;h3 id=&#34;infrastructure&#34;&gt;Infrastructure&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt;&lt;/a&gt; uses the Google Clouds&amp;rsquo; virtual machine instances to create servers, with specific support for R.&lt;/p&gt;

&lt;p&gt;It uses &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker containers&lt;/a&gt; to make launching the servers and applications you need quick and simple, without you needing to know too much dev-ops to get started.&lt;/p&gt;

&lt;p&gt;Due to Docker, the applications created can be more easily transferred into other IT environments, such as within internal client intranets or AWS.&lt;/p&gt;

&lt;p&gt;To help with a digital analytics workflow, &lt;code&gt;googleComputeEngineR&lt;/code&gt; includes templates for the below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RStudio Server&lt;/strong&gt; - an R IDE you can work with from your browser. The server edition means you can access it from anywhere, and can always ensure the correct packages are installed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shiny&lt;/strong&gt; - A server to run your Shiny apps that display your data in dashboards or applications end users can work with.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenCPU&lt;/strong&gt; - A server that turns your R code into a JSON API.  Used for turning R output into a format web development teams can use straight within a website.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For instance, launching an RStudio server is as simple as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleComputeEngineR)
vm &amp;lt;- gce_vm_template(&amp;quot;rstudio&amp;quot;, 
                      name = &amp;quot;rstudio-server&amp;quot;, 
                      predefined_type = &amp;quot;f1-micro&amp;quot;, 
                      username = &amp;quot;mark&amp;quot;, 
                      password = &amp;quot;mark1234&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The instance will launch within a few minutes and give you a URL you can then login with.&lt;/p&gt;

&lt;h3 id=&#34;collection&#34;&gt;Collection&lt;/h3&gt;

&lt;p&gt;Once you are logged in to your RStudio Server, you can use all of R&amp;rsquo;s power to download and work with your data.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;googleAuthR&lt;/code&gt; packages can all be authenticated under the same OAuth2 token, to simplify access.&lt;/p&gt;

&lt;p&gt;Other packages useful to digital analytics include APIs such as &lt;a href=&#34;https://github.com/randyzwitch/RSiteCatalyst&#34;&gt;&lt;code&gt;RSiteCatalyst&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;twitteR&lt;/code&gt;.  A full list of digital analytics R packages can be found in the &lt;a href=&#34;https://cran.r-project.org/web/views/WebTechnologies.html&#34;&gt;web technologies section of CRAN Task Views&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another option is the R package &lt;a href=&#34;https://github.com/jennybc/googlesheets&#34;&gt;&lt;code&gt;googlesheets&lt;/code&gt;&lt;/a&gt; by Jenny Bryan, which could either be used as a data source or as a data storage for reports, to be processed onwards later.&lt;/p&gt;

&lt;p&gt;The below example R script fetches data from Google Analytics, SEO data from Google Search Console and CRM data from BigQuery.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleAnalyticsR)
library(searchConsoleR)
library(bigQueryR)
library(googleAuthR)

## authenticate with all services 
gar_auth_service(&amp;quot;auth.json&amp;quot;)

## get search console data
seo_data &amp;lt;- search_analytics(&amp;quot;http://example.com&amp;quot;, 
                             &amp;quot;2015-07-01&amp;quot;, &amp;quot;2015-07-31&amp;quot;, 
                              c(&amp;quot;date&amp;quot;, &amp;quot;query&amp;quot;, &amp;quot;page&amp;quot;))

## get google analytics data
ga_data &amp;lt;- google_analytics_4(1235678,
                              c(&amp;quot;2015-07-01&amp;quot;, &amp;quot;2015-07-31&amp;quot;),
                              metrics = c(&amp;quot;users&amp;quot;),
                              dimensions = c(&amp;quot;date&amp;quot;, &amp;quot;userID&amp;quot; , &amp;quot;landingPagePath&amp;quot;, &amp;quot;campaign&amp;quot;))

## get CRM data from BigQuery
crm_data &amp;lt;- bqr_query(&amp;quot;big-query-database&amp;quot;, &amp;quot;my_data&amp;quot;,
                      &amp;quot;SELECT userID, lifetimeValue FROM [my_dataset]&amp;quot;)
                              
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;transformation&#34;&gt;Transformation&lt;/h3&gt;

&lt;p&gt;This ground is well covered by existing R packages.  My suggestion here is to embrace the &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyverse/index.html&#34;&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/a&gt; and use that to create and generate your information.&lt;/p&gt;

&lt;p&gt;Applications include anomaly detection, measurement of causal effect, clustering and forecasting. Hadley Wickham&amp;rsquo;s book &lt;a href=&#34;http://r4ds.had.co.nz/index.html&#34;&gt;&amp;ldquo;R for Data Science&amp;rdquo;&lt;/a&gt; is a recent compendium of knowledge on this topic, which includes this suggested work flow:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/tidy-r.png&#34; alt=&#34;tidyr&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;storage&#34;&gt;Storage&lt;/h3&gt;

&lt;p&gt;Once you have your data in the format you want, you often need to keep it somewhere it is easily accessible for other systems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/storage/&#34;&gt;Google Cloud Storage&lt;/a&gt; is a cheap, reliable method of storing any type of data object so that its always available for further use, and is heavily used within Google Cloud applications as a central data store.  I use it for storing &lt;code&gt;RData&lt;/code&gt; files or storing &lt;code&gt;csv&lt;/code&gt; files with a public link that is emailed to users when available.  It is accessible in R via the &lt;a href=&#34;http://code.markedmondson.me/googleCloudStorageR/&#34;&gt;&lt;code&gt;googleCloudStorageR&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;

&lt;p&gt;For database style access, &lt;a href=&#34;https://cloud.google.com/bigquery/&#34;&gt;BigQuery&lt;/a&gt; can be queried from many data services, including data visualisation platforms such as Google&amp;rsquo;s Data Studio or Tableau.  BigQuery offers incredibly fast analytical queries for TBs of data, accessible via the &lt;a href=&#34;http://code.markedmondson.me/bigQueryR/&#34;&gt;&lt;code&gt;bigQueryR&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;

&lt;p&gt;An example of uploading data is below - again only one authentication is needed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleCloudStorageR)
library(bigQueryR)

## authenticate with all services 
gar_auth_service(&amp;quot;auth.json&amp;quot;)

## upload to Big Query
bqr_upload_data(&amp;quot;projectID&amp;quot;, &amp;quot;datasetID&amp;quot;, &amp;quot;tableID&amp;quot;,
                my_data_for_sql_queries)

## upload to Google Cloud Storage
gcs_upload(&amp;quot;my_rdata_file&amp;quot;)


&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;output&#34;&gt;Output&lt;/h3&gt;

&lt;p&gt;Finally, outputs include &lt;a href=&#34;http://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt; apps, &lt;a href=&#34;http://rmarkdown.rstudio.com/&#34;&gt;RMarkdown&lt;/a&gt;, a &lt;a href=&#34;https://gist.github.com/MarkEdmondson1234/ddcac436cbdfd4557639522573bfc7b6&#34;&gt;scheduled email&lt;/a&gt; or an R API call using &lt;a href=&#34;https://www.opencpu.org/&#34;&gt;OpenCPU&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All &lt;code&gt;googleAuthR&lt;/code&gt; functions are Shiny and RMarkdown compatible for user authentication - this means a user can login themselves and access their own data whilst using the logic of your app to gain insight, without you needing access to their data at all.  An example of an RMarkdown app taking advantage of this is the demo of the &lt;a href=&#34;https://github.com/MarkEdmondson1234/gentelellaShiny&#34;&gt;GentelellaShiny GA dashboard&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/gentellaShinyGA.png&#34; alt=&#34;gentella-shiny&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can launch OpenCPU and Shiny servers just as easily as RStudio Server via &lt;code&gt;googleComputeEngineR&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleComputeEngineR)

## creates a Shiny server
vm2 &amp;lt;- gce_vm_template(&amp;quot;shiny&amp;quot;, 
                      name = &amp;quot;shiny-server&amp;quot;, 
                      predefined_type = &amp;quot;f1-micro&amp;quot;, 
                      username = &amp;quot;mark&amp;quot;, 
                      password = &amp;quot;mark1234&amp;quot;)

## creates an OpenCPU server                      
vm3 &amp;lt;- gce_vm_template(&amp;quot;opencpu&amp;quot;, 
                      name = &amp;quot;opencpu-server&amp;quot;, 
                      predefined_type = &amp;quot;f1-micro&amp;quot;, 
                      username = &amp;quot;mark&amp;quot;, 
                      password = &amp;quot;mark1234&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Shiny Apps or RMarkdown HTML files can then be uploaded for display to the end user.  If the server needs more power, simply save the container and relaunch with a bigger RAM or CPU.&lt;/p&gt;

&lt;p&gt;OpenCPU is the technology demonstrated in my recent EARL London talk on using R to &lt;a href=&#34;http://code.markedmondson.me/predictClickOpenCPU/supercharge.html&#34;&gt;forecast HTML prefetching and deploying through Google Tag Manager&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Your Shiny, RMarkdown or OpenCPU functions can download data via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleCloudStorageR)
library(bigQueryR)

## authenticate with all services 
gar_auth_service(&amp;quot;auth.json&amp;quot;)

## download Google Cloud Storage
my_data &amp;lt;- gce_get_object(&amp;quot;my_rdata_file&amp;quot;)

## query data from Big Query dependent on user input
query &amp;lt;- bqr_query(&amp;quot;big-query-database&amp;quot;, &amp;quot;my_data&amp;quot;,
                   &amp;quot;SELECT userID, lifetimeValue FROM [my_dataset]&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Hopefully this has shown where some efficiencies could be made in your own digital analysis. For me, the reduction of computer servers to atoms of work has expanded the horizons on what is possible: applications such as sending big calculations to the cloud if taking too long locally; being able to send clients entire clusters of computers with a data application ready and working; and having customised R environments for every occasion, such as R workshops.&lt;/p&gt;

&lt;p&gt;For the future, I hope to introduce Spark clusters via &lt;a href=&#34;https://cloud.google.com/dataproc/&#34;&gt;Google Dataproc&lt;/a&gt;, giving the ability to use machine learning directly on a dataset without needing to download locally; scheduled scripts that launch servers as needed; and working with Google&amp;rsquo;s newly launched &lt;a href=&#34;https://cloud.google.com/ml/&#34;&gt;machine learning APIs&lt;/a&gt; that dovetail into the Google Cloud.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scheduling R scripts for a team using RStudio Server, Docker, Github and Google Compute Engine</title>
      <link>http://code.markedmondson.me/setting-up-scheduled-R-scripts-for-an-analytics-team/</link>
      <pubDate>Thu, 21 Apr 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/setting-up-scheduled-R-scripts-for-an-analytics-team/</guid>
      <description>

&lt;p&gt;&lt;em&gt;edit 20th November, 2016 - now everything in this post is abstracted away and available in the googleComputeEngineR package - I would say its a lot easier to use that.  Here is a post on getting started with it. &lt;a href=&#34;http://code.markedmondson.me/launch-rstudio-server-google-cloud-in-two-lines-r/&#34;&gt;http://code.markedmondson.me/launch-rstudio-server-google-cloud-in-two-lines-r/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This blog will give you steps that allows you to run on Google Compute Engine a server that has these features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RStudio Server instance with multiple login.&lt;/li&gt;
&lt;li&gt;Apache to host a welcome webpage.&lt;/li&gt;
&lt;li&gt;Restart the server to securely load private Github R packages / Docker images.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;A shared cron folder that runs every day at 0630 that user&amp;rsquo;s can put their own scripts into.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is inspired by conversations with &lt;a href=&#34;https://twitter.com/chipoglesby&#34;&gt;&lt;code&gt;@Chipoglesby&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/RichardFergie&#34;&gt;&lt;code&gt;@RichardFergie&lt;/code&gt;&lt;/a&gt; on twitter about this kind of setup, and was useful to me as a way to organise my thoughts on the subject ;).  If you have any suggestions on improvements please tweet to me at &lt;a href=&#34;https://twitter.com/HoloMarkeD&#34;&gt;&lt;code&gt;@HoloMarkeD&lt;/code&gt;&lt;/a&gt; or email via the blog.&lt;/p&gt;

&lt;h2 id=&#34;motivation-in-search-of-a-4-day-work-week&#34;&gt;Motivation: In search of a 4-day work week&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ve just started a new job at &lt;a href=&#34;http://iihnordic.dk/&#34;&gt;IIH Nordic&lt;/a&gt; where a part of my duties is to look for ways to automate the boring tasks that come up in a digital web analytics team.&lt;/p&gt;

&lt;p&gt;IIH Nordic have an aim to introduce a 4-day work week within a couple of years, so the motivation is to find out how to save 20% of the current work schedule without harming productivity.  Downloading data and creating reports is a big time-sink that looks ripe for optimisation.&lt;/p&gt;

&lt;h3 id=&#34;introducing-the-wonders-of-r-to-a-team&#34;&gt;Introducing the wonders of R to a team&lt;/h3&gt;

&lt;p&gt;The current strategy is to introduce the team to R and train everyone up in running simple scripts that download the data they need for their reports.  We will tackle data transformation/ETL, statistics and visualisation/presentation later, but the first goal is to solve the &amp;ldquo;How can I get data&amp;rdquo; problem.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m well placed to help due to writing &lt;a href=&#34;https://github.com/MarkEdmondson1234/googleAuthR&#34;&gt;&lt;code&gt;googleAuthR&lt;/code&gt;&lt;/a&gt;, so have played with R libraries with simple authentication and functions to download from &lt;a href=&#34;https://github.com/MarkEdmondson1234/googleAnalyticsR_public&#34;&gt;&lt;code&gt;Google Analytics&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/MarkEdmondson1234/searchConsoleR&#34;&gt;&lt;code&gt;Search Console&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/MarkEdmondson1234/bigQueryR&#34;&gt;&lt;code&gt;BigQuery&lt;/code&gt;&lt;/a&gt;, as well as using Randy Zwitch&amp;rsquo;s &lt;a href=&#34;https://github.com/randyzwitch/RSiteCatalyst&#34;&gt;&lt;code&gt;Adobe Analytics&lt;/code&gt;&lt;/a&gt; package.  These libraries alone cover 90% of the data sources we need.&lt;/p&gt;

&lt;h3 id=&#34;why-use-rstudio-server-in-the-cloud&#34;&gt;Why use RStudio Server in the cloud?&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;ve had a few internal workshops now and the team look comfortable writing the R-scripts, but during the process I noticed a few issues that are solved using RStudio Server.  Using it means:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Everyone is working on the same versions and libraries.&lt;/li&gt;
&lt;li&gt;Scripts can be logged for quality and errors.&lt;/li&gt;
&lt;li&gt;Scheduled scripts can be run from the server not someones local PC.&lt;/li&gt;
&lt;li&gt;Training material can be made available on same server.&lt;/li&gt;
&lt;li&gt;Web accessible private login for remote working.&lt;/li&gt;
&lt;li&gt;Can use R from an iPad or Google Chromebook!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;setting-up-rstudio-server-on-google-cloud-compute-gce&#34;&gt;Setting up RStudio Server on Google Cloud Compute (GCE)&lt;/h2&gt;

&lt;p&gt;I have a old blog post on the &lt;a href=&#34;http://markedmondson.me/run-r-rstudio-and-opencpu-on-google-compute-engine-free-vm-image&#34;&gt;installation of RStudio Server on GCE&lt;/a&gt;, but this will update that since technology and my understanding has improved.&lt;/p&gt;

&lt;p&gt;A brief overview of the components are below:&lt;/p&gt;

&lt;h3 id=&#34;docker&#34;&gt;Docker&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt; is a virtual machine-lite that can run on anything, which means that the set-up is very transferable to other operating systems like Windows or OSX.&lt;/p&gt;

&lt;p&gt;It offers easier set-up via pre-created Docker images, and is embraced and &lt;a href=&#34;https://cloud.google.com/compute/docs/containers/container_vms&#34;&gt;well supported by Google&lt;/a&gt; via its &lt;a href=&#34;https://cloud.google.com/container-registry/docs/&#34;&gt;container registry&lt;/a&gt;, giving you unlimited private repositories, unlike Docker Hub that gives you only one.  It provides reproducibility and scale.&lt;/p&gt;

&lt;h3 id=&#34;github&#34;&gt;Github&lt;/h3&gt;

&lt;p&gt;Github is the go-to place to review and share code, and offers web-hooks that means it can push updates when you update a repository.  Every coder should use a version control system so its good practice to introduce it to the team, and it also allows installation of experimental and private R packages via &lt;code&gt;devtools::install_github&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;Google also has support for using Github via its &lt;a href=&#34;https://cloud.google.com/source-repositories/docs/setting-up-repositories&#34;&gt;Cloud Source Repositories&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;google-compute-engine&#34;&gt;Google Compute Engine&lt;/h3&gt;

&lt;p&gt;The up-and-coming pretender to AWS&amp;rsquo;s cloud computing crown, the Google Cloud is a natural choice for me as it offers integrations for Google Analytics and BigQuery.  The server images it offers have a great API and user interface, and it offers very quick I/O and restart times on Google&amp;rsquo;s world-class infrastructure, and the Docker container support as mentioned makes things more simple to scale in the future.&lt;/p&gt;

&lt;h2 id=&#34;setup-steps&#34;&gt;Setup steps&lt;/h2&gt;

&lt;p&gt;Now we get to the actual commands you use to get things up and running.  It will give you:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RStudio Server instance with multiple login.&lt;/li&gt;
&lt;li&gt;Apache to host a welcome webpage.&lt;/li&gt;
&lt;li&gt;Restart the server to load your private Github R packages if you have any.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;A shared cron folder that runs every day at 0430 that user&amp;rsquo;s can put their own scripts into.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In general I use the command line from a local terminal, but all actions can also be carried out within the &lt;a href=&#34;https://console.cloud.google.com/compute/instances&#34;&gt;Compute Engine web interface&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;create-the-google-cloud-engine&#34;&gt;Create the Google Cloud Engine&lt;/h3&gt;

&lt;p&gt;Here is a reference for &lt;a href=&#34;https://cloud.google.com/compute/docs/containers/vm-image/&#34;&gt;GCE docker enabled VMs&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download and install the &lt;a href=&#34;https://cloud.google.com/sdk/&#34;&gt;Google Cloud SDK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Authorize and set-up billing for a Google Project&lt;/li&gt;
&lt;li&gt;Start up your terminal and set your project and zone:&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;  gcloud auth login
  
  gcloud config set project your-project-name
  
  gcloud config set compute/region europe-west1
  
  gcloud config set compute/zone europe-west1-b
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Issue this &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/reference/compute/instances/create&#34;&gt;gcloud create command&lt;/a&gt; to start up a docker enabled instance:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;edit 18th September 2016 to update to new images as per &lt;a href=&#34;https://cloud.google.com/compute/docs/containers/vm-image/&#34;&gt;https://cloud.google.com/compute/docs/containers/vm-image/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud compute instances create your-r-server-name \
  --image-family gci-stable \
  --image-project google-containers \
  --tags http-server
  --scopes https://www.googleapis.com/auth/devstorage.read_write \
  --machine-type n1-standard-1

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unlike previously, this new (from Aug 2016) container ready VM comes with Google Cloud Storage scopes already set, so you can use private docker repos.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;--tags&lt;/code&gt; flag sets the default http firewall rules to apply to this instance so we can reach it via the internet on port 80.&lt;/p&gt;

&lt;p&gt;You should now be able to see your instance has launched in the &lt;a href=&#34;https://console.cloud.google.com/compute/instances&#34;&gt;Google Compute Dashboard&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;create-persistent-disks&#34;&gt;Create persistent disks&lt;/h3&gt;

&lt;p&gt;Anything new in the Docker container will disappear on a restart if you also don&amp;rsquo;t commit and push the image, so to avoid that data and scripts are linked to the container via the &lt;code&gt;--volumes&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;We also link to persistent disk rather than the VM&amp;rsquo;s own, which has the advantage of being able to connect (read-only) to multiple servers at the same time, should you need it.  This also means if you need a more powerful server, you can safely create one knowing you will have the same data and scripts available.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud compute disks create --size=200GB my-data-disk
gcloud compute instances attach-disk your-r-server-name --disk my-data-disk
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;initial-configuration-on-google-compute-engine&#34;&gt;Initial Configuration on Google Compute Engine&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Login to GCE&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;  gcloud compute ssh your-r-server-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are now making commands from within the GCE VM.&lt;/p&gt;

&lt;p&gt;If you have any problems, use the web interface to login via the Cloud Shell.  Sometimes you had to add your username to the instance, and then login with that user like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;  gcloud compute ssh user@your-r-server-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See &lt;a href=&#34;https://cloud.google.com/compute/docs/troubleshooting#ssherrors&#34;&gt;here&lt;/a&gt; for more diagnostics help.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Format and mount the persistent disk&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;See instructions &lt;a href=&#34;https://cloud.google.com/compute/docs/disks/persistent-disks&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;```sh
sudo mkfs.ext4 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/disk/by-id/google-persistent-disk-1

##make mount point
sudo mkdir /mnt/data/
sudo mount -o discard,defaults /dev/disk/by-id/google-persistent-disk-1 /mnt/data/

## user folders
sudo mkdir /mnt/data/home/

## custom packages
sudo mkdir /mnt/data/R/

##permissions
sudo chmod a+w /mnt/data

## make it mount automatically each time
echo &#39;/dev/disk/by-id/google-persistent-disk-1 /mnt/data ext4 discard,defaults 1 1&#39; | sudo tee -a /etc/fstab
```
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Install and Configure Apache&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This gives you a user-friendly webpage pointing to the RStudio login, and I also use it as a place to put training material such as RMarkdown documents.&lt;/p&gt;

&lt;p&gt;I suppose this could also be done via another Docker container if you have a more complicated Apache setup to use.&lt;/p&gt;

&lt;p&gt;The ProxyPassMatch line is needed for the Shiny engine thats within RStudio Server to work.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo apt-get update
sudo apt-get -y install apache2

## Need proxy and proxy_http to create nice URLs, proxy_wstunnel for Shiny
sudo a2enmod proxy proxy_http proxy_wstunnel

## this may be 000-default.conf rather than 000-default 
## depending on version of apache
echo &#39;&amp;lt;VirtualHost *:80&amp;gt;
        ServerAdmin your@email.com

        DocumentRoot /var/www

        ProxyPassMatch ^/rstudio/p/([0-9]+)/(websocket|.*/websocket)/$ ws://localhost:8787/p/$1/$2/
        ProxyPass /rstudio/ http://localhost:8787/
        ProxyPassReverse /rstudio/ http://localhost:8787/
        RedirectMatch permanent ^/rstudio$ /rstudio/

        ErrorLog ${APACHE_LOG_DIR}/error.log

        LogLevel warn

        CustomLog ${APACHE_LOG_DIR}/access.log combined

&amp;lt;/VirtualHost&amp;gt;&#39; | sudo tee /etc/apache2/sites-enabled/000-default

sudo service apache2 restart

## A startup HTML page for you to customise
echo &#39;&amp;lt;!doctype html&amp;gt;&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;RStudio on Google Cloud Compute&amp;lt;/h1&amp;gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;./rstudio/&amp;quot;&amp;gt;RStudio Server&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;&#39; | sudo tee /var/www/index.html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should now be able to see your server running Apache.&lt;/p&gt;

&lt;p&gt;I then upload a website to &lt;code&gt;/var/www/&lt;/code&gt; via the &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/reference/compute/copy-files&#34;&gt;&lt;code&gt;gcloud copy-files&lt;/code&gt; command&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;An example, assuming your website is in your local folder &lt;code&gt;~/dev/website/www/&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo gcloud --project your-project-name compute copy-files ~/dev/website/www/ your-r-server-name:/var/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is what we have at IIH Nordic:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/rstudio-server-webpage.png&#34; alt=&#34;batching_example&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;a-download-folder&#34;&gt;A download folder&lt;/h4&gt;

&lt;p&gt;Now, you may want to have a dropbox style folder for the data your scripts are running, say scheduled data downloads.  There are a few ways to skin this cat, such as uploading to cloud storage in your script, but the simplest way for me was to use the Apache functionality to create a logged in download area.&lt;/p&gt;

&lt;p&gt;For this, you need to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create a folder on your data disk where scripts will dump their data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir /mnt/data/downloads
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a symlink to an Apache web server folder&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ln -s /mnt/data/downloads /var/www/downloads
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[optional] Style the folder with CSS so it looks nice using say &lt;a href=&#34;http://adamwhitcroft.com/apaxy/&#34;&gt;Apaxy&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Update your Apache config to have logged in access to the folder.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The extra Apache config is below.  It requires installation of &lt;code&gt;sudo a2enmod headers&lt;/code&gt; and a restart.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;### In download folder, auto-download .txt,.csv, and .pdf files
### Protect with a password
&amp;lt;Directory /var/www/downloads/&amp;gt;
      &amp;lt;FilesMatch &amp;quot;.+\.(txt|csv|pdf)$&amp;quot;&amp;gt;
          ForceType application/octet-stream
          Header set Content-Disposition attachment
      &amp;lt;/FilesMatch&amp;gt;
      AuthType Basic
      AuthName &amp;quot;Enter password&amp;quot;
      AuthUserFile /etc/apache2/.htpasswd
      Require valid-user
      Order allow,deny
      Allow from all
      Options Indexes FollowSymLinks
      AllowOverride All
&amp;lt;/Directory&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;download-and-run-the-rstudio-server-docker-image&#34;&gt;Download and Run the RStudio Server Docker image&lt;/h3&gt;

&lt;p&gt;We first download a pre-prepared RStudio Docker container created by the &lt;a href=&#34;http://dirk.eddelbuettel.com/blog/2014/10/23/&#34;&gt;Rocker&lt;/a&gt; team led by Dirk Eddelbuettel. This is what our custom image will be based upon.  Here we download a variant that also loads &lt;a href=&#34;https://hub.docker.com/r/rocker/hadleyverse/&#34;&gt;RStudio and all of Hadley&amp;rsquo;s packages&lt;/a&gt; to give us a great base to work from.&lt;/p&gt;

&lt;p&gt;Most of the below is gleaned from the &lt;a href=&#34;https://github.com/rocker-org/rocker/wiki&#34;&gt;Rocker Wiki&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The below docker command runs the docker image - if that image is not available it will look for the image on Docker hub and download. This will take a long time as it downloads everything, perhaps time to put the kettle on.&lt;/p&gt;

&lt;p&gt;Subsequent times will load quickly from local version.&lt;/p&gt;

&lt;p&gt;We run it with a custom username and password we want, as this will be exposed to the web and we don&amp;rsquo;t want the defaults to be exposed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## Run the docker image with RStudio and Hadley Universe
sudo docker run --name rstudio-server -d -p 8787:8787 \
     -e USER=YOUR_USERNAME -e PASSWORD=YOUR_PW \
     -v /mnt/data/:/home/ \
     rocker/hadleyverse
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;configure-rstudio-server-docker-image&#34;&gt;Configure RStudio Server Docker image&lt;/h3&gt;

&lt;p&gt;Now it could be that you are done from here - you should have a working RStudio interface available on the IP of your container (&lt;a href=&#34;http://your-vm-ip-address/rstudio/auth-sign-in&#34;&gt;http://your-vm-ip-address/rstudio/auth-sign-in&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;But we will configure it a bit more, adding users, more packages, and scheduled jobs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you have a lot of configurations then it is better to create your own &lt;a href=&#34;https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/&#34;&gt;DOCKERFILE&lt;/a&gt; and build the image yourself.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We log in to the running docker container here:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo docker exec -it rstudio-server bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a good one to remember for when you are doing future customisation.&lt;/p&gt;

&lt;p&gt;You are now in the Docker container.&lt;/p&gt;

&lt;p&gt;Install stuff then CTRL-D to come out again to commit and push your changes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## Make users that will create a directory on the data disk
adduser mark

## [Optional] Install packages
## install as sudo to ensure all user&#39;s have access
sudo su - -c &amp;quot;R -e \&amp;quot;install.packages(&#39;abc&#39;, repos=&#39;http://cran.rstudio.com/&#39;)\&amp;quot;&amp;quot;

## [Optional] Install libraries from Github packages
sudo su - -c &amp;quot;R -e \&amp;quot;devtools::install_github(&#39;MarkEdmondson1234/bigQueryR&#39;)\&amp;quot;&amp;quot;

## [Optional] Install libraries from private Github packages
sudo su - -c &amp;quot;R -e \&amp;quot;devtools::install_github(&#39;MarkEdmondson1234/privatePackge&#39;, auth_token=YOUR_GITHUB_PAT)\&amp;quot;&amp;quot;
````

### Configure scheduling via CRON

A big reason to have a server is for the team to schedule their data fetching scripts.  We achieve this by running CRON within the Docker container (to ensure all packages are installed) and then providing a link to a folder that runs the script when they need it.

First we install CRON in the container:

```sh
## Download and install cron
sudo apt-get update
sudo apt-get -y install cron nano
sudo service cron start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then schedule scripts via RScript to run daily.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## annoying nano bug in Docker
export TERM=xterm

## open up the cron editor (select 2)
sudo crontab -e

## add this to the bottom of file
## runs script at 0420 every day
# m h  dom mon dow   command
20 4 * * * /home/cron/r-cron.R &amp;gt;/home/cron/cronlog.log 2&amp;gt;&amp;amp;1

## CTRL-X and Y to save changes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;r-cron.R&lt;/code&gt; script needs to have &lt;code&gt;#!/usr/bin/Rscript&lt;/code&gt; at the top to run correctly.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#!/usr/bin/Rscript
cat(&amp;quot;Cron script started: &amp;quot;, date())

....do R stuff

cat(&amp;quot;Cron script stopped: &amp;quot;, date())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Repeat for all scripts you want to run.&lt;/p&gt;

&lt;h3 id=&#34;sending-emails&#34;&gt;Sending emails&lt;/h3&gt;

&lt;p&gt;Its useful to send an email once a script has run successfully (or not), one that uses Mailrun is below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Email a user a report is ready
#&#39;
#&#39; Requires an account at Mailgun: https://mailgun.com
#&#39; Pre-verification can only send to a whitelist of emails you configure
#&#39;
#&#39; @param email Email to send to
#&#39; @param mail_message Any extra info
#&#39;
#&#39; @return TRUE if successful email sent
#&#39; @import httr
#&#39; @export
sendEmail &amp;lt;- function(email = &amp;quot;XXXXX@you.com&amp;quot;,
                      mail_message = &amp;quot;Hello&amp;quot;){

  url &amp;lt;- &amp;quot;https://api.mailgun.net/v3/sandbox5f2XXXXXXXa.mailgun.org/messages&amp;quot;
  ## username:password so api_key is all after the api:
  api_key &amp;lt;- &amp;quot;key-c5957XXXXXXXXXXXbb9cf8ce&amp;quot;
  the_body &amp;lt;-
    list(
      from=&amp;quot;Mailgun Sandbox &amp;lt;postmaster@sandbox5XXXXXXXXa.mailgun.org&amp;gt;&amp;quot;,
      to=email,
      subject=&amp;quot;Mailgun from R&amp;quot;,
      text=mailmessage,
    )

  req &amp;lt;- httr::POST(url,
                    httr::authenticate(&amp;quot;api&amp;quot;, api_key),
                    encode = &amp;quot;form&amp;quot;,
                    body = the_body)

  httr::stop_for_status(req)
  
  TRUE

}

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;scheduling-packages&#34;&gt;Scheduling packages&lt;/h3&gt;

&lt;p&gt;There is also an R package that manages cron, &lt;a href=&#34;https://github.com/bnosac/cronR&#34;&gt;cronR&lt;/a&gt; , which now has an RStudio Server addin, which looks like a good option.&lt;/p&gt;

&lt;h2 id=&#34;pushing-the-docker-changes&#34;&gt;Pushing the Docker changes&lt;/h2&gt;

&lt;p&gt;We now commit and push changes to the &lt;a href=&#34;https://cloud.google.com/container-registry/docs/&#34;&gt;Google Docker Hub&lt;/a&gt;.  The Docker command docs are &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/commit/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## CTRL-D to come out of the docker container again

## to get the container id e.g. c3f279d17e0a
sudo docker ps 

## commit with a message
sudo docker commit -a &amp;quot;Mark&amp;quot; -m &amp;quot;Added R stuff&amp;quot; \
    CONTAINER_ID yourname/your-new-docker-image

## list your new image with the old
sudo docker images

## tag the image with the location
sudo docker tag yourname/your-new-docker-image \
                gcr.io/your-project-id/your-new-docker-image

## push to Google Docker registry
sudo gcloud docker push \
     gcr.io/your-project-id/your-new-docker-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You could now pull this image using:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo gcloud docker pull \
  gcr.io/your-project-id/your-new-image-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;but this will be taken care of in the startup behaviour below.&lt;/p&gt;

&lt;p&gt;Remember to commit any changes each time you change the configuration of RStudio.&lt;/p&gt;

&lt;h2 id=&#34;setup-restart-behaviour&#34;&gt;Setup restart behaviour&lt;/h2&gt;

&lt;p&gt;Now we want to configure the above to happen everytime the VM starts up.  I use a startup script for pulling the latest docker image and updating any packages or data from github so to refresh I just need to restart the server.&lt;/p&gt;

&lt;h3 id=&#34;download-latest-custom-packages-and-data&#34;&gt;Download latest custom packages and data&lt;/h3&gt;

&lt;p&gt;In the custom metadata for the VM, we need the field &lt;code&gt;startup-script&lt;/code&gt; and then optional other metadata.&lt;/p&gt;

&lt;p&gt;The metadata is kept seperate away from your running containers, but available via the Google metadata commands.  This can be used for things like passwords and security settings you would prefer not to be shipped in a Docker container, and is easier to manage - just edit the metadata keys.&lt;/p&gt;

&lt;p&gt;Create one metadata field per bash script variable - examples below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;github-user: your-githubname

github-repo: your-github-repo

github-data: your-github-data

github-pat: your-github-pat
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This startup script loads the metadata above and downloads custom R packages and data files from github.&lt;/p&gt;

&lt;p&gt;You can save this locally as &lt;code&gt;startup.sh&lt;/code&gt; and upload via &lt;code&gt;gcloud&lt;/code&gt; or paste it into the interface into a metadata field called &lt;code&gt;startup-script&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;#!/bin/bash
GH_USER=$(curl http://metadata/computeMetadata/v1/instance/attributes/github-user -H &amp;quot;Metadata-Flavor: Google&amp;quot;)
GH_REPO=$(curl http://metadata/computeMetadata/v1/instance/attributes/github-repo -H &amp;quot;Metadata-Flavor: Google&amp;quot;)
GH_DATA=$(curl http://metadata/computeMetadata/v1/instance/attributes/github-data -H &amp;quot;Metadata-Flavor: Google&amp;quot;)
GH_PAT=$(curl http://metadata/computeMetadata/v1/instance/attributes/github-pat -H &amp;quot;Metadata-Flavor: Google&amp;quot;)

## Run Docker image
docker run --name rstudio-server \
       -d -p 8787:8787 \
       -e USER=ADMIN_USERNAME \
       -e PASSWORD=ADMIN_PW \
       -v /mnt/data/:/home/ \
       gcr.io/your-project-name/your-image-name

## Data files: update from github
cd /mnt/data/user_name/project_name/data
git pull &#39;https://&#39;$GH_USER&#39;:&#39;$GH_PAT&#39;@github.com/&#39;$GH_USER&#39;/&#39;$GH_DATA&#39;.git&#39;

## Private packages 
# 1. pull from github
cd /mnt/data/R/privatePackageName
git pull &#39;https://&#39;$GH_USER&#39;:&#39;$GH_PAT&#39;@github.com/&#39;$GH_USER&#39;/&#39;$GH_REPO&#39;.git&#39;

# 2. Install the package from the local file you just updated from Git
sudo docker exec -it rstudio-server \
     sudo su - -c &amp;quot;R -e \&amp;quot;devtools::install(&#39;/home/R/localPackageName/&#39;)\&amp;quot;&amp;quot;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can debug your startup script by connecting to your instance and viewing &lt;code&gt;cat /var/log/startupscript.log&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You could also have a &lt;code&gt;shutdown-script&lt;/code&gt; that would execute before any shutdown.  Check out the &lt;a href=&#34;https://cloud.google.com/compute/docs/metadata&#34;&gt;list of metadata&lt;/a&gt; you can pass into scripts.&lt;/p&gt;

&lt;h2 id=&#34;launch&#34;&gt;Launch&lt;/h2&gt;

&lt;p&gt;Now we relaunch the newly configured VM from your local computer, to test the startup script works.  This will copy over all the configurations from the RStudio server above.&lt;/p&gt;

&lt;p&gt;Save the &lt;code&gt;startup.sh&lt;/code&gt; script to a file on your local machine, cd into the same folder and add the metadata:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud compute instances add-metadata your-r-server-name \
  --metadata-from-file startup-script=startup.sh \
  --metadata github-user=YOUR-USER,github-repo=YOUR_REPO,github-data=YOUR_DATA,github-pat=YOUR_PAT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now reset, cross fingers:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## reset
gcloud compute instances reset your-r-server-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a couple of minutes everything should now be running as configured before.&lt;/p&gt;

&lt;p&gt;If you want to stop or start the server again, use the below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## stop (no billing) but not deleted
gcloud compute instances stop your-r-server-name

## start up a stopped server
gcloud compute instances start your-r-server-name
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;the-future&#34;&gt;The future&lt;/h2&gt;

&lt;p&gt;This should put you in a great position to support R-scripts to the team, but also in a scalable way where starting up faster and bigger machines is just a case of updating configuration files.&lt;/p&gt;

&lt;p&gt;I would like to start up containers using the container manifest syntax but couldn&amp;rsquo;t get it to work for me yet, but for just one VM it means a few less lines in the start up script.&lt;/p&gt;

&lt;p&gt;We also have &lt;a href=&#34;http://iihnordic.dk/blog/posts/2016/marts/creating-a-content-recommendation-engine-using-r-opencpu-and-gtm&#34;&gt;OpenCPU&lt;/a&gt; and &lt;a href=&#34;https://www.rstudio.com/products/shiny/shiny-server/&#34;&gt;Shiny Server&lt;/a&gt; in the Google project, as we move into providing data output such as visualisations, APIs and dashboards.  They are setup in a similar fashion, just swap out the Docker image for the appropriate version you need.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>