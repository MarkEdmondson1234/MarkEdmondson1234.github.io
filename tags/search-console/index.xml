<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Search Console on Mark Edmondson</title>
    <link>/tags/search-console/index.xml</link>
    <description>Recent content in Search Console on Mark Edmondson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-GB</language>
    <copyright>Powered by [Hugo](//gohugo.io). Theme by [PPOffice](http://github.com/ppoffice).</copyright>
    <atom:link href="/tags/search-console/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Comparing Google Search Console queries with Google&#39;s Cloud Natural Language API</title>
      <link>/searchconsoler-vs-googlelanguager</link>
      <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/searchconsoler-vs-googlelanguager</guid>
      <description>&lt;p&gt;With the launch of the Google Natural Language API (NLP API), and the emphasis of machine learning that is said to account for up to 30% of the SEO algorithmn for Google search, a natural question is whether you can use Google’s own macine learning APIs to help optimise your website for search.&lt;/p&gt;
&lt;p&gt;Whilst I don’t believe they will offer exactly the same results, I can see useful applications that include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Identifying what entities are on your website, to see what topics Google Knowledge Graph may categorise your website as&lt;/li&gt;
&lt;li&gt;Running your development website through the API to see if the topics are what you expect your SEO to cover&lt;/li&gt;
&lt;li&gt;Identify content that has very similar topics, that may be competing with one another in search&lt;/li&gt;
&lt;li&gt;Auto-optimisation of content by altering content to desired query targets&lt;/li&gt;
&lt;li&gt;Competitor analysis of SEO website performance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both these data sources are available through R via &lt;code&gt;searchConsoleR&lt;/code&gt; and &lt;code&gt;googleLanguageR&lt;/code&gt;, so below is a workflow on using them together to help answer questions like above.&lt;/p&gt;
&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;For this proof of concept we will use search console API and the NLP API to generate keywords for the same URLs, then compare the results.&lt;/p&gt;
&lt;p&gt;The general outline is:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Get Search Console data for a website&lt;/li&gt;
&lt;li&gt;For each SEO landing page, generate a corpus of NLP results&lt;/li&gt;
&lt;li&gt;Look for evidence that there is a relationship to a high agreement between NLP and SEO rankings&lt;/li&gt;
&lt;li&gt;Identify optimisation opportunities with suggested topics.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;As we’re authenticating with two &lt;code&gt;googleAuthR&lt;/code&gt; libraries, we set the scopes and authenticate directly with &lt;code&gt;googleAuthR::gar_auth()&lt;/code&gt; rather than authenticate seperatly. The NLP API requires you to set up your own Google Cloud project, so that projects client ID, secret etc are used to generate one authentication token that covers both. See the &lt;a href=&#34;http://code.markedmondson.me/googleAuthR/articles/google-authentication-types.html#multiple-authentication-tokens&#34;&gt;googleAuthR website&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;The libraries used are also below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleAuthR)     # Authentication
library(googleLanguageR) # Google NLP API
library(searchConsoleR)  # Webmasters API
library(tidyverse)       # Data processing
library(rvest)           # URL scraping
library(cld2)            # Offline language detection

# set google project to your own
# assumes you have downloaded your own client ID JSON and set in environment argument GAR_CLIENT_JSON
gar_set_client(scopes = c(&amp;quot;https://www.googleapis.com/auth/webmasters&amp;quot;,
                          &amp;quot;https://www.googleapis.com/auth/cloud-platform&amp;quot;))

# creates an auth token for reuse called &amp;quot;scgl.oauth&amp;quot; that works with search console and Language API
gar_auth(&amp;quot;scgl.oauth&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;gather-search-console-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gather Search Console data&lt;/h2&gt;
&lt;p&gt;For this we need the keywords and each SEO landing page that appeared in the Google results, so the dimensions &lt;code&gt;query&lt;/code&gt; and &lt;code&gt;page&lt;/code&gt; are required:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(searchConsoleR)

test_website &amp;lt;- &amp;quot;https://www.example.co.uk&amp;quot;
sc &amp;lt;- search_analytics(test_website, dimensions = c(&amp;quot;query&amp;quot;,&amp;quot;page&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The NLP API supports &lt;a href=&#34;https://cloud.google.com/natural-language/docs/languages&#34;&gt;these 10 languages&lt;/a&gt;, so the queries’ language also need to be on that list.&lt;/p&gt;
&lt;p&gt;For this example, we only use English keywords so we can limit to just those by detecting the query keywords.&lt;/p&gt;
&lt;p&gt;I suggest using the &lt;a href=&#34;https://CRAN.R-project.org/package=cld2&#34;&gt;&lt;code&gt;cld2&lt;/code&gt; library&lt;/a&gt;. I use this offline library for language detection as its free and fine for quick processing, whilst if more heavy duty detection and translation needed, then I would use &lt;code&gt;gl_translate()&lt;/code&gt; from &lt;code&gt;googleLanguageR&lt;/code&gt; although that has a cost.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(cld2)

sc$language &amp;lt;- detect_language(sc$query)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Keeping the languages that are &lt;code&gt;en&lt;/code&gt; or &lt;code&gt;NA&lt;/code&gt; (we couldn’t tell)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sc_eng &amp;lt;- sc %&amp;gt;% filter(is.na(language) | language == &amp;quot;en&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a clean dataset to send to the NLP API.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gather-nlp-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gather NLP data&lt;/h2&gt;
&lt;p&gt;To avoid running NLP on lots of unnecessary HTML boiler plate, we need to consider what data we want to send in.&lt;/p&gt;
&lt;p&gt;For example, as we’re interested in SEO, the relevant data will include the title tags and the body content. Limiting the data sent to those will mean we have cleaner data out.&lt;/p&gt;
&lt;p&gt;There are two approaches to this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Examine the HTML or the website to only extract the useful bits using &lt;code&gt;rvest&lt;/code&gt;’s CSS selectors (needs bespoke programming for each website)&lt;/li&gt;
&lt;li&gt;Take the text only cache from Google as the text source instead (cleaner data, but you’ll be blocked by Google if you scrape to many URLs)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For this example, we take the latter method, but if you are running this beyond proof of concept scale I would advise the first.&lt;/p&gt;
&lt;p&gt;To take the “Text Only” cached version of a page, we use the URL that is supplied by Google Search’s cache:&lt;/p&gt;
&lt;p&gt;e.g.&lt;/p&gt;
&lt;p&gt;If your URL is - &lt;code&gt;http://example.com/your-website&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;…then the text only webcache will be: &lt;code&gt;http://webcache.googleusercontent.com/search?q=cache:example.com/your-website&amp;amp;num=1&amp;amp;strip=1&amp;amp;vwsrc=0#/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;An example function that does the above is shown below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
library(tidyverse)

## create a function for scraping this website
scrape &amp;lt;- function(the_url){
  message(&amp;quot;Scraping &amp;quot;, the_url)
  
  read &amp;lt;- read_html(the_url)
  
  Sys.sleep(5)   # be nice to website
  
  read %&amp;gt;%
    html_text() %&amp;gt;% 
    trimws()
}

scrape_googlecache &amp;lt;- function(the_url){
  
  wc &amp;lt;- gsub(&amp;quot;https?://&amp;quot;,&amp;quot;&amp;quot;, the_url)
  cache_me &amp;lt;- sprintf(
    &amp;quot;http://webcache.googleusercontent.com/search?q=cache:%s&amp;amp;num=1&amp;amp;strip=1&amp;amp;vwsrc=0#/&amp;quot;,
    wc
  )
  
  scraped &amp;lt;- scrape(cache_me)
  
  # remove whitespace and double whitespace
  out &amp;lt;- gsub(&amp;quot;(\r|\n|\t)&amp;quot;,&amp;quot; &amp;quot;, scraped)
  out &amp;lt;- gsub(&amp;quot;\\s\\s+&amp;quot;,&amp;quot; &amp;quot;, out)
  
  # remove first 1000 characters of boilerplate
  out &amp;lt;- substr(out, 1000, nchar(out))

  data.frame(cached = out, url = the_url, stringsAsFactors = FALSE)

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a demo of how it works on one URL:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;the_url &amp;lt;- &amp;quot;http://code.markedmondson.me&amp;quot;

html_url &amp;lt;- scrape_googlecache(the_url)

substr(html_url$cached, 1000, 2000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;oogle&amp;#39;s Cloud Natural Language API 2018-01-11 · 1867 words · 9 minute read r · search-console · google-language With the launch of the Google Natural Language API (NLP API), and the emphasis of machine learning that is said to account for up to 30% of the SEO algorithmn for Google search, a natural question is whether you can use Google’s own macine learning APIs to help optimise your website for search. Whilst I don’t believe they will offer exactly the same results, I can see useful applications that include: Read more Share Comments Run RStudio Server on a Chromebook as a Cloud Native 2017-09-05 · 1534 words · 8 minute read google-cloud-storage · google-compute-engine · r · rstudio-server · docker · google-app-engine I recently got an Asus Chromebook Flip with which I’m very happy, but it did make me realise that if a Chromebook was to replace my normal desktop as my primary workstation, my RStudio Server setup would need to be more cloud native than was available up until now. TL;DR&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can now apply this function to all the unique URLs in your search console URLs using a purrr loop, but hit it too fast and you’ll be blocked.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

all_urls_scrape &amp;lt;- sc_eng %&amp;gt;% 
  distinct(page) %&amp;gt;% 
  select(page) %&amp;gt;% 
  map_df(scrape_googlecache)
  &lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;do-the-nlp-api&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Do the NLP API&lt;/h3&gt;
&lt;p&gt;Now we can get results from Google NLP - test this out first with a couple of URLs as it costs money for each API call.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(googleLanguageR)

nlp_results &amp;lt;- all_urls_scrape %&amp;gt;% 
  select(cached) %&amp;gt;% 
  map(gl_nlp) # this line makes the API calls&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The NLP API returns lots of data, for now we are interested in the entities section:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
entities &amp;lt;- map(nlp_results, &amp;quot;entities&amp;quot;)

## only get the types that are not &amp;quot;OTHER&amp;quot;
types &amp;lt;- map(entities, function(x){
  y &amp;lt;- x[[1]]
  y &amp;lt;- y[y$type != &amp;quot;OTHER&amp;quot;,]
  y &amp;lt;- y[!is.na(y$type),]
  y
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a list of the NLP results for each URL, for all the URLs that were in the search console data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nlp-features&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;NLP features&lt;/h3&gt;
&lt;p&gt;You can see the &lt;a href=&#34;http://code.markedmondson.me/googleLanguageR/articles/nlp.html&#34;&gt;googleLanguageR website&lt;/a&gt; for more details on what is returned, as one example here is what recognised entities the API returned that have Wikipedia links:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

## only get entries that have wikipedia links
types[[1]] %&amp;gt;% 
  filter(!is.na(wikipedia_url)) %&amp;gt;% 
  distinct(name, wikipedia_url) %&amp;gt;% 
  head %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;name&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;wikipedia_url&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Afghanistan&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Afghanistan&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Afghanistan&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;British&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/United_Kingdom&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/United_Kingdom&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Charing Cross&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Charing&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Charing&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;England&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/England&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/England&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Epilepsy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Epilepsy&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Epilepsy&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Euro&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Euro&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Euro&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison-between-search-console-and-nlp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison between Search Console and NLP&lt;/h2&gt;
&lt;p&gt;We can now compare the URL entities in &lt;code&gt;types&lt;/code&gt; and the Search Console data in &lt;code&gt;sc_eng&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;For demo purposes, we only look at the home page for the example website, but you can repeat this by looping over the list of &lt;code&gt;types&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;homepage_types &amp;lt;- types[[1]]
homepage_search_console &amp;lt;- sc_eng %&amp;gt;% filter(page = &amp;quot;https://www.example.co.uk/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;data-processing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data processing&lt;/h3&gt;
&lt;p&gt;We first get the NLP entity names and the search console queries into the same format, lowercase and deduplicated, and for the NLP results the &lt;code&gt;salience&lt;/code&gt; score is how important it thinks that entity is to the page, so we’ll sort downwards from that:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
page_types &amp;lt;- page_types %&amp;gt;% 
  distinct(name, .keep_all = TRUE) %&amp;gt;% 
  mutate(name = tolower(name)) %&amp;gt;% 
  arrange(desc(salience))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the vector of entity names in &lt;code&gt;page_types$name&lt;/code&gt; and the vector of search console queries in &lt;code&gt;sc_eng$query&lt;/code&gt; are the two vectors of keywords we want to compare. Are they similar?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;similarity-of-keywords&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Similarity of keywords&lt;/h3&gt;
&lt;p&gt;In my example case I have 10 distinct queries from search console, and 464 unique named entites extracted out of the HTML from the NLP API, sorted by salience.&lt;/p&gt;
&lt;p&gt;A simple way of matching strings in R is using its base function &lt;code&gt;agrep()&lt;/code&gt; which is an implementation of Levenshtein distance AKA “fuzzy matching”. I’ll run all the queries and compare against the entities:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
## named vector
queries &amp;lt;- page_search_console$query %&amp;gt;% unique()
queries &amp;lt;- setNames(queries, queries) 

fuzzy_ranks &amp;lt;- queries %&amp;gt;% 
  map(~ agrep(., page_types$name, value = FALSE)) %&amp;gt;% 
  enframe(name = &amp;quot;query&amp;quot;, value = &amp;quot;nlp_rank&amp;quot;) %&amp;gt;% 
  mutate(nlp_hits = map_int(nlp_rank, length)) %&amp;gt;% 
  filter(nlp_hits &amp;gt; 0)

fuzzy_values &amp;lt;- queries %&amp;gt;% 
  map(~ agrep(., page_types$name, value = TRUE)) %&amp;gt;% 
  enframe(name = &amp;quot;query&amp;quot;, value = &amp;quot;nlp_value&amp;quot;)

fuzzy &amp;lt;- left_join(fuzzy_ranks, fuzzy_values, by = &amp;quot;query&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since its a one-to-many relationship (one query can match several entities on the page), a list-column created by &lt;code&gt;enframe()&lt;/code&gt; keeps it all neat and tidy within the tidyverse style.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;nlp_rank&lt;/code&gt; is a vector of rankings of the salience of the words&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nlp_hits&lt;/code&gt; is a numeric of how many entities each query had&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nlp_value&lt;/code&gt; is a vector of what entities are matched&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ve had to change a few values to protect client data, but here is an example of the output:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;query&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;nlp_rank&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;nlp_hits&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;nlp_value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;non uk resident handbag cleaning&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;non uk residents handbag cleaning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;non uk residents handbag cleaning&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;non uk residents handbag cleaning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;eu resident handbag cleaning&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;non uk residents handbag cleaning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;eu residents handbag cleaning&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;non uk residents handbag cleaning&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;does-the-nlp-help-with-any-insight&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Does the NLP help with any insight?&lt;/h2&gt;
&lt;p&gt;A few take aways from the above in the real case, which was an e-commerce page targeting customers in Europe was:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The NLP ranked the salience of the top keyword referrer as 8 - meaning it had 7 more entities it thought the page was more relevant to the page. This may mean we should work more on stressing the subject we want to be recognised in Google for.&lt;/li&gt;
&lt;li&gt;One of these was “British” which didn’t appear on the page which only mentioned “UK”, so it looks like it adds synonyms as well into its analysis.&lt;/li&gt;
&lt;li&gt;The NLP ranked “customers” as rank 3, even though it was mentioned only two times on the page. It got that this was an ecommerce page. Likewise it identified “European Econmic area” as important, even though it was only mentioned once. Since this was a sales page aimed at EEC users, this looks fair.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Its early days but I am starting to run through new client websites to see what comes up (by writing more custom code to parse out the content) to pass on to the SEO specialists, at the very least its another perspective to inform topic and keyword choices. If anyone takes this approach and finds it useful, do please comment as I’ll be interested if this helps. It would also be good to compare this with other non-Google NLP APIs such as on Azure and AWS to see if the Google one is especially useful for Google SEO.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>My R Packages</title>
      <link>/r-packages/</link>
      <pubDate>Tue, 24 Jan 2017 21:20:50 +0100</pubDate>
      
      <guid>/r-packages/</guid>
      <description>&lt;p&gt;A full list of R packages I have published are on &lt;a href=&#34;https://github.com/MarkEdmondson1234&#34;&gt;my Github&lt;/a&gt;, but some notable ones are below.&lt;/p&gt;
&lt;p&gt;Some are part of the &lt;a href=&#34;http://cloudyr.github.io/&#34;&gt;cloudyR project&lt;/a&gt;, which has many packages useful for using R in the cloud. &lt;img src=&#34;/images/cloudyr2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I concentrate on the Google cloud below, but be sure to check out the other packages if you’re looking to work with AWS or other cloud based services.&lt;/p&gt;
&lt;div id=&#34;cran&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CRAN&lt;/h2&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;19%&#34; /&gt;
&lt;col width=&#34;25%&#34; /&gt;
&lt;col width=&#34;55%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;URL&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=googleAuthR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/googleAuthR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/googleAuthR/&#34;&gt;googleAuthR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The central workhorse for authentication on Google APIs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=googleAnalyticsR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/googleAnalyticsR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/&#34;&gt;googleAnalyticsR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Works with Google Analytics Reporting V3/V4 and Management APIs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=googleComputeEngineR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/googleComputeEngineR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/&#34;&gt;googleComputeEngineR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Launch Virtual Machines within the Google Cloud, via templates or your own Docker containers.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=googleCloudStorageR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/googleCloudStorageR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/googleCloudStorageR/&#34;&gt;googleCloudStorageR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Interact with Google Cloud Storage via R&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=bigQueryR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/bigQueryR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/bigQueryR/&#34;&gt;bigQueryR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Interact with Google BigQuery via R&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=searchConsoleR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/searchConsoleR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/searchConsoleR/&#34;&gt;searchConsoleR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Download Search Console data into R&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can tell, most are aimed towards helping R users with help in digital analytics and cloud based services. You can get some idea of how they can &lt;a href=&#34;/digital-analytics-workflow-through-google-cloud/&#34;&gt;work together in a digital analytics workflow here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;github&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GITHUB&lt;/h2&gt;
&lt;p&gt;More experimental packages:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;19%&#34; /&gt;
&lt;col width=&#34;25%&#34; /&gt;
&lt;col width=&#34;55%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;URL&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Dev&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/googleMeasureR&#34;&gt;googleMeasureR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Send tracking hits to Google Analytics from R code using the Google Analytics Measurement Protocol&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;https://travis-ci.org/MarkEdmondson1234/googleLanguageR&#34;&gt;&lt;img src=&#34;https://travis-ci.org/MarkEdmondson1234/googleLanguageR.png?branch=master&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/googleLanguageR&#34;&gt;googleLanguageR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Access Speech to text, translation and NLP text processing APIs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;https://travis-ci.org/MarkEdmondson1234/googleID&#34;&gt;&lt;img src=&#34;https://travis-ci.org/MarkEdmondson1234/googleID.svg?branch=master&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/googleID&#34;&gt;googleID&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;In production, but very small so not on CRAN. Allows user authentication via Google+ API for Shiny and RMarkdown documents.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Dev&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/youtubeAnalyticsR&#34;&gt;youtubeAnalyticsR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Access YouTube Analytics data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Deprecated&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/gtmR&#34;&gt;gtmR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Superceded by &lt;a href=&#34;https://github.com/IronistM/googleTagManageR&#34;&gt;googleTagManagerR&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Reference&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/autoGoogleAPI&#34;&gt;autoGoogleAPI&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;152 R packages auto generated via &lt;code&gt;googleAuthR&lt;/code&gt;’s discovery API feature&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Done&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/gentelellaShiny&#34;&gt;gentelellaShiny&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A custom Shiny theme available in a package&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Deprecated&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/stripeR&#34;&gt;stripeR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Interact with the Stripe payment API, but superseded by another R package, &lt;a href=&#34;https://cran.r-project.org/web/packages/RStripe/index.html&#34;&gt;RStripe&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>SEO keyword research using searchConsoleR and googleAnalyticsR</title>
      <link>/search-console-google-analytics-r-keyword-research/</link>
      <pubDate>Tue, 21 Jun 2016 23:03:57 +0100</pubDate>
      
      <guid>/search-console-google-analytics-r-keyword-research/</guid>
      <description>

&lt;p&gt;In this blog we look at a method to estimate where to prioritise your SEO resources, estimating which keywords will give the greatest increase in revenue if you could improve their Google rank.&lt;/p&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&#34;https://data-seo.com/&#34;&gt;Vincent at data-seo.com&lt;/a&gt; who proof read and corrected some errors in the first draft&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Data comes from &lt;a href=&#34;https://www.google.com/webmasters/tools/home&#34;&gt;Google Search Console&lt;/a&gt; and &lt;a href=&#34;https://www.google.com/webmasters/tools/home?hl=en&#34;&gt;Google Analytics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Search Console is used to provide the keywords in these days post &lt;a href=&#34;http://www.notprovidedcount.com/&#34;&gt;(not provided)&lt;/a&gt;.  We then link the two data sets by using the URLs as a key, and estimate how much each keyword made in revenue by splitting them in the same proportion as the traffic they have sent to the page.&lt;/p&gt;

&lt;p&gt;This approach assumes each keyword converts at the same rate once on the page, and will work better with some websites more than others - the best results I have seen are those websites with a lot of content on seperate URLs, such that they capture long-tail queries.  This is because the amount of keywords per URL is small, but with enough volume to make the estimates trustworthy.&lt;/p&gt;

&lt;p&gt;We also try to incorporate magins of error in the results.  This avoids situations where one click on position 40 gets a massive weighting in potential revenue, which in reality could have been a freak event.&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;The method produced a targeted keyword list of 226 from an original seed list of ~21,000.  The top 30 revenue targets are shown in the plot below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/see-potential.png&#34; alt=&#34;seo-forecast-estimate&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Read below on how the plot was generated and what the figures mean.&lt;/p&gt;

&lt;h2 id=&#34;getting-the-data&#34;&gt;Getting the data&lt;/h2&gt;

&lt;p&gt;Google Analytics recently provided more integration between the Search Console imports and the web session data, of which &lt;a href=&#34;http://online-behavior.com/analytics/search-console&#34;&gt;Daniel Waisberg has an excellent walk-through&lt;/a&gt;, which could mean you can skip the first part of this script.&lt;/p&gt;

&lt;p&gt;However, there are circumstances where the integration won&amp;rsquo;t work, such as when the URLs in Google Analytics are a different format to Search Console - with the below script you have control on how to link the URLs, by formatting them to look the same.&lt;/p&gt;

&lt;p&gt;Data is provided via my &lt;a href=&#34;https://cran.r-project.org/web/packages/searchConsoleR/index.html&#34;&gt;searchConsoleR&lt;/a&gt; and &lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/&#34;&gt;googleAnalyticsR&lt;/a&gt; R packages.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(searchConsoleR)
library(googleAnalyticsR)

## authentication with both GA and SC
options(googleAuthR.scopes.selected = 
  c(&amp;quot;https://www.googleapis.com/auth/webmasters&amp;quot;,
    &amp;quot;https://www.googleapis.com/auth/analytics&amp;quot;,
    &amp;quot;https://www.googleapis.com/auth/analytics.readonly&amp;quot;))
    
googleAuthR::gar_auth()

## replace with your GA ID
ga_id &amp;lt;- 1111111 

## date range to fetch
start &amp;lt;- as.character(Sys.Date() - 93)
end &amp;lt;- &amp;quot;2016-06-01&amp;quot;

## Using new GA v4 API
## GAv4 filters
google_seo &amp;lt;- 
  filter_clause_ga4(list(dim_filter(&amp;quot;medium&amp;quot;, 
                                     &amp;quot;EXACT&amp;quot;, 
                                     &amp;quot;organic&amp;quot;),
                          dim_filter(&amp;quot;source&amp;quot;, 
                                     &amp;quot;EXACT&amp;quot;, 
                                     &amp;quot;google&amp;quot;)),
                     operator = &amp;quot;AND&amp;quot;)

## Getting the GA data
gadata &amp;lt;-
  google_analytics_4(ga_id,
                     date_range = c(start,end),
                     metrics = c(&amp;quot;sessions&amp;quot;,
                                 &amp;quot;transactions&amp;quot;,
                                 &amp;quot;transactionRevenue&amp;quot;),
                     dimensions = c(&amp;quot;landingPagePath&amp;quot;),
                     dim_filters = google_seo,
                     order = order_type(&amp;quot;transactions&amp;quot;, 
                                        sort_order = &amp;quot;DESC&amp;quot;, 
                                        orderType = &amp;quot;VALUE&amp;quot;),
                     max = 20000)

## Getting the Search Console data
## The development version &amp;gt;0.2.0.9000 lets you get more than 5000 rows
scdata &amp;lt;- search_analytics(&amp;quot;http://www.example.co.uk&amp;quot;, 
                           startDate = start, endDate = end,
                           dimensions = c(&amp;quot;page&amp;quot;,&amp;quot;query&amp;quot;),
                           rowLimit = 20000)

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;transforming-the-data&#34;&gt;Transforming the data&lt;/h2&gt;

&lt;p&gt;First we change the Search Console URLs into the same format as Google Analytics.  In this example, the hostname is appended to the GA URLs already (a reason why the native support won&amp;rsquo;t work), but you may also need to append the hostname to the GA URLs via &lt;code&gt;paste0(&amp;quot;www.yourdomain.com&amp;quot;, gadata$page)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We also get the search page result (e.g. Page 1 = 1-10, Page 2 = 11-20) as it may be useful.&lt;/p&gt;

&lt;p&gt;For the split of revenue later, the last call calculates the % of clicks going to each URL.&lt;/p&gt;

&lt;h3 id=&#34;search-console-data-transformation&#34;&gt;Search Console data transformation&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)

## get urls in same format

## this will differ from website to website, 
## but in most cases you will need to add the domain to the GA urls:
## gadata$page &amp;lt;- paste0(&amp;quot;www.yourdomain.com&amp;quot;, gadata$landingPagePath)
## gadata has urls www.example.com/pagePath
## scdata has urls in http://www.example.com/pagePath
scdata$page2 &amp;lt;- gsub(&amp;quot;http://&amp;quot;,&amp;quot;&amp;quot;, scdata$page)

## get SERP
scdata$serp &amp;lt;- cut(scdata$position, 
                   breaks = seq(1, 100, 10), 
                   labels = as.character(1:9),
                   include.lowest = TRUE, 
                   ordered_result = TRUE)

## % of SEO traffic to each page per keyword
scdata &amp;lt;- scdata %&amp;gt;% 
  group_by(page2) %&amp;gt;% 
  mutate(clickP = clicks / sum(clicks)) %&amp;gt;%
  ungroup()

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;merging-with-google-analytics&#34;&gt;Merging with Google Analytics&lt;/h3&gt;

&lt;p&gt;Now we merge the data, and calculate the estimates of revenue, transactions and sessions.&lt;/p&gt;

&lt;p&gt;Why estimate sessions when we already have them?  This is how we assess how accurate this approach is - if the clicks is roughly the same as the estimated sessions, we can go further.&lt;/p&gt;

&lt;p&gt;The accuracy metric is assessed as the ratio between the estiamted sessions and the clicks, minus 1.  This will be 0 when 100% accuracy, and then the further away from 0 the figure is the less we trust the results.&lt;/p&gt;

&lt;p&gt;We also round the avergae position to the nearest whole number.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)

## join data on page
joined_data &amp;lt;- gadata %&amp;gt;% 
  left_join(scdata, by = c(landingPagePath = &amp;quot;page2&amp;quot;)) %&amp;gt;%
  mutate(transactionsEst = clickP*transactions,
         revenueEst = clickP*transactionRevenue,
         sessionEst = clickP*sessions,
         accuracyEst = ((sessionEst / clicks) - 1),
         positionRound = round(position))

## we only want clicks over 0, and get rid of a few columns.
tidy_data &amp;lt;- joined_data %&amp;gt;% 
  filter(clicks &amp;gt; 0) %&amp;gt;% 
  select(-page, -sessions, -transactions, -transactionRevenue) 

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;is-it-reliable&#34;&gt;Is it reliable?&lt;/h3&gt;

&lt;p&gt;A histogram of the accuracy estimate shows we consistently over estimate but the clicks and estimated sessions are within a magnitude:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/histogram1.png&#34; alt=&#34;keyword-sessio-click-estimate-histogram&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Most of the session estimates were intrestingly around 1.3 times more than the clicks.  This may be because Search Console clicks act more like Google SEO users, but any other ideas please say in comments!&lt;/p&gt;

&lt;p&gt;From above I discarded all rows with an accuracy &amp;gt; 10 as unreliable, although you may want to be stricter in your criteria.  All figures are to be taken with a pinch of salt with this many assumptions, but if the relative performance looked ok then I feel there is still enough to get some action from the data.&lt;/p&gt;

&lt;h2 id=&#34;creating-the-seo-forecasts&#34;&gt;Creating the SEO forecasts&lt;/h2&gt;

&lt;p&gt;We now use the data to create a click curve table, with estimates on the CTR for each position, and the confidence in those results.&lt;/p&gt;

&lt;p&gt;I first attempted some models on making predictions of click curves for a website, but didn&amp;rsquo;t find any general satisifactory regressions.&lt;/p&gt;

&lt;p&gt;The diagram below uses a weighted &lt;code&gt;loess&lt;/code&gt; within &lt;code&gt;ggplot2&lt;/code&gt; which is good to show trend but not for making predictions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/ctr_scatter.png&#34; alt=&#34;ctr-scatter-plot&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;a-click-curve-to-use&#34;&gt;A click curve to use&lt;/h3&gt;

&lt;p&gt;However, 99% of the time we will only be concerned with the top 10, so it wasn&amp;rsquo;t too taxing to calculate the click through rates per website based on the data they had:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)

click_curve &amp;lt;- tidy_data %&amp;gt;% 
  group_by(positionRound) %&amp;gt;% 
  summarise(CTRmean = sum(clicks) / sum(impressions),
            n = n(),
            click.sum = sum(clicks),
            impressions.sum = sum(impressions),
            sd = sd(ctr),
            E = poisson.test(click.sum)$conf.int[2] / poisson.test(impressions.sum)$conf.int[1],
            lower = CTRmean - E/2,
            upper = CTRmean + E/2) %&amp;gt;% ungroup()

## add % increase to position 1
## could also include other positions
click_curve &amp;lt;- click_curve %&amp;gt;% 
  mutate(CTR1 = CTRmean[1] / CTRmean,
         CTR1.upper = upper[1] / upper,
         CTR1.lower = lower[1] / lower)


&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/images/ctr-curve.png&#34; alt=&#34;ctr-curve-seo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;These CTR rates are then used to predict how much more traffic/revenue etc. a keyword could get if they moved up to position 1.&lt;/p&gt;

&lt;h3 id=&#34;how-valuable-is-a-keyword-if-position-1&#34;&gt;How valuable is a keyword if position 1?&lt;/h3&gt;

&lt;p&gt;Once happy with the click curve, we now apply it to the original data, and work out estimates on SEO revenue for each keyword if they were at position 1.&lt;/p&gt;

&lt;p&gt;I trust results more if they have had more than 10 clicks, and the accuracy estimate figure is within 10 of 0. I would play around with this limits a little yourself to get something you can work with.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)
## combine with data

predict_click &amp;lt;- tidy_data1 %&amp;gt;% 
  mutate(positionRound = round(position)) %&amp;gt;%
  left_join(click_curve, by=c(positionRound = &amp;quot;positionRound&amp;quot;)) %&amp;gt;%
  mutate(revenueEst1 = revenueEst * CTR1,
         transEst1 = transactionsEst * CTR1,
         clickEst1 = clicks * CTR1,
         sessionsEst1 = sessionEst * CTR1,
         revenueEst1.lower = revenueEst * CTR1.lower,
         revenueEst1.upper = revenueEst * CTR1.upper,
         revenueEst1.change = revenueEst1 - revenueEst)

estimates &amp;lt;- predict_click %&amp;gt;% 
  select(landingPagePath, query, clicks, impressions, 
         ctr, position, serp, revenueEst, revenueEst1, 
         revenueEst1.change, revenueEst1.lower, revenueEst1.upper, 
         accuracyEst) %&amp;gt;%
  arrange(desc(revenueEst1)) %&amp;gt;% 
  dplyr::filter(abs(accuracyEst) &amp;lt; 10, 
                revenueEst1.change &amp;gt; 0, 
                clicks &amp;gt; 10)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;estimates&lt;/code&gt; now in this example holds 226 rows sorted in order or how much revenue they are to make if position #1 in Google. This is from an original keyword list of 21437, which is at least a way to narrow down to important keywords.&lt;/p&gt;

&lt;h2 id=&#34;plotting-the-data&#34;&gt;Plotting the data&lt;/h2&gt;

&lt;p&gt;All that remains to to present the data: limiting the keywords to the top 30 lets you present like below.&lt;/p&gt;

&lt;p&gt;The bars show the range of the estimate, as you can see its quite wide but lets you be more realistic in your expectations.&lt;/p&gt;

&lt;p&gt;The number in the middle of the bar is the current position, with the revenue at the x axis and keyword on the y.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/see-potential.png&#34; alt=&#34;seo-forecast-estimate&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;ggplot2-code&#34;&gt;ggplot2 code&lt;/h3&gt;

&lt;p&gt;To create the plots in this post, please see the ggplot2 code below.  Feel free to modify for your own needs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggplot2)

## CTR per position
ctr_plot &amp;lt;- ggplot(tidy_data, aes(x = position, 
                                  y = ctr
                                  ))
ctr_plot &amp;lt;- ctr_plot + theme_minimal()
ctr_plot &amp;lt;- ctr_plot + coord_cartesian(xlim = c(1,30), 
                                       ylim = c(0, 0.5))
ctr_plot &amp;lt;- ctr_plot + geom_point(aes(alpha = log(clicks),
                                      color = serp, 
                                      size = clicks))
ctr_plot &amp;lt;- ctr_plot + geom_smooth(aes(weight = clicks), 
                                   size = 0.2)
ctr_plot + scale_y_continuous(labels = scales::percent)
ctr_plot

hh &amp;lt;- ggplot(click_curve, aes(positionRound, CTRmean)) 
hh &amp;lt;- hh + theme_minimal()
hh &amp;lt;- hh + geom_line(linetype = 2) + coord_cartesian(xlim = c(1, 30), 
                                                     ylim = c(0,0.5))
hh &amp;lt;- hh + geom_ribbon(aes(positionRound, ymin = lower, ymax = upper), 
                       alpha = 0.2, 
                       fill = &amp;quot;orange&amp;quot;)
hh &amp;lt;- hh + scale_y_continuous(labels = scales::percent)
hh &amp;lt;- hh + geom_point() 
hh &amp;lt;- hh + geom_label(aes(label = scales::percent(CTRmean)))
hh


est_plot &amp;lt;- ggplot(estimates[1:30,], 
                   aes(reorder(query, revenueEst1), 
                       revenueEst1, 
                       ymax = revenueEst1.upper, 
                       ymin =  revenueEst1.lower))
est_plot &amp;lt;- est_plot + theme_minimal() + coord_flip()

est_plot &amp;lt;- est_plot + geom_crossbar(aes(fill = cut(accuracyEst, 
                                                    3, 
                                                    labels = c(&amp;quot;Good&amp;quot;,
                                                               &amp;quot;Ok&amp;quot;,
                                                               &amp;quot;Poor&amp;quot;))
                                                               ), 
                                     alpha = 0.7, 
                                     show.legend = FALSE)
                                     
est_plot &amp;lt;- est_plot + scale_x_discrete(name = &amp;quot;Query&amp;quot;)
est_plot &amp;lt;- est_plot + scale_y_continuous(name = &amp;quot;Estimated SEO Revenue Increase for Google #1&amp;quot;, 
                                          labels = scales::dollar_format(prefix = &amp;quot;£&amp;quot;))
est_plot &amp;lt;- est_plot + geom_label(aes(label = round(position)), 
                                  hjust = &amp;quot;center&amp;quot;)
est_plot &amp;lt;- est_plot + ggtitle(&amp;quot;SEO Potential Revenue (Current position)&amp;quot;)
est_plot

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>