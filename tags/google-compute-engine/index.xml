<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Google Compute Engine on Mark Edmondson</title>
    <link>http://code.markedmondson.me/tags/google-compute-engine/index.xml</link>
    <description>Recent content in Google Compute Engine on Mark Edmondson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-GB</language>
    <copyright>Powered by [Hugo](//gohugo.io). Theme by [PPOffice](http://github.com/ppoffice).</copyright>
    <atom:link href="http://code.markedmondson.me/tags/google-compute-engine/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Four Ways to Schedule R scripts on Google Cloud Platform</title>
      <link>http://code.markedmondson.me/4-ways-schedule-r-scripts-on-google-cloud-platform</link>
      <pubDate>Sun, 09 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>http://code.markedmondson.me/4-ways-schedule-r-scripts-on-google-cloud-platform</guid>
      <description>&lt;p&gt;A common question I come across is how to automate scheduling of R scripts downloading data. This post goes through some options that I have played around with, which I’ve mostly used for downloading API data such as Google Analytics using the Google Cloud platform, but the same principles could apply for AWS or Azure.&lt;/p&gt;
&lt;!--more--&gt;
&lt;div id=&#34;scheduling-scripts-advice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scheduling scripts advice&lt;/h2&gt;
&lt;p&gt;But first, some notes on the scripts you are scheduling, that I’ve picked up.&lt;/p&gt;
&lt;div id=&#34;dont-save-data-to-the-scheduling-server&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Don’t save data to the scheduling server&lt;/h3&gt;
&lt;p&gt;I would suggest to not save or use data in the same place you are doing the scheduling. Use a service like BigQuery (&lt;code&gt;bigQueryR&lt;/code&gt;) or googleCloudStorageR (&lt;code&gt;googleCloudStorageR&lt;/code&gt;) to first load any necessary data, do your work then save it out again. This may be a bit more complicated to set up, but will save you tears if the VM or service goes down - you still have your data.&lt;/p&gt;
&lt;p&gt;To help with this, on Google Cloud you can authenticate with the same details you used to launch a VM to authenticate with the storage services above (as all are covered under the &lt;code&gt;http://www.googleapis.com/auth/cloud-services&lt;/code&gt; scope) - you can access this auth when on a GCE VM in R via &lt;code&gt;googleAuthR::gar_gce_auth()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;An example skeleton script is shown below that may be something you are scheduling.&lt;/p&gt;
&lt;p&gt;It downloads authentication files, does an API call, then saves it up to the cloud again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleAuthR)
library(googleCloudStorageR)

gcs_global_bucket(&amp;quot;my-bucket&amp;quot;)
## auth on the VM
options(googleAuthR.scopes.selected = &amp;quot;https://www.googleapis.com/auth/cloud-platform&amp;quot;)
gar_gce_auth()

## use the GCS auth to download the auth files for your API
auth_file &amp;lt;- &amp;quot;auth/my_auth_file.json&amp;quot;
gcs_get_object(auth_file, saveToDisk = TRUE)

## now auth with the file you just download
gar_auth_service(auth_file)

## do your work with APIs etc.
.....

## upload results back up to GCS (or BigQuery, etc.)
gcs_upload(my_results, name = &amp;quot;results/my_results.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;set-up-a-schedule-for-logs-too&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Set up a schedule for logs too&lt;/h3&gt;
&lt;p&gt;Logs are important for scheduled jobs, so you have some idea on whats happened when things go wrong. To help with scheduling debugging, most &lt;code&gt;googleAuthR&lt;/code&gt; packages now have a timestamp on their output messages.&lt;/p&gt;
&lt;p&gt;You can send the output of your scripts to log files, if using cron and RScript it looks something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;RScript /your-r-script.R &amp;gt; your-r-script.log&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;…where &lt;code&gt;&amp;gt;&lt;/code&gt; sends the output to the new file.&lt;/p&gt;
&lt;p&gt;Over time though, this can get big and (sigh) fill up your disk so you can’t log in to the VM (speaking from experience here!) so I now set up another scheduled job that every week takes the logs and uploads to GCS, then deletes the current ones.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;consider-using-docker-for-environments&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Consider using Docker for environments&lt;/h3&gt;
&lt;p&gt;Several of the methods below use &lt;a href=&#34;https://www.docker.com/&#34;&gt;&lt;code&gt;Docker&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The reasons for that is Docker provides a nice reprodueable way to define exactly what packages and dependencies you need for your script to run, which can run on top of any type of infrastructure as &lt;code&gt;Docker&lt;/code&gt; has quickly become a cloud standard.&lt;/p&gt;
&lt;p&gt;For instance, migrating from Google Cloud to AWS is much easier if both can be deployed using Docker, and below Docker is instrumental in allowing you to run on multiple solutions.&lt;/p&gt;
&lt;p&gt;Bear in mind that when a Docker container relaunches it won’t save any data, so any non-saved state will be lost (you should make a new container if you need it to contain data), but you’re not saving your data to the docker container anyway, aren’t you?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;scheduling-options---pros-and-cons&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scheduling options - Pros and cons&lt;/h2&gt;
&lt;p&gt;Here is an overview of the pros and cons of the options presented in more detail below:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;29%&#34; /&gt;
&lt;col width=&#34;35%&#34; /&gt;
&lt;col width=&#34;35%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Pros&lt;/th&gt;
&lt;th&gt;Cons&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1. &lt;code&gt;cronR&lt;/code&gt; Addin on &lt;code&gt;RStudio Server&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Simple and quick&lt;/td&gt;
&lt;td&gt;Not so robust, need to log into server to make changes, versioning packages.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2. &lt;code&gt;gce_vm_scheduler&lt;/code&gt; and &lt;code&gt;Dockerfiles&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Robust and can launch from local R session, support versioning&lt;/td&gt;
&lt;td&gt;Need to build Dockerfiles, all scripts on one VM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;3. Master &amp;amp; Slave VM&lt;/td&gt;
&lt;td&gt;Tailor a fresh VM for each script, cheaper&lt;/td&gt;
&lt;td&gt;Need to build Dockerfiles, more complicated VM setup.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;4. Google AppEngine with flexible containers&lt;/td&gt;
&lt;td&gt;Managed platform&lt;/td&gt;
&lt;td&gt;Need to turn script into web responses, more complicated setup&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;cronr-plus-rstudio-server&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1 - cronR plus RStudio Server&lt;/h2&gt;
&lt;p&gt;This is the simplest and the one to start with.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Start up an RStudio Server instance&lt;/li&gt;
&lt;li&gt;Install &lt;a href=&#34;https://github.com/bnosac/cronR&#34;&gt;&lt;code&gt;cronR&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Upload your R script&lt;/li&gt;
&lt;li&gt;Schedule your script using &lt;code&gt;cronR&lt;/code&gt; RStudio addin&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With &lt;code&gt;googleComputeEngineR&lt;/code&gt; and the new &lt;code&gt;gcer-public&lt;/code&gt; project containing public images that include one with &lt;code&gt;cronR&lt;/code&gt; already installed, this is as simple as the few lines of code below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleComputeEngineR)

## get the tag for prebuilt Docker image with googleAuthRverse, cronR and tidyverse
tag &amp;lt;- gce_tag_container(&amp;quot;google-auth-r-cron-tidy&amp;quot;, project = &amp;quot;gcer-public&amp;quot;)
# gcr.io/gcer-public/google-auth-r-cron-tidy

## start a custom Rstudio instance
vm &amp;lt;- gce_vm(name = &amp;quot;my-rstudio&amp;quot;,
              predefined_type = &amp;quot;n1-highmem-8&amp;quot;,
              template = &amp;quot;rstudio&amp;quot;,
              dynamic_image = tag,
              username = &amp;quot;me&amp;quot;, password = &amp;quot;mypassword&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait for it to launch and give you an IP, then log in, upload a script and configure the schedule via the &lt;code&gt;cronR&lt;/code&gt; addin.&lt;/p&gt;
&lt;p&gt;Some more detail about this workflow can be found at these &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/rstudio-team.html&#34;&gt;custom RStudio example workflows&lt;/a&gt; on the &lt;code&gt;googleComputeEngineR&lt;/code&gt; website.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gce_vm_scheduler-and-dockerfiles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2- &lt;em&gt;gce_vm_scheduler&lt;/em&gt; and &lt;em&gt;Dockerfiles&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;This method I prefer to the above since it lets you create the exact environment (e.g. package versions, dependencies) to run your script in, that you can trail dev and production versions with. It also works locally without needing to log into the server each time to deploy a script.&lt;/p&gt;
&lt;div id=&#34;handy-tools-for-docker---containerit-and-build-triggers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Handy tools for Docker - containerit and Build Triggers&lt;/h3&gt;
&lt;p&gt;Here we introduce Docker images, which may have been more a technical barrier for some before (but worth knowing, I think)&lt;/p&gt;
&lt;div id=&#34;containerit&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;containerit&lt;/h4&gt;
&lt;p&gt;Things are much easier now though, as we have the magic new R package &lt;a href=&#34;http://o2r.info/2017/05/30/containerit-package/&#34;&gt;&lt;code&gt;containerit&lt;/code&gt;&lt;/a&gt; which can generate these Docker files for you - just send &lt;code&gt;containerit::dockerfile()&lt;/code&gt; around the script file.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build-triggers&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Build Triggers&lt;/h4&gt;
&lt;p&gt;Along with auto-generating Dockerfiles, for Google Cloud in particular we now also have &lt;a href=&#34;https://cloud.google.com/container-builder/docs/how-to/build-triggers&#34;&gt;Build Triggers&lt;/a&gt; which automates building the Docker image for you.&lt;/p&gt;
&lt;p&gt;Just make the Dockerfile, then set up a trigger for when you push that file up to GitHub - you can see the ones used to create the public R resources here in the &lt;a href=&#34;https://github.com/cloudyr/googleComputeEngineR/tree/master/inst/dockerfiles&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt; repo&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;recipe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Recipe&lt;/h3&gt;
&lt;p&gt;Putting it all together then, documentation of this workflow for &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/scheduled-rscripts.html&#34;&gt;scheduling R scripts is found here&lt;/a&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If you don’t already have one, start up a scheduler VM using &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/gce_vm_scheduler.html&#34;&gt;&lt;code&gt;gce_vm_scheduler&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Create a Dockerfile either manually or using &lt;code&gt;containerit&lt;/code&gt; that will run your script upon execution&lt;/li&gt;
&lt;li&gt;Upload the Dockerfile to a git repo (private or public)&lt;/li&gt;
&lt;li&gt;Setup a build trigger for that Dockerfile&lt;/li&gt;
&lt;li&gt;Once built, set a script to schedule within that Dockerfile with &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/gce_schedule_docker.html&#34;&gt;&lt;code&gt;gce_schedule_docker&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is still in beta at time of writing but should be stable by the time &lt;code&gt;googlecomputeEngineR&lt;/code&gt; hits CRAN &lt;code&gt;0.2.0&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;master-and-slave-vms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3 - Master and Slave VMs&lt;/h2&gt;
&lt;p&gt;Some scripts take more resources than others, and since you are using VMs already you can have more control over what specifications of VM to launch based on the script you want to run.&lt;/p&gt;
&lt;p&gt;This means you can have a cheap scheduler server, that launch biggers VMs for the duration of the job. As GCP charges per minute, this can save you money over having a schedule server that is as big as what your most expensive script needs running 24/7.&lt;/p&gt;
&lt;p&gt;This method is largely like the scheduled scripts above, except in this case the scheduled script is also launching VMs to run the job upon.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;googleCloudStorageR::gcs_source&lt;/code&gt; you can run an R script straight from where it is hosted upon GCS, meaning all data, authentication files and scripts can be kept seperate from the computation. An example master script is shown below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## intended to be run on a small instance via cron
## use this script to launch other VMs with more expensive tasks
library(googleComputeEngineR)
library(googleCloudStorageR)
gce_global_project(&amp;quot;my-project&amp;quot;)
gce_global_zone(&amp;quot;europe-west1-b&amp;quot;)
gcs_global_bucket(&amp;quot;your-gcs-bucket&amp;quot;)

## auth to same project we&amp;#39;re on
googleAuthR::gar_gce_auth()

## launch the premade VM
vm &amp;lt;- gce_vm(&amp;quot;slave-1&amp;quot;)

## set SSH to use &amp;#39;master&amp;#39; username as configured before
vm &amp;lt;- gce_ssh_setup(vm, username = &amp;quot;master&amp;quot;, ssh_overwrite = TRUE)

## run the script on the VM that will source from GCS
runme &amp;lt;- &amp;quot;Rscript -e \&amp;quot;googleAuthR::gar_gce_auth();googleCloudStorageR::gcs_source(&amp;#39;download.R&amp;#39;, bucket = &amp;#39;your-gcs-bucket&amp;#39;)\&amp;quot;&amp;quot;
out &amp;lt;- docker_cmd(vm, 
                  cmd = &amp;quot;exec&amp;quot;, 
                  args = c(&amp;quot;rstudio&amp;quot;, runme), 
                  wait = TRUE)

## once finished, stop the VM
gce_vm_stop(vm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More detail is again available at the &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/scheduled-rscripts.html#master-slave-scheduler&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;google-app-engine-with-flexible-custom-runtimes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4 - Google App Engine with flexible custom runtimes&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/appengine/docs/&#34;&gt;Google App Engine&lt;/a&gt; has always had schedule options, but only for its supported languages of Python, Java, PHP etc. Now with the &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/&#34;&gt;introduction of flexible containers&lt;/a&gt;, any Docker container running any language (including R) can also be run.&lt;/p&gt;
&lt;p&gt;This is potentially the best solution since it runs upon a 100% managed platform, meaning you don’t need to worry about servers at all, and it takes care of things like server maintence, logging etc.&lt;/p&gt;
&lt;div id=&#34;setting-up-your-script-for-app-engine&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting up your script for App Engine&lt;/h3&gt;
&lt;p&gt;There are some &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/build&#34;&gt;requirements for the container&lt;/a&gt; that need configuring so it can run:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can not use &lt;code&gt;googleAuthR::gar_gce_auth()&lt;/code&gt; so will need to upload the auth token within the Dockerfile.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;AppEngine expects a web service to be listening on port 8080, so your schedule script needs to be triggered via HTTP requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For authentication, I use the system environment arguments (i.e. those usually set in &lt;code&gt;.Renviron&lt;/code&gt;) that &lt;code&gt;googleAuthR&lt;/code&gt; packages use for auto-authentication. Put the auth file (such as JSON or a &lt;code&gt;.httr-oauth&lt;/code&gt; file) into the deployment folder, then point to its location via specifying in the &lt;code&gt;app.yaml&lt;/code&gt;. Details below.&lt;/p&gt;
&lt;p&gt;To solve the need for being a webservice on port 8080 (which is then proxied to normal webports 80/443), &lt;a href=&#34;https://www.rplumber.io/&#34;&gt;&lt;code&gt;plumber&lt;/code&gt;&lt;/a&gt; is a great service by Jeff Allen of RStudio, which already comes with its own Docker solution. You can then modify that &lt;code&gt;Dockerfile&lt;/code&gt; slightly so that it works on App Engine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;recipe-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Recipe&lt;/h3&gt;
&lt;p&gt;To then schedule your R script on app engine, follow the guide below, first making sure you have setup the &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/&#34;&gt;gcloud CLI&lt;/a&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create a Google Appengine project in the US region (only region that supports flexible containers at the moment)&lt;/li&gt;
&lt;li&gt;Create a scheduled script e.g. &lt;code&gt;schedule.R&lt;/code&gt; - you can use auth from environment files specified in &lt;code&gt;app.yaml&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Make an API out of the script by using &lt;code&gt;plumber&lt;/code&gt; - example:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleAuthR)         ## authentication
library(googleCloudStorageR)  ## google cloud storage
library(readr)                ## 
## gcs auto authenticated via environment file 
## pointed to via sys.env GCS_AUTH_FILE

#* @get /demoR
demoScheduleAPI &amp;lt;- function(){
  
  ## download or do something
  something &amp;lt;- tryCatch({
      gcs_get_object(&amp;quot;schedule/test.csv&amp;quot;, 
                     bucket = &amp;quot;mark-edmondson-public-files&amp;quot;)
    }, error = function(ex) {
      NULL
    })
      
  something_else &amp;lt;- data.frame(X1 = 1,
                               time = Sys.time(), 
                               blah = paste(sample(letters, 10, replace = TRUE), collapse = &amp;quot;&amp;quot;))
  something &amp;lt;- rbind(something, something_else)
  
  tmp &amp;lt;- tempfile(fileext = &amp;quot;.csv&amp;quot;)
  on.exit(unlink(tmp))
  write.csv(something, file = tmp, row.names = FALSE)
  ## upload something
  gcs_upload(tmp, 
             bucket = &amp;quot;mark-edmondson-public-files&amp;quot;, 
             name = &amp;quot;schedule/test.csv&amp;quot;)
  
  message(&amp;quot;Done&amp;quot;, Sys.time())
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create Dockerfile. If using &lt;code&gt;containerit&lt;/code&gt; then replace FROM with &lt;code&gt;trestletech/plumber&lt;/code&gt; and add the below lines to use correct AppEngine port:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(containerit)

dockerfile &amp;lt;- dockerfile(&amp;quot;schedule.R&amp;quot;, copy = &amp;quot;script_dir&amp;quot;, soft = TRUE)
write(dockerfile, file = &amp;quot;Dockerfile&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then change/add these lines to the created Dockerfile:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;EXPOSE 8080
ENTRYPOINT [&amp;quot;R&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;pr &amp;lt;- plumber::plumb(commandArgs()[4]); pr$run(host=&amp;#39;0.0.0.0&amp;#39;, port=8080)&amp;quot;]
CMD [&amp;quot;schedule.R&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Example final Dockerfile below. This doesn’t need to be built in say a build trigger as its built upon app engine deployment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FROM trestletech/plumber
LABEL maintainer=&amp;quot;mark&amp;quot;
RUN export DEBIAN_FRONTEND=noninteractive; apt-get -y update \
 &amp;amp;&amp;amp; apt-get install -y libcairo2-dev \
    libcurl4-openssl-dev \
    libgmp-dev \
    libpng-dev \
    libssl-dev \
    libxml2-dev \
    make \
    pandoc \
    pandoc-citeproc \
    zlib1g-dev
RUN [&amp;quot;install2.r&amp;quot;, &amp;quot;-r &amp;#39;https://cloud.r-project.org&amp;#39;&amp;quot;, &amp;quot;readr&amp;quot;, &amp;quot;googleCloudStorageR&amp;quot;, &amp;quot;Rcpp&amp;quot;, &amp;quot;digest&amp;quot;, &amp;quot;crayon&amp;quot;, &amp;quot;withr&amp;quot;, &amp;quot;mime&amp;quot;, &amp;quot;R6&amp;quot;, &amp;quot;jsonlite&amp;quot;, &amp;quot;xtable&amp;quot;, &amp;quot;magrittr&amp;quot;, &amp;quot;httr&amp;quot;, &amp;quot;curl&amp;quot;, &amp;quot;testthat&amp;quot;, &amp;quot;devtools&amp;quot;, &amp;quot;hms&amp;quot;, &amp;quot;shiny&amp;quot;, &amp;quot;httpuv&amp;quot;, &amp;quot;memoise&amp;quot;, &amp;quot;htmltools&amp;quot;, &amp;quot;openssl&amp;quot;, &amp;quot;tibble&amp;quot;, &amp;quot;remotes&amp;quot;]
RUN [&amp;quot;installGithub.r&amp;quot;, &amp;quot;MarkEdmondson1234/googleAuthR@7917351&amp;quot;, &amp;quot;hadley/rlang@ff87439&amp;quot;]
WORKDIR /payload/
COPY [&amp;quot;.&amp;quot;, &amp;quot;./&amp;quot;]

EXPOSE 8080
ENTRYPOINT [&amp;quot;R&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;pr &amp;lt;- plumber::plumb(commandArgs()[4]); pr$run(host=&amp;#39;0.0.0.0&amp;#39;, port=8080)&amp;quot;]
CMD [&amp;quot;schedule.R&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Specify &lt;code&gt;app.yaml&lt;/code&gt; for flexible containers as &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/&#34;&gt;detailed here&lt;/a&gt;. Add any environment vars such as auth files, that will be included in same deployment folder.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;runtime: custom
env: flex

env_variables:
  GCS_AUTH_FILE: auth.json&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Specify &lt;code&gt;cron.yaml&lt;/code&gt; for the schedule needed:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;cron:
- description: &amp;quot;test cron&amp;quot;
  url: /demoR
  schedule: every 1 hours&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;You should now have these files in the deployment folder:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;app.yaml&lt;/code&gt; - configuration of general app settings&lt;/li&gt;
&lt;li&gt;&lt;code&gt;auth.json&lt;/code&gt; - an authentication file specified in env arguments or app.yaml&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cron.yaml&lt;/code&gt; - specification of when your scheduling is&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Dockerfile&lt;/code&gt; - specification of the environment&lt;/li&gt;
&lt;li&gt;&lt;code&gt;schedule.R&lt;/code&gt; - the plumber version of your script containing your endpoints&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Open the terminal in that folder, and deploy via &lt;code&gt;gcloud app deploy --project your-project&lt;/code&gt; and the cron schedule via &lt;code&gt;gcloud app deploy cron.yaml --project your-project&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It will take a while (up to 10 mins) the first time.&lt;/p&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The App Engine should then be deployed on &lt;a href=&#34;https://your-project.appspot.com/&#34; class=&#34;uri&#34;&gt;https://your-project.appspot.com/&lt;/a&gt; - every &lt;code&gt;GET&lt;/code&gt; request to &lt;a href=&#34;https://your-project.appspot.com/demoR&#34; class=&#34;uri&#34;&gt;https://your-project.appspot.com/demoR&lt;/a&gt; (or other endpoints you have specified in R script) will run the R code. The cron example above will run every hour to this endpoint.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Logs for the instance are found &lt;a href=&#34;https://console.cloud.google.com/logs/viewer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This approach is the most flexible, and offers a fully managed platform for your scripts. Scheduled scripts are only the beginning, since deploying such actually gives you a way to run R scripts in response to any HTTP request from any language - triggers could also include if someone updates a spreadsheet, adds a file to a folder, pushes to GitHub etc. which opens up a lot of exciting possibilities. You can also scale it up to become a fully functioning R API.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Hopefully this has given you an idea on your options for R on Google Cloud regarding scheduling. If you have some other easier workflows or suggestions for improvements please put them in the comments below!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>My R Packages</title>
      <link>http://code.markedmondson.me/r-packages/</link>
      <pubDate>Tue, 24 Jan 2017 21:20:50 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/r-packages/</guid>
      <description>&lt;p&gt;A full list of R packages I have published are on &lt;a href=&#34;https://github.com/MarkEdmondson1234&#34;&gt;my Github&lt;/a&gt;, but some notable ones are below.&lt;/p&gt;
&lt;p&gt;Some are part of the &lt;a href=&#34;http://cloudyr.github.io/&#34;&gt;cloudyR project&lt;/a&gt;, which has many packages useful for using R in the cloud. &lt;img src=&#34;http://code.markedmondson.me/images/cloudyr2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I concentrate on the Google cloud below, but be sure to check out the other packages if you’re looking to work with AWS or other cloud based services.&lt;/p&gt;
&lt;div id=&#34;cran&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CRAN&lt;/h2&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;19%&#34; /&gt;
&lt;col width=&#34;25%&#34; /&gt;
&lt;col width=&#34;55%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;URL&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=googleAuthR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/googleAuthR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/googleAuthR/&#34;&gt;googleAuthR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The central workhorse for authentication on Google APIs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=googleAnalyticsR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/googleAnalyticsR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/&#34;&gt;googleAnalyticsR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Works with Google Analytics Reporting V3/V4 and Management APIs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=googleComputeEngineR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/googleComputeEngineR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/&#34;&gt;googleComputeEngineR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Launch Virtual Machines within the Google Cloud, via templates or your own Docker containers.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=googleCloudStorageR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/googleCloudStorageR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/googleCloudStorageR/&#34;&gt;googleCloudStorageR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Interact with Google Cloud Storage via R&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=bigQueryR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/bigQueryR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/bigQueryR/&#34;&gt;bigQueryR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Interact with Google BigQuery via R&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=searchConsoleR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/searchConsoleR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/searchConsoleR/&#34;&gt;searchConsoleR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Download Search Console data into R&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can tell, most are aimed towards helping R users with help in digital analytics and cloud based services. You can get some idea of how they can &lt;a href=&#34;http://code.markedmondson.me/digital-analytics-workflow-through-google-cloud/&#34;&gt;work together in a digital analytics workflow here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;github&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GITHUB&lt;/h2&gt;
&lt;p&gt;More experimental packages:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;19%&#34; /&gt;
&lt;col width=&#34;25%&#34; /&gt;
&lt;col width=&#34;55%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;URL&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;https://travis-ci.org/MarkEdmondson1234/googleLanguageR&#34;&gt;&lt;img src=&#34;https://travis-ci.org/MarkEdmondson1234/googleLanguageR.png?branch=master&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/googleLanguageR&#34;&gt;googleLanguageR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Access Speech to text, translation and NLP text processing APIs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;https://travis-ci.org/MarkEdmondson1234/googleID&#34;&gt;&lt;img src=&#34;https://travis-ci.org/MarkEdmondson1234/googleID.svg?branch=master&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/googleID&#34;&gt;googleID&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;In production, but very small so not on CRAN. Allows user authentication via Google+ API for Shiny and RMarkdown documents.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;In development&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/youtubeAnalyticsR&#34;&gt;youtubeAnalyticsR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Access YouTube Analytics data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;In development&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/gtmR&#34;&gt;gtmR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Access the Google Tag Manager API from R&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Reference&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/autoGoogleAPI&#34;&gt;autoGoogleAPI&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;152 R packages auto generated via &lt;code&gt;googleAuthR&lt;/code&gt;’s discovery API feature&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Done&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/gentelellaShiny&#34;&gt;gentelellaShiny&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A custom Shiny theme available in a package&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Deprecated&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/stripeR&#34;&gt;stripeR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Interact with the Stripe payment API, but superseded by another R package, &lt;a href=&#34;https://cran.r-project.org/web/packages/RStripe/index.html&#34;&gt;RStripe&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Launch RStudio Server in the Google Cloud with two lines of R</title>
      <link>http://code.markedmondson.me/launch-rstudio-server-google-cloud-in-two-lines-r/</link>
      <pubDate>Thu, 20 Oct 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/launch-rstudio-server-google-cloud-in-two-lines-r/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;ve written previously about how to get &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download-server/&#34;&gt;RStudio Server&lt;/a&gt; running on Google Compute Engine: the &lt;a href=&#34;http://markedmondson.me/run-r-rstudio-and-opencpu-on-google-compute-engine-free-vm-image&#34;&gt;first in July 2014&lt;/a&gt; gave you a snapshot to download then customise, the second in &lt;a href=&#34;http://code.markedmondson.me/setting-up-scheduled-R-scripts-for-an-analytics-team/&#34;&gt;April 2016&lt;/a&gt; launched via a Docker container.&lt;/p&gt;

&lt;p&gt;Things move on, and I now recommend using the process below that uses the RStudio template in the new on CRAN &lt;a href=&#34;https://cran.r-project.org/web/packages/googleComputeEngineR/&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt;&lt;/a&gt; package.  Not only does it abstract away a lot of the dev-ops set up, but it also gives you more flexibility by taking advantage of &lt;code&gt;Dockerfiles&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;launching-an-rstudio-server&#34;&gt;Launching an Rstudio Server&lt;/h2&gt;

&lt;p&gt;This example is taken from the &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/example-workflows.html#custom-team-rstudio-server&#34;&gt;example workflows&lt;/a&gt; that are on the &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt; website&lt;/a&gt;, which includes other examples for Shiny, OpenCPU and R-clusters.&lt;/p&gt;

&lt;p&gt;You do need to do a bit of &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/installation-and-authentication.html&#34;&gt;initial setup&lt;/a&gt; to setup your Google project and download the authentication file, but after that you just need to issue these two commands:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleComputeEngineR)
vm &amp;lt;- gce_vm(template = &amp;quot;rstudio&amp;quot;,
             name = &amp;quot;my-rstudio&amp;quot;,
             username = &amp;quot;mark&amp;quot;, password = &amp;quot;mark1234&amp;quot;,
             predefined_type = &amp;quot;n1-highmem-2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And thats it.  Wait a bit, it will output an IP address for you to log in with.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://code.markedmondson.me/images/rstudio-launch-example.png&#34; alt=&#34;rstudio-googleComputeEngineR-launch&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://code.markedmondson.me/images/rstudio-login.png&#34; alt=&#34;rstudio login&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can now carry on by logging in and installing packages as you would on RStudio Desktop, then use &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/gce_vm_stop.html&#34;&gt;&lt;code&gt;gce_vm_stop(vm)&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/gce_vm_start.html&#34;&gt;&lt;code&gt;gce_vm_start(vm)&lt;/code&gt;&lt;/a&gt; to stop and start your instance, or if say you are on a Chromebook and cannot run R locally, use the Google Cloud Web UI to start and stop it.&lt;/p&gt;

&lt;h2 id=&#34;further-customisation&#34;&gt;Further customisation&lt;/h2&gt;

&lt;p&gt;You customise further by creating a custom image that launches a fresh RStudio Server instance with your own packages and files installed.  This takes advantage of some Google Cloud benefits such as the &lt;a href=&#34;https://cloud.google.com/container-registry/&#34;&gt;Container Registry&lt;/a&gt; which lets you save private Docker containers.&lt;/p&gt;

&lt;p&gt;With that, you can save your custom RStudio server to its own custom image, that can be used to launch anew in another instance as needed:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## push your rstudio image to container registry
gce_push_registry(vm, &amp;quot;my-rstudio&amp;quot;, container_name = &amp;quot;my-rstudio&amp;quot;)

## launch another rstudio instance with your settings
vm2 &amp;lt;- gce_vm(template = &amp;quot;rstudio&amp;quot;,
              name = &amp;quot;my-rstudio-2&amp;quot;,
              username = &amp;quot;mark&amp;quot;, password = &amp;quot;mark1234&amp;quot;,
              predefined_type = &amp;quot;n1-highmem-2&amp;quot;,
              dynamic_image = gce_tag_container(&amp;quot;my-rstudio&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you want to go further still, use &lt;a href=&#34;https://docs.docker.com/engine/reference/builder/&#34;&gt;&lt;code&gt;Dockerfiles&lt;/code&gt;&lt;/a&gt; to customise the underlying linux libraries and CRAN/github packages to install in a more replicable manner - a good way to keep track in Github exactly how your server is configured.&lt;/p&gt;

&lt;p&gt;A &lt;code&gt;Dockerfile&lt;/code&gt; example is shown below - construct this locally:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM rocker/hadleyverse
MAINTAINER Mark Edmondson (r@sunholo.com)

# install cron and R package dependencies
RUN apt-get update &amp;amp;&amp;amp; apt-get install -y \
    cron \
    nano \
    ## clean up
    &amp;amp;&amp;amp; apt-get clean \ 
    &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/ \ 
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds
    
## Install packages from CRAN
RUN install2.r --error \ 
    -r &#39;http://cran.rstudio.com&#39; \
    googleAuthR shinyFiles googleCloudStorage bigQueryR gmailR googleAnalyticsR \
    ## install Github packages
    &amp;amp;&amp;amp; Rscript -e &amp;quot;devtools::install_github(c(&#39;bnosac/cronR&#39;))&amp;quot; \
    ## clean up
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds \
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then build it on your VM via &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/docker_build.html&#34;&gt;&lt;code&gt;docker_build&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;docker_build(vm, 
             dockerfile = &amp;quot;file/location/dockerfile&amp;quot;, 
             new_image = &amp;quot;my-custom-image&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can then save this up to the Container Registry and launch as before:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gce_push_registry(vm, &amp;quot;my-custom-image&amp;quot;, image_name = &amp;quot;my-custom-image&amp;quot;
vm3 &amp;lt;- gce_vm(template = &amp;quot;rstudio&amp;quot;,
              name = &amp;quot;my-rstudio-3&amp;quot;,
              username = &amp;quot;mark&amp;quot;, password = &amp;quot;mark1234&amp;quot;,
              predefined_type = &amp;quot;n1-highmem-2&amp;quot;,
              dynamic_image = gce_tag_container(&amp;quot;my-custom-image&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A digital analytics workflow through the Google Cloud using R</title>
      <link>http://code.markedmondson.me/digital-analytics-workflow-through-google-cloud/</link>
      <pubDate>Mon, 10 Oct 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/digital-analytics-workflow-through-google-cloud/</guid>
      <description>

&lt;p&gt;There are now several packages built upon the &lt;code&gt;googleAuthR&lt;/code&gt; framework which are helpful to a digital analyst who uses R, so this post looks to demonstrate how they all work together.  If you&amp;rsquo;re new to R, and would like to know how it helps with your digital analytics, &lt;a href=&#34;http://analyticsdemystified.com/blog/tim-wilson/&#34;&gt;Tim Wilson&lt;/a&gt; and I ran a workshop last month aimed at getting a digital analyst up and running.  The course material is online at &lt;a href=&#34;http://www.dartistics.com/&#34;&gt;www.dartistics.com&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;diagram&#34;&gt;Diagram&lt;/h2&gt;

&lt;p&gt;A top level overview is shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/r-infrastructure.png&#34; alt=&#34;R-infrastructure-digital-analytics&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The diagram shows &lt;a href=&#34;http://code.markedmondson.me/googleAuthR/&#34;&gt;&lt;code&gt;googleAuthR&lt;/code&gt;&lt;/a&gt; packages, other packages, servers and APIs all of which interact to turn data into actionable analytics.&lt;/p&gt;

&lt;p&gt;The most recent addition is &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt;&lt;/a&gt; which has helped make great savings in the time it takes to set up servers.  I have in the past blogged about &lt;a href=&#34;http://code.markedmondson.me/setting-up-scheduled-R-scripts-for-an-analytics-team/&#34;&gt;setting up R servers in the Google Cloud&lt;/a&gt;, but it was still more dev-ops than I really wanted to be doing.  Now, I can do setups similar to those I have written about in a couple of lines of R.&lt;/p&gt;

&lt;h2 id=&#34;data-workflow&#34;&gt;Data workflow&lt;/h2&gt;

&lt;p&gt;A suggested workflow for working with data is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Infrastructure&lt;/em&gt; - Creating a computer to run your analysis.  This can be either your own laptop, or a server in the cloud.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Collection&lt;/em&gt; - Downloading your raw data from one or multiple sources, such as APIs or your CRM database.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Transformation&lt;/em&gt; - Turning your raw data into useful information via ETL, modelling or statistics.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Storage&lt;/em&gt; - Storing the information you have created so that it can be automatically updated and used.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Output&lt;/em&gt; - Displaying or using the information into an end user application.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The components of the diagram can be combined into the workflow above.  You can swap out various bits for your own needs, but its possible to use R for all of these steps.&lt;/p&gt;

&lt;p&gt;You could do all of this with an Excel workbook on your laptop.  However, as data analysis becomes more complicated, it makes sense to start breaking out the components into more specialised tools, since Excel will start to strain when you increase the data volume or want reproducibility.&lt;/p&gt;

&lt;h3 id=&#34;infrastructure&#34;&gt;Infrastructure&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt;&lt;/a&gt; uses the Google Clouds&amp;rsquo; virtual machine instances to create servers, with specific support for R.&lt;/p&gt;

&lt;p&gt;It uses &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker containers&lt;/a&gt; to make launching the servers and applications you need quick and simple, without you needing to know too much dev-ops to get started.&lt;/p&gt;

&lt;p&gt;Due to Docker, the applications created can be more easily transferred into other IT environments, such as within internal client intranets or AWS.&lt;/p&gt;

&lt;p&gt;To help with a digital analytics workflow, &lt;code&gt;googleComputeEngineR&lt;/code&gt; includes templates for the below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RStudio Server&lt;/strong&gt; - an R IDE you can work with from your browser. The server edition means you can access it from anywhere, and can always ensure the correct packages are installed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shiny&lt;/strong&gt; - A server to run your Shiny apps that display your data in dashboards or applications end users can work with.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenCPU&lt;/strong&gt; - A server that turns your R code into a JSON API.  Used for turning R output into a format web development teams can use straight within a website.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For instance, launching an RStudio server is as simple as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleComputeEngineR)
vm &amp;lt;- gce_vm_template(&amp;quot;rstudio&amp;quot;, 
                      name = &amp;quot;rstudio-server&amp;quot;, 
                      predefined_type = &amp;quot;f1-micro&amp;quot;, 
                      username = &amp;quot;mark&amp;quot;, 
                      password = &amp;quot;mark1234&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The instance will launch within a few minutes and give you a URL you can then login with.&lt;/p&gt;

&lt;h3 id=&#34;collection&#34;&gt;Collection&lt;/h3&gt;

&lt;p&gt;Once you are logged in to your RStudio Server, you can use all of R&amp;rsquo;s power to download and work with your data.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;googleAuthR&lt;/code&gt; packages can all be authenticated under the same OAuth2 token, to simplify access.&lt;/p&gt;

&lt;p&gt;Other packages useful to digital analytics include APIs such as &lt;a href=&#34;https://github.com/randyzwitch/RSiteCatalyst&#34;&gt;&lt;code&gt;RSiteCatalyst&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;twitteR&lt;/code&gt;.  A full list of digital analytics R packages can be found in the &lt;a href=&#34;https://cran.r-project.org/web/views/WebTechnologies.html&#34;&gt;web technologies section of CRAN Task Views&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another option is the R package &lt;a href=&#34;https://github.com/jennybc/googlesheets&#34;&gt;&lt;code&gt;googlesheets&lt;/code&gt;&lt;/a&gt; by Jenny Bryan, which could either be used as a data source or as a data storage for reports, to be processed onwards later.&lt;/p&gt;

&lt;p&gt;The below example R script fetches data from Google Analytics, SEO data from Google Search Console and CRM data from BigQuery.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleAnalyticsR)
library(searchConsoleR)
library(bigQueryR)
library(googleAuthR)

## authenticate with all services 
gar_auth_service(&amp;quot;auth.json&amp;quot;)

## get search console data
seo_data &amp;lt;- search_analytics(&amp;quot;http://example.com&amp;quot;, 
                             &amp;quot;2015-07-01&amp;quot;, &amp;quot;2015-07-31&amp;quot;, 
                              c(&amp;quot;date&amp;quot;, &amp;quot;query&amp;quot;, &amp;quot;page&amp;quot;))

## get google analytics data
ga_data &amp;lt;- google_analytics_4(1235678,
                              c(&amp;quot;2015-07-01&amp;quot;, &amp;quot;2015-07-31&amp;quot;),
                              metrics = c(&amp;quot;users&amp;quot;),
                              dimensions = c(&amp;quot;date&amp;quot;, &amp;quot;userID&amp;quot; , &amp;quot;landingPagePath&amp;quot;, &amp;quot;campaign&amp;quot;))

## get CRM data from BigQuery
crm_data &amp;lt;- bqr_query(&amp;quot;big-query-database&amp;quot;, &amp;quot;my_data&amp;quot;,
                      &amp;quot;SELECT userID, lifetimeValue FROM [my_dataset]&amp;quot;)
                              
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;transformation&#34;&gt;Transformation&lt;/h3&gt;

&lt;p&gt;This ground is well covered by existing R packages.  My suggestion here is to embrace the &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyverse/index.html&#34;&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/a&gt; and use that to create and generate your information.&lt;/p&gt;

&lt;p&gt;Applications include anomaly detection, measurement of causal effect, clustering and forecasting. Hadley Wickham&amp;rsquo;s book &lt;a href=&#34;http://r4ds.had.co.nz/index.html&#34;&gt;&amp;ldquo;R for Data Science&amp;rdquo;&lt;/a&gt; is a recent compendium of knowledge on this topic, which includes this suggested work flow:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/tidy-r.png&#34; alt=&#34;tidyr&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;storage&#34;&gt;Storage&lt;/h3&gt;

&lt;p&gt;Once you have your data in the format you want, you often need to keep it somewhere it is easily accessible for other systems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/storage/&#34;&gt;Google Cloud Storage&lt;/a&gt; is a cheap, reliable method of storing any type of data object so that its always available for further use, and is heavily used within Google Cloud applications as a central data store.  I use it for storing &lt;code&gt;RData&lt;/code&gt; files or storing &lt;code&gt;csv&lt;/code&gt; files with a public link that is emailed to users when available.  It is accessible in R via the &lt;a href=&#34;http://code.markedmondson.me/googleCloudStorageR/&#34;&gt;&lt;code&gt;googleCloudStorageR&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;

&lt;p&gt;For database style access, &lt;a href=&#34;https://cloud.google.com/bigquery/&#34;&gt;BigQuery&lt;/a&gt; can be queried from many data services, including data visualisation platforms such as Google&amp;rsquo;s Data Studio or Tableau.  BigQuery offers incredibly fast analytical queries for TBs of data, accessible via the &lt;a href=&#34;http://code.markedmondson.me/bigQueryR/&#34;&gt;&lt;code&gt;bigQueryR&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;

&lt;p&gt;An example of uploading data is below - again only one authentication is needed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleCloudStorageR)
library(bigQueryR)

## authenticate with all services 
gar_auth_service(&amp;quot;auth.json&amp;quot;)

## upload to Big Query
bqr_upload_data(&amp;quot;projectID&amp;quot;, &amp;quot;datasetID&amp;quot;, &amp;quot;tableID&amp;quot;,
                my_data_for_sql_queries)

## upload to Google Cloud Storage
gcs_upload(&amp;quot;my_rdata_file&amp;quot;)


&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;output&#34;&gt;Output&lt;/h3&gt;

&lt;p&gt;Finally, outputs include &lt;a href=&#34;http://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt; apps, &lt;a href=&#34;http://rmarkdown.rstudio.com/&#34;&gt;RMarkdown&lt;/a&gt;, a &lt;a href=&#34;https://gist.github.com/MarkEdmondson1234/ddcac436cbdfd4557639522573bfc7b6&#34;&gt;scheduled email&lt;/a&gt; or an R API call using &lt;a href=&#34;https://www.opencpu.org/&#34;&gt;OpenCPU&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All &lt;code&gt;googleAuthR&lt;/code&gt; functions are Shiny and RMarkdown compatible for user authentication - this means a user can login themselves and access their own data whilst using the logic of your app to gain insight, without you needing access to their data at all.  An example of an RMarkdown app taking advantage of this is the demo of the &lt;a href=&#34;https://github.com/MarkEdmondson1234/gentelellaShiny&#34;&gt;GentelellaShiny GA dashboard&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/gentellaShinyGA.png&#34; alt=&#34;gentella-shiny&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can launch OpenCPU and Shiny servers just as easily as RStudio Server via &lt;code&gt;googleComputeEngineR&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleComputeEngineR)

## creates a Shiny server
vm2 &amp;lt;- gce_vm_template(&amp;quot;shiny&amp;quot;, 
                      name = &amp;quot;shiny-server&amp;quot;, 
                      predefined_type = &amp;quot;f1-micro&amp;quot;, 
                      username = &amp;quot;mark&amp;quot;, 
                      password = &amp;quot;mark1234&amp;quot;)

## creates an OpenCPU server                      
vm3 &amp;lt;- gce_vm_template(&amp;quot;opencpu&amp;quot;, 
                      name = &amp;quot;opencpu-server&amp;quot;, 
                      predefined_type = &amp;quot;f1-micro&amp;quot;, 
                      username = &amp;quot;mark&amp;quot;, 
                      password = &amp;quot;mark1234&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Shiny Apps or RMarkdown HTML files can then be uploaded for display to the end user.  If the server needs more power, simply save the container and relaunch with a bigger RAM or CPU.&lt;/p&gt;

&lt;p&gt;OpenCPU is the technology demonstrated in my recent EARL London talk on using R to &lt;a href=&#34;http://code.markedmondson.me/predictClickOpenCPU/supercharge.html&#34;&gt;forecast HTML prefetching and deploying through Google Tag Manager&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Your Shiny, RMarkdown or OpenCPU functions can download data via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleCloudStorageR)
library(bigQueryR)

## authenticate with all services 
gar_auth_service(&amp;quot;auth.json&amp;quot;)

## download Google Cloud Storage
my_data &amp;lt;- gce_get_object(&amp;quot;my_rdata_file&amp;quot;)

## query data from Big Query dependent on user input
query &amp;lt;- bqr_query(&amp;quot;big-query-database&amp;quot;, &amp;quot;my_data&amp;quot;,
                   &amp;quot;SELECT userID, lifetimeValue FROM [my_dataset]&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Hopefully this has shown where some efficiencies could be made in your own digital analysis. For me, the reduction of computer servers to atoms of work has expanded the horizons on what is possible: applications such as sending big calculations to the cloud if taking too long locally; being able to send clients entire clusters of computers with a data application ready and working; and having customised R environments for every occasion, such as R workshops.&lt;/p&gt;

&lt;p&gt;For the future, I hope to introduce Spark clusters via &lt;a href=&#34;https://cloud.google.com/dataproc/&#34;&gt;Google Dataproc&lt;/a&gt;, giving the ability to use machine learning directly on a dataset without needing to download locally; scheduled scripts that launch servers as needed; and working with Google&amp;rsquo;s newly launched &lt;a href=&#34;https://cloud.google.com/ml/&#34;&gt;machine learning APIs&lt;/a&gt; that dovetail into the Google Cloud.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scheduling R scripts for a team using RStudio Server, Docker, Github and Google Compute Engine</title>
      <link>http://code.markedmondson.me/setting-up-scheduled-R-scripts-for-an-analytics-team/</link>
      <pubDate>Thu, 21 Apr 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/setting-up-scheduled-R-scripts-for-an-analytics-team/</guid>
      <description>

&lt;p&gt;&lt;em&gt;edit 20th November, 2016 - now everything in this post is abstracted away and available in the googleComputeEngineR package - I would say its a lot easier to use that.  Here is a post on getting started with it. &lt;a href=&#34;http://code.markedmondson.me/launch-rstudio-server-google-cloud-in-two-lines-r/&#34;&gt;http://code.markedmondson.me/launch-rstudio-server-google-cloud-in-two-lines-r/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This blog will give you steps that allows you to run on Google Compute Engine a server that has these features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RStudio Server instance with multiple login.&lt;/li&gt;
&lt;li&gt;Apache to host a welcome webpage.&lt;/li&gt;
&lt;li&gt;Restart the server to securely load private Github R packages / Docker images.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;A shared cron folder that runs every day at 0630 that user&amp;rsquo;s can put their own scripts into.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is inspired by conversations with &lt;a href=&#34;https://twitter.com/chipoglesby&#34;&gt;&lt;code&gt;@Chipoglesby&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/RichardFergie&#34;&gt;&lt;code&gt;@RichardFergie&lt;/code&gt;&lt;/a&gt; on twitter about this kind of setup, and was useful to me as a way to organise my thoughts on the subject ;).  If you have any suggestions on improvements please tweet to me at &lt;a href=&#34;https://twitter.com/HoloMarkeD&#34;&gt;&lt;code&gt;@HoloMarkeD&lt;/code&gt;&lt;/a&gt; or email via the blog.&lt;/p&gt;

&lt;h2 id=&#34;motivation-in-search-of-a-4-day-work-week&#34;&gt;Motivation: In search of a 4-day work week&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ve just started a new job at &lt;a href=&#34;http://iihnordic.dk/&#34;&gt;IIH Nordic&lt;/a&gt; where a part of my duties is to look for ways to automate the boring tasks that come up in a digital web analytics team.&lt;/p&gt;

&lt;p&gt;IIH Nordic have an aim to introduce a 4-day work week within a couple of years, so the motivation is to find out how to save 20% of the current work schedule without harming productivity.  Downloading data and creating reports is a big time-sink that looks ripe for optimisation.&lt;/p&gt;

&lt;h3 id=&#34;introducing-the-wonders-of-r-to-a-team&#34;&gt;Introducing the wonders of R to a team&lt;/h3&gt;

&lt;p&gt;The current strategy is to introduce the team to R and train everyone up in running simple scripts that download the data they need for their reports.  We will tackle data transformation/ETL, statistics and visualisation/presentation later, but the first goal is to solve the &amp;ldquo;How can I get data&amp;rdquo; problem.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m well placed to help due to writing &lt;a href=&#34;https://github.com/MarkEdmondson1234/googleAuthR&#34;&gt;&lt;code&gt;googleAuthR&lt;/code&gt;&lt;/a&gt;, so have played with R libraries with simple authentication and functions to download from &lt;a href=&#34;https://github.com/MarkEdmondson1234/googleAnalyticsR_public&#34;&gt;&lt;code&gt;Google Analytics&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/MarkEdmondson1234/searchConsoleR&#34;&gt;&lt;code&gt;Search Console&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/MarkEdmondson1234/bigQueryR&#34;&gt;&lt;code&gt;BigQuery&lt;/code&gt;&lt;/a&gt;, as well as using Randy Zwitch&amp;rsquo;s &lt;a href=&#34;https://github.com/randyzwitch/RSiteCatalyst&#34;&gt;&lt;code&gt;Adobe Analytics&lt;/code&gt;&lt;/a&gt; package.  These libraries alone cover 90% of the data sources we need.&lt;/p&gt;

&lt;h3 id=&#34;why-use-rstudio-server-in-the-cloud&#34;&gt;Why use RStudio Server in the cloud?&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;ve had a few internal workshops now and the team look comfortable writing the R-scripts, but during the process I noticed a few issues that are solved using RStudio Server.  Using it means:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Everyone is working on the same versions and libraries.&lt;/li&gt;
&lt;li&gt;Scripts can be logged for quality and errors.&lt;/li&gt;
&lt;li&gt;Scheduled scripts can be run from the server not someones local PC.&lt;/li&gt;
&lt;li&gt;Training material can be made available on same server.&lt;/li&gt;
&lt;li&gt;Web accessible private login for remote working.&lt;/li&gt;
&lt;li&gt;Can use R from an iPad or Google Chromebook!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;setting-up-rstudio-server-on-google-cloud-compute-gce&#34;&gt;Setting up RStudio Server on Google Cloud Compute (GCE)&lt;/h2&gt;

&lt;p&gt;I have a old blog post on the &lt;a href=&#34;http://markedmondson.me/run-r-rstudio-and-opencpu-on-google-compute-engine-free-vm-image&#34;&gt;installation of RStudio Server on GCE&lt;/a&gt;, but this will update that since technology and my understanding has improved.&lt;/p&gt;

&lt;p&gt;A brief overview of the components are below:&lt;/p&gt;

&lt;h3 id=&#34;docker&#34;&gt;Docker&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt; is a virtual machine-lite that can run on anything, which means that the set-up is very transferable to other operating systems like Windows or OSX.&lt;/p&gt;

&lt;p&gt;It offers easier set-up via pre-created Docker images, and is embraced and &lt;a href=&#34;https://cloud.google.com/compute/docs/containers/container_vms&#34;&gt;well supported by Google&lt;/a&gt; via its &lt;a href=&#34;https://cloud.google.com/container-registry/docs/&#34;&gt;container registry&lt;/a&gt;, giving you unlimited private repositories, unlike Docker Hub that gives you only one.  It provides reproducibility and scale.&lt;/p&gt;

&lt;h3 id=&#34;github&#34;&gt;Github&lt;/h3&gt;

&lt;p&gt;Github is the go-to place to review and share code, and offers web-hooks that means it can push updates when you update a repository.  Every coder should use a version control system so its good practice to introduce it to the team, and it also allows installation of experimental and private R packages via &lt;code&gt;devtools::install_github&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;Google also has support for using Github via its &lt;a href=&#34;https://cloud.google.com/source-repositories/docs/setting-up-repositories&#34;&gt;Cloud Source Repositories&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;google-compute-engine&#34;&gt;Google Compute Engine&lt;/h3&gt;

&lt;p&gt;The up-and-coming pretender to AWS&amp;rsquo;s cloud computing crown, the Google Cloud is a natural choice for me as it offers integrations for Google Analytics and BigQuery.  The server images it offers have a great API and user interface, and it offers very quick I/O and restart times on Google&amp;rsquo;s world-class infrastructure, and the Docker container support as mentioned makes things more simple to scale in the future.&lt;/p&gt;

&lt;h2 id=&#34;setup-steps&#34;&gt;Setup steps&lt;/h2&gt;

&lt;p&gt;Now we get to the actual commands you use to get things up and running.  It will give you:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RStudio Server instance with multiple login.&lt;/li&gt;
&lt;li&gt;Apache to host a welcome webpage.&lt;/li&gt;
&lt;li&gt;Restart the server to load your private Github R packages if you have any.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;A shared cron folder that runs every day at 0430 that user&amp;rsquo;s can put their own scripts into.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In general I use the command line from a local terminal, but all actions can also be carried out within the &lt;a href=&#34;https://console.cloud.google.com/compute/instances&#34;&gt;Compute Engine web interface&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;create-the-google-cloud-engine&#34;&gt;Create the Google Cloud Engine&lt;/h3&gt;

&lt;p&gt;Here is a reference for &lt;a href=&#34;https://cloud.google.com/compute/docs/containers/vm-image/&#34;&gt;GCE docker enabled VMs&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download and install the &lt;a href=&#34;https://cloud.google.com/sdk/&#34;&gt;Google Cloud SDK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Authorize and set-up billing for a Google Project&lt;/li&gt;
&lt;li&gt;Start up your terminal and set your project and zone:&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;  gcloud auth login
  
  gcloud config set project your-project-name
  
  gcloud config set compute/region europe-west1
  
  gcloud config set compute/zone europe-west1-b
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Issue this &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/reference/compute/instances/create&#34;&gt;gcloud create command&lt;/a&gt; to start up a docker enabled instance:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;edit 18th September 2016 to update to new images as per &lt;a href=&#34;https://cloud.google.com/compute/docs/containers/vm-image/&#34;&gt;https://cloud.google.com/compute/docs/containers/vm-image/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud compute instances create your-r-server-name \
  --image-family gci-stable \
  --image-project google-containers \
  --tags http-server
  --scopes https://www.googleapis.com/auth/devstorage.read_write \
  --machine-type n1-standard-1

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unlike previously, this new (from Aug 2016) container ready VM comes with Google Cloud Storage scopes already set, so you can use private docker repos.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;--tags&lt;/code&gt; flag sets the default http firewall rules to apply to this instance so we can reach it via the internet on port 80.&lt;/p&gt;

&lt;p&gt;You should now be able to see your instance has launched in the &lt;a href=&#34;https://console.cloud.google.com/compute/instances&#34;&gt;Google Compute Dashboard&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;create-persistent-disks&#34;&gt;Create persistent disks&lt;/h3&gt;

&lt;p&gt;Anything new in the Docker container will disappear on a restart if you also don&amp;rsquo;t commit and push the image, so to avoid that data and scripts are linked to the container via the &lt;code&gt;--volumes&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;We also link to persistent disk rather than the VM&amp;rsquo;s own, which has the advantage of being able to connect (read-only) to multiple servers at the same time, should you need it.  This also means if you need a more powerful server, you can safely create one knowing you will have the same data and scripts available.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud compute disks create --size=200GB my-data-disk
gcloud compute instances attach-disk your-r-server-name --disk my-data-disk
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;initial-configuration-on-google-compute-engine&#34;&gt;Initial Configuration on Google Compute Engine&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Login to GCE&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;  gcloud compute ssh your-r-server-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are now making commands from within the GCE VM.&lt;/p&gt;

&lt;p&gt;If you have any problems, use the web interface to login via the Cloud Shell.  Sometimes you had to add your username to the instance, and then login with that user like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;  gcloud compute ssh user@your-r-server-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See &lt;a href=&#34;https://cloud.google.com/compute/docs/troubleshooting#ssherrors&#34;&gt;here&lt;/a&gt; for more diagnostics help.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Format and mount the persistent disk&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;See instructions &lt;a href=&#34;https://cloud.google.com/compute/docs/disks/persistent-disks&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;```sh
sudo mkfs.ext4 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/disk/by-id/google-persistent-disk-1

##make mount point
sudo mkdir /mnt/data/
sudo mount -o discard,defaults /dev/disk/by-id/google-persistent-disk-1 /mnt/data/

## user folders
sudo mkdir /mnt/data/home/

## custom packages
sudo mkdir /mnt/data/R/

##permissions
sudo chmod a+w /mnt/data

## make it mount automatically each time
echo &#39;/dev/disk/by-id/google-persistent-disk-1 /mnt/data ext4 discard,defaults 1 1&#39; | sudo tee -a /etc/fstab
```
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Install and Configure Apache&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This gives you a user-friendly webpage pointing to the RStudio login, and I also use it as a place to put training material such as RMarkdown documents.&lt;/p&gt;

&lt;p&gt;I suppose this could also be done via another Docker container if you have a more complicated Apache setup to use.&lt;/p&gt;

&lt;p&gt;The ProxyPassMatch line is needed for the Shiny engine thats within RStudio Server to work.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo apt-get update
sudo apt-get -y install apache2

## Need proxy and proxy_http to create nice URLs, proxy_wstunnel for Shiny
sudo a2enmod proxy proxy_http proxy_wstunnel

## this may be 000-default.conf rather than 000-default 
## depending on version of apache
echo &#39;&amp;lt;VirtualHost *:80&amp;gt;
        ServerAdmin your@email.com

        DocumentRoot /var/www

        ProxyPassMatch ^/rstudio/p/([0-9]+)/(websocket|.*/websocket)/$ ws://localhost:8787/p/$1/$2/
        ProxyPass /rstudio/ http://localhost:8787/
        ProxyPassReverse /rstudio/ http://localhost:8787/
        RedirectMatch permanent ^/rstudio$ /rstudio/

        ErrorLog ${APACHE_LOG_DIR}/error.log

        LogLevel warn

        CustomLog ${APACHE_LOG_DIR}/access.log combined

&amp;lt;/VirtualHost&amp;gt;&#39; | sudo tee /etc/apache2/sites-enabled/000-default

sudo service apache2 restart

## A startup HTML page for you to customise
echo &#39;&amp;lt;!doctype html&amp;gt;&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;RStudio on Google Cloud Compute&amp;lt;/h1&amp;gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;./rstudio/&amp;quot;&amp;gt;RStudio Server&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;&#39; | sudo tee /var/www/index.html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should now be able to see your server running Apache.&lt;/p&gt;

&lt;p&gt;I then upload a website to &lt;code&gt;/var/www/&lt;/code&gt; via the &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/reference/compute/copy-files&#34;&gt;&lt;code&gt;gcloud copy-files&lt;/code&gt; command&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;An example, assuming your website is in your local folder &lt;code&gt;~/dev/website/www/&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo gcloud --project your-project-name compute copy-files ~/dev/website/www/ your-r-server-name:/var/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is what we have at IIH Nordic:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/rstudio-server-webpage.png&#34; alt=&#34;batching_example&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;a-download-folder&#34;&gt;A download folder&lt;/h4&gt;

&lt;p&gt;Now, you may want to have a dropbox style folder for the data your scripts are running, say scheduled data downloads.  There are a few ways to skin this cat, such as uploading to cloud storage in your script, but the simplest way for me was to use the Apache functionality to create a logged in download area.&lt;/p&gt;

&lt;p&gt;For this, you need to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create a folder on your data disk where scripts will dump their data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir /mnt/data/downloads
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a symlink to an Apache web server folder&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ln -s /mnt/data/downloads /var/www/downloads
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[optional] Style the folder with CSS so it looks nice using say &lt;a href=&#34;http://adamwhitcroft.com/apaxy/&#34;&gt;Apaxy&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Update your Apache config to have logged in access to the folder.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The extra Apache config is below.  It requires installation of &lt;code&gt;sudo a2enmod headers&lt;/code&gt; and a restart.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;### In download folder, auto-download .txt,.csv, and .pdf files
### Protect with a password
&amp;lt;Directory /var/www/downloads/&amp;gt;
      &amp;lt;FilesMatch &amp;quot;.+\.(txt|csv|pdf)$&amp;quot;&amp;gt;
          ForceType application/octet-stream
          Header set Content-Disposition attachment
      &amp;lt;/FilesMatch&amp;gt;
      AuthType Basic
      AuthName &amp;quot;Enter password&amp;quot;
      AuthUserFile /etc/apache2/.htpasswd
      Require valid-user
      Order allow,deny
      Allow from all
      Options Indexes FollowSymLinks
      AllowOverride All
&amp;lt;/Directory&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;download-and-run-the-rstudio-server-docker-image&#34;&gt;Download and Run the RStudio Server Docker image&lt;/h3&gt;

&lt;p&gt;We first download a pre-prepared RStudio Docker container created by the &lt;a href=&#34;http://dirk.eddelbuettel.com/blog/2014/10/23/&#34;&gt;Rocker&lt;/a&gt; team led by Dirk Eddelbuettel. This is what our custom image will be based upon.  Here we download a variant that also loads &lt;a href=&#34;https://hub.docker.com/r/rocker/hadleyverse/&#34;&gt;RStudio and all of Hadley&amp;rsquo;s packages&lt;/a&gt; to give us a great base to work from.&lt;/p&gt;

&lt;p&gt;Most of the below is gleaned from the &lt;a href=&#34;https://github.com/rocker-org/rocker/wiki&#34;&gt;Rocker Wiki&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The below docker command runs the docker image - if that image is not available it will look for the image on Docker hub and download. This will take a long time as it downloads everything, perhaps time to put the kettle on.&lt;/p&gt;

&lt;p&gt;Subsequent times will load quickly from local version.&lt;/p&gt;

&lt;p&gt;We run it with a custom username and password we want, as this will be exposed to the web and we don&amp;rsquo;t want the defaults to be exposed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## Run the docker image with RStudio and Hadley Universe
sudo docker run --name rstudio-server -d -p 8787:8787 \
     -e USER=YOUR_USERNAME -e PASSWORD=YOUR_PW \
     -v /mnt/data/:/home/ \
     rocker/hadleyverse
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;configure-rstudio-server-docker-image&#34;&gt;Configure RStudio Server Docker image&lt;/h3&gt;

&lt;p&gt;Now it could be that you are done from here - you should have a working RStudio interface available on the IP of your container (&lt;a href=&#34;http://your-vm-ip-address/rstudio/auth-sign-in&#34;&gt;http://your-vm-ip-address/rstudio/auth-sign-in&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;But we will configure it a bit more, adding users, more packages, and scheduled jobs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you have a lot of configurations then it is better to create your own &lt;a href=&#34;https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/&#34;&gt;DOCKERFILE&lt;/a&gt; and build the image yourself.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We log in to the running docker container here:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo docker exec -it rstudio-server bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a good one to remember for when you are doing future customisation.&lt;/p&gt;

&lt;p&gt;You are now in the Docker container.&lt;/p&gt;

&lt;p&gt;Install stuff then CTRL-D to come out again to commit and push your changes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## Make users that will create a directory on the data disk
adduser mark

## [Optional] Install packages
## install as sudo to ensure all user&#39;s have access
sudo su - -c &amp;quot;R -e \&amp;quot;install.packages(&#39;abc&#39;, repos=&#39;http://cran.rstudio.com/&#39;)\&amp;quot;&amp;quot;

## [Optional] Install libraries from Github packages
sudo su - -c &amp;quot;R -e \&amp;quot;devtools::install_github(&#39;MarkEdmondson1234/bigQueryR&#39;)\&amp;quot;&amp;quot;

## [Optional] Install libraries from private Github packages
sudo su - -c &amp;quot;R -e \&amp;quot;devtools::install_github(&#39;MarkEdmondson1234/privatePackge&#39;, auth_token=YOUR_GITHUB_PAT)\&amp;quot;&amp;quot;
````

### Configure scheduling via CRON

A big reason to have a server is for the team to schedule their data fetching scripts.  We achieve this by running CRON within the Docker container (to ensure all packages are installed) and then providing a link to a folder that runs the script when they need it.

First we install CRON in the container:

```sh
## Download and install cron
sudo apt-get update
sudo apt-get -y install cron nano
sudo service cron start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then schedule scripts via RScript to run daily.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## annoying nano bug in Docker
export TERM=xterm

## open up the cron editor (select 2)
sudo crontab -e

## add this to the bottom of file
## runs script at 0420 every day
# m h  dom mon dow   command
20 4 * * * /home/cron/r-cron.R &amp;gt;/home/cron/cronlog.log 2&amp;gt;&amp;amp;1

## CTRL-X and Y to save changes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;r-cron.R&lt;/code&gt; script needs to have &lt;code&gt;#!/usr/bin/Rscript&lt;/code&gt; at the top to run correctly.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#!/usr/bin/Rscript
cat(&amp;quot;Cron script started: &amp;quot;, date())

....do R stuff

cat(&amp;quot;Cron script stopped: &amp;quot;, date())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Repeat for all scripts you want to run.&lt;/p&gt;

&lt;h3 id=&#34;sending-emails&#34;&gt;Sending emails&lt;/h3&gt;

&lt;p&gt;Its useful to send an email once a script has run successfully (or not), one that uses Mailrun is below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Email a user a report is ready
#&#39;
#&#39; Requires an account at Mailgun: https://mailgun.com
#&#39; Pre-verification can only send to a whitelist of emails you configure
#&#39;
#&#39; @param email Email to send to
#&#39; @param mail_message Any extra info
#&#39;
#&#39; @return TRUE if successful email sent
#&#39; @import httr
#&#39; @export
sendEmail &amp;lt;- function(email = &amp;quot;XXXXX@you.com&amp;quot;,
                      mail_message = &amp;quot;Hello&amp;quot;){

  url &amp;lt;- &amp;quot;https://api.mailgun.net/v3/sandbox5f2XXXXXXXa.mailgun.org/messages&amp;quot;
  ## username:password so api_key is all after the api:
  api_key &amp;lt;- &amp;quot;key-c5957XXXXXXXXXXXbb9cf8ce&amp;quot;
  the_body &amp;lt;-
    list(
      from=&amp;quot;Mailgun Sandbox &amp;lt;postmaster@sandbox5XXXXXXXXa.mailgun.org&amp;gt;&amp;quot;,
      to=email,
      subject=&amp;quot;Mailgun from R&amp;quot;,
      text=mailmessage,
    )

  req &amp;lt;- httr::POST(url,
                    httr::authenticate(&amp;quot;api&amp;quot;, api_key),
                    encode = &amp;quot;form&amp;quot;,
                    body = the_body)

  httr::stop_for_status(req)
  
  TRUE

}

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;scheduling-packages&#34;&gt;Scheduling packages&lt;/h3&gt;

&lt;p&gt;There is also an R package that manages cron, &lt;a href=&#34;https://github.com/bnosac/cronR&#34;&gt;cronR&lt;/a&gt; , which now has an RStudio Server addin, which looks like a good option.&lt;/p&gt;

&lt;h2 id=&#34;pushing-the-docker-changes&#34;&gt;Pushing the Docker changes&lt;/h2&gt;

&lt;p&gt;We now commit and push changes to the &lt;a href=&#34;https://cloud.google.com/container-registry/docs/&#34;&gt;Google Docker Hub&lt;/a&gt;.  The Docker command docs are &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/commit/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## CTRL-D to come out of the docker container again

## to get the container id e.g. c3f279d17e0a
sudo docker ps 

## commit with a message
sudo docker commit -a &amp;quot;Mark&amp;quot; -m &amp;quot;Added R stuff&amp;quot; \
    CONTAINER_ID yourname/your-new-docker-image

## list your new image with the old
sudo docker images

## tag the image with the location
sudo docker tag yourname/your-new-docker-image \
                gcr.io/your-project-id/your-new-docker-image

## push to Google Docker registry
sudo gcloud docker push \
     gcr.io/your-project-id/your-new-docker-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You could now pull this image using:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo gcloud docker pull \
  gcr.io/your-project-id/your-new-image-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;but this will be taken care of in the startup behaviour below.&lt;/p&gt;

&lt;p&gt;Remember to commit any changes each time you change the configuration of RStudio.&lt;/p&gt;

&lt;h2 id=&#34;setup-restart-behaviour&#34;&gt;Setup restart behaviour&lt;/h2&gt;

&lt;p&gt;Now we want to configure the above to happen everytime the VM starts up.  I use a startup script for pulling the latest docker image and updating any packages or data from github so to refresh I just need to restart the server.&lt;/p&gt;

&lt;h3 id=&#34;download-latest-custom-packages-and-data&#34;&gt;Download latest custom packages and data&lt;/h3&gt;

&lt;p&gt;In the custom metadata for the VM, we need the field &lt;code&gt;startup-script&lt;/code&gt; and then optional other metadata.&lt;/p&gt;

&lt;p&gt;The metadata is kept seperate away from your running containers, but available via the Google metadata commands.  This can be used for things like passwords and security settings you would prefer not to be shipped in a Docker container, and is easier to manage - just edit the metadata keys.&lt;/p&gt;

&lt;p&gt;Create one metadata field per bash script variable - examples below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;github-user: your-githubname

github-repo: your-github-repo

github-data: your-github-data

github-pat: your-github-pat
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This startup script loads the metadata above and downloads custom R packages and data files from github.&lt;/p&gt;

&lt;p&gt;You can save this locally as &lt;code&gt;startup.sh&lt;/code&gt; and upload via &lt;code&gt;gcloud&lt;/code&gt; or paste it into the interface into a metadata field called &lt;code&gt;startup-script&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;#!/bin/bash
GH_USER=$(curl http://metadata/computeMetadata/v1/instance/attributes/github-user -H &amp;quot;Metadata-Flavor: Google&amp;quot;)
GH_REPO=$(curl http://metadata/computeMetadata/v1/instance/attributes/github-repo -H &amp;quot;Metadata-Flavor: Google&amp;quot;)
GH_DATA=$(curl http://metadata/computeMetadata/v1/instance/attributes/github-data -H &amp;quot;Metadata-Flavor: Google&amp;quot;)
GH_PAT=$(curl http://metadata/computeMetadata/v1/instance/attributes/github-pat -H &amp;quot;Metadata-Flavor: Google&amp;quot;)

## Run Docker image
docker run --name rstudio-server \
       -d -p 8787:8787 \
       -e USER=ADMIN_USERNAME \
       -e PASSWORD=ADMIN_PW \
       -v /mnt/data/:/home/ \
       gcr.io/your-project-name/your-image-name

## Data files: update from github
cd /mnt/data/user_name/project_name/data
git pull &#39;https://&#39;$GH_USER&#39;:&#39;$GH_PAT&#39;@github.com/&#39;$GH_USER&#39;/&#39;$GH_DATA&#39;.git&#39;

## Private packages 
# 1. pull from github
cd /mnt/data/R/privatePackageName
git pull &#39;https://&#39;$GH_USER&#39;:&#39;$GH_PAT&#39;@github.com/&#39;$GH_USER&#39;/&#39;$GH_REPO&#39;.git&#39;

# 2. Install the package from the local file you just updated from Git
sudo docker exec -it rstudio-server \
     sudo su - -c &amp;quot;R -e \&amp;quot;devtools::install(&#39;/home/R/localPackageName/&#39;)\&amp;quot;&amp;quot;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can debug your startup script by connecting to your instance and viewing &lt;code&gt;cat /var/log/startupscript.log&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You could also have a &lt;code&gt;shutdown-script&lt;/code&gt; that would execute before any shutdown.  Check out the &lt;a href=&#34;https://cloud.google.com/compute/docs/metadata&#34;&gt;list of metadata&lt;/a&gt; you can pass into scripts.&lt;/p&gt;

&lt;h2 id=&#34;launch&#34;&gt;Launch&lt;/h2&gt;

&lt;p&gt;Now we relaunch the newly configured VM from your local computer, to test the startup script works.  This will copy over all the configurations from the RStudio server above.&lt;/p&gt;

&lt;p&gt;Save the &lt;code&gt;startup.sh&lt;/code&gt; script to a file on your local machine, cd into the same folder and add the metadata:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud compute instances add-metadata your-r-server-name \
  --metadata-from-file startup-script=startup.sh \
  --metadata github-user=YOUR-USER,github-repo=YOUR_REPO,github-data=YOUR_DATA,github-pat=YOUR_PAT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now reset, cross fingers:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## reset
gcloud compute instances reset your-r-server-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a couple of minutes everything should now be running as configured before.&lt;/p&gt;

&lt;p&gt;If you want to stop or start the server again, use the below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## stop (no billing) but not deleted
gcloud compute instances stop your-r-server-name

## start up a stopped server
gcloud compute instances start your-r-server-name
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;the-future&#34;&gt;The future&lt;/h2&gt;

&lt;p&gt;This should put you in a great position to support R-scripts to the team, but also in a scalable way where starting up faster and bigger machines is just a case of updating configuration files.&lt;/p&gt;

&lt;p&gt;I would like to start up containers using the container manifest syntax but couldn&amp;rsquo;t get it to work for me yet, but for just one VM it means a few less lines in the start up script.&lt;/p&gt;

&lt;p&gt;We also have &lt;a href=&#34;http://iihnordic.dk/blog/posts/2016/marts/creating-a-content-recommendation-engine-using-r-opencpu-and-gtm&#34;&gt;OpenCPU&lt;/a&gt; and &lt;a href=&#34;https://www.rstudio.com/products/shiny/shiny-server/&#34;&gt;Shiny Server&lt;/a&gt; in the Google project, as we move into providing data output such as visualisations, APIs and dashboards.  They are setup in a similar fashion, just swap out the Docker image for the appropriate version you need.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>