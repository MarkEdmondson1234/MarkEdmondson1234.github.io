<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Google Analytics on Mark Edmondson</title>
    <link>http://code.markedmondson.me/tags/google-analytics/index.xml</link>
    <description>Recent content in Google Analytics on Mark Edmondson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-GB</language>
    <copyright>Powered by [Hugo](//gohugo.io). Theme by [PPOffice](http://github.com/ppoffice).</copyright>
    <atom:link href="http://code.markedmondson.me/tags/google-analytics/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Four Ways to Schedule R scripts on Google Cloud Platform</title>
      <link>http://code.markedmondson.me/4-ways-schedule-r-scripts-on-google-cloud-platform</link>
      <pubDate>Sun, 09 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>http://code.markedmondson.me/4-ways-schedule-r-scripts-on-google-cloud-platform</guid>
      <description>&lt;p&gt;A common question I come across is how to automate scheduling of R scripts downloading data. This post goes through some options that I have played around with, which I’ve mostly used for downloading API data such as Google Analytics using the Google Cloud platform, but the same principles could apply for AWS or Azure.&lt;/p&gt;
&lt;!--more--&gt;
&lt;div id=&#34;scheduling-scripts-advice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scheduling scripts advice&lt;/h2&gt;
&lt;p&gt;But first, some notes on the scripts you are scheduling, that I’ve picked up.&lt;/p&gt;
&lt;div id=&#34;dont-save-data-to-the-scheduling-server&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Don’t save data to the scheduling server&lt;/h3&gt;
&lt;p&gt;I would suggest to not save or use data in the same place you are doing the scheduling. Use a service like BigQuery (&lt;code&gt;bigQueryR&lt;/code&gt;) or googleCloudStorageR (&lt;code&gt;googleCloudStorageR&lt;/code&gt;) to first load any necessary data, do your work then save it out again. This may be a bit more complicated to set up, but will save you tears if the VM or service goes down - you still have your data.&lt;/p&gt;
&lt;p&gt;To help with this, on Google Cloud you can authenticate with the same details you used to launch a VM to authenticate with the storage services above (as all are covered under the &lt;code&gt;http://www.googleapis.com/auth/cloud-services&lt;/code&gt; scope) - you can access this auth when on a GCE VM in R via &lt;code&gt;googleAuthR::gar_gce_auth()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;An example skeleton script is shown below that may be something you are scheduling.&lt;/p&gt;
&lt;p&gt;It downloads authentication files, does an API call, then saves it up to the cloud again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleAuthR)
library(googleCloudStorageR)

gcs_global_bucket(&amp;quot;my-bucket&amp;quot;)
## auth on the VM
options(googleAuthR.scopes.selected = &amp;quot;https://www.googleapis.com/auth/cloud-platform&amp;quot;)
gar_gce_auth()

## use the GCS auth to download the auth files for your API
auth_file &amp;lt;- &amp;quot;auth/my_auth_file.json&amp;quot;
gcs_get_object(auth_file, saveToDisk = TRUE)

## now auth with the file you just download
gar_auth_service(auth_file)

## do your work with APIs etc.
.....

## upload results back up to GCS (or BigQuery, etc.)
gcs_upload(my_results, name = &amp;quot;results/my_results.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;set-up-a-schedule-for-logs-too&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Set up a schedule for logs too&lt;/h3&gt;
&lt;p&gt;Logs are important for scheduled jobs, so you have some idea on whats happened when things go wrong. To help with scheduling debugging, most &lt;code&gt;googleAuthR&lt;/code&gt; packages now have a timestamp on their output messages.&lt;/p&gt;
&lt;p&gt;You can send the output of your scripts to log files, if using cron and RScript it looks something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;RScript /your-r-script.R &amp;gt; your-r-script.log&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;…where &lt;code&gt;&amp;gt;&lt;/code&gt; sends the output to the new file.&lt;/p&gt;
&lt;p&gt;Over time though, this can get big and (sigh) fill up your disk so you can’t log in to the VM (speaking from experience here!) so I now set up another scheduled job that every week takes the logs and uploads to GCS, then deletes the current ones.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;consider-using-docker-for-environments&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Consider using Docker for environments&lt;/h3&gt;
&lt;p&gt;Several of the methods below use &lt;a href=&#34;https://www.docker.com/&#34;&gt;&lt;code&gt;Docker&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The reasons for that is Docker provides a nice reprodueable way to define exactly what packages and dependencies you need for your script to run, which can run on top of any type of infrastructure as &lt;code&gt;Docker&lt;/code&gt; has quickly become a cloud standard.&lt;/p&gt;
&lt;p&gt;For instance, migrating from Google Cloud to AWS is much easier if both can be deployed using Docker, and below Docker is instrumental in allowing you to run on multiple solutions.&lt;/p&gt;
&lt;p&gt;Bear in mind that when a Docker container relaunches it won’t save any data, so any non-saved state will be lost (you should make a new container if you need it to contain data), but you’re not saving your data to the docker container anyway, aren’t you?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;scheduling-options---pros-and-cons&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scheduling options - Pros and cons&lt;/h2&gt;
&lt;p&gt;Here is an overview of the pros and cons of the options presented in more detail below:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;29%&#34; /&gt;
&lt;col width=&#34;35%&#34; /&gt;
&lt;col width=&#34;35%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Pros&lt;/th&gt;
&lt;th&gt;Cons&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1. &lt;code&gt;cronR&lt;/code&gt; Addin on &lt;code&gt;RStudio Server&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Simple and quick&lt;/td&gt;
&lt;td&gt;Not so robust, need to log into server to make changes, versioning packages.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2. &lt;code&gt;gce_vm_scheduler&lt;/code&gt; and &lt;code&gt;Dockerfiles&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Robust and can launch from local R session, support versioning&lt;/td&gt;
&lt;td&gt;Need to build Dockerfiles, all scripts on one VM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;3. Master &amp;amp; Slave VM&lt;/td&gt;
&lt;td&gt;Tailor a fresh VM for each script, cheaper&lt;/td&gt;
&lt;td&gt;Need to build Dockerfiles, more complicated VM setup.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;4. Google AppEngine with flexible containers&lt;/td&gt;
&lt;td&gt;Managed platform&lt;/td&gt;
&lt;td&gt;Need to turn script into web responses, more complicated setup&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;cronr-plus-rstudio-server&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1 - cronR plus RStudio Server&lt;/h2&gt;
&lt;p&gt;This is the simplest and the one to start with.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Start up an RStudio Server instance&lt;/li&gt;
&lt;li&gt;Install &lt;a href=&#34;https://github.com/bnosac/cronR&#34;&gt;&lt;code&gt;cronR&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Upload your R script&lt;/li&gt;
&lt;li&gt;Schedule your script using &lt;code&gt;cronR&lt;/code&gt; RStudio addin&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With &lt;code&gt;googleComputeEngineR&lt;/code&gt; and the new &lt;code&gt;gcer-public&lt;/code&gt; project containing public images that include one with &lt;code&gt;cronR&lt;/code&gt; already installed, this is as simple as the few lines of code below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleComputeEngineR)

## get the tag for prebuilt Docker image with googleAuthRverse, cronR and tidyverse
tag &amp;lt;- gce_tag_container(&amp;quot;google-auth-r-cron-tidy&amp;quot;, project = &amp;quot;gcer-public&amp;quot;)
# gcr.io/gcer-public/google-auth-r-cron-tidy

## start a custom Rstudio instance
vm &amp;lt;- gce_vm(name = &amp;quot;my-rstudio&amp;quot;,
              predefined_type = &amp;quot;n1-highmem-8&amp;quot;,
              template = &amp;quot;rstudio&amp;quot;,
              dynamic_image = tag,
              username = &amp;quot;me&amp;quot;, password = &amp;quot;mypassword&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait for it to launch and give you an IP, then log in, upload a script and configure the schedule via the &lt;code&gt;cronR&lt;/code&gt; addin.&lt;/p&gt;
&lt;p&gt;Some more detail about this workflow can be found at these &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/rstudio-team.html&#34;&gt;custom RStudio example workflows&lt;/a&gt; on the &lt;code&gt;googleComputeEngineR&lt;/code&gt; website.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gce_vm_scheduler-and-dockerfiles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2- &lt;em&gt;gce_vm_scheduler&lt;/em&gt; and &lt;em&gt;Dockerfiles&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;This method I prefer to the above since it lets you create the exact environment (e.g. package versions, dependencies) to run your script in, that you can trail dev and production versions with. It also works locally without needing to log into the server each time to deploy a script.&lt;/p&gt;
&lt;div id=&#34;handy-tools-for-docker---containerit-and-build-triggers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Handy tools for Docker - containerit and Build Triggers&lt;/h3&gt;
&lt;p&gt;Here we introduce Docker images, which may have been more a technical barrier for some before (but worth knowing, I think)&lt;/p&gt;
&lt;div id=&#34;containerit&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;containerit&lt;/h4&gt;
&lt;p&gt;Things are much easier now though, as we have the magic new R package &lt;a href=&#34;http://o2r.info/2017/05/30/containerit-package/&#34;&gt;&lt;code&gt;containerit&lt;/code&gt;&lt;/a&gt; which can generate these Docker files for you - just send &lt;code&gt;containerit::dockerfile()&lt;/code&gt; around the script file.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build-triggers&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Build Triggers&lt;/h4&gt;
&lt;p&gt;Along with auto-generating Dockerfiles, for Google Cloud in particular we now also have &lt;a href=&#34;https://cloud.google.com/container-builder/docs/how-to/build-triggers&#34;&gt;Build Triggers&lt;/a&gt; which automates building the Docker image for you.&lt;/p&gt;
&lt;p&gt;Just make the Dockerfile, then set up a trigger for when you push that file up to GitHub - you can see the ones used to create the public R resources here in the &lt;a href=&#34;https://github.com/cloudyr/googleComputeEngineR/tree/master/inst/dockerfiles&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt; repo&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;recipe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Recipe&lt;/h3&gt;
&lt;p&gt;Putting it all together then, documentation of this workflow for &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/scheduled-rscripts.html&#34;&gt;scheduling R scripts is found here&lt;/a&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;If you don’t already have one, start up a scheduler VM using &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/gce_vm_scheduler.html&#34;&gt;&lt;code&gt;gce_vm_scheduler&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Create a Dockerfile either manually or using &lt;code&gt;containerit&lt;/code&gt; that will run your script upon execution&lt;/li&gt;
&lt;li&gt;Upload the Dockerfile to a git repo (private or public)&lt;/li&gt;
&lt;li&gt;Setup a build trigger for that Dockerfile&lt;/li&gt;
&lt;li&gt;Once built, set a script to schedule within that Dockerfile with &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/gce_schedule_docker.html&#34;&gt;&lt;code&gt;gce_schedule_docker&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is still in beta at time of writing but should be stable by the time &lt;code&gt;googlecomputeEngineR&lt;/code&gt; hits CRAN &lt;code&gt;0.2.0&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;master-and-slave-vms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3 - Master and Slave VMs&lt;/h2&gt;
&lt;p&gt;Some scripts take more resources than others, and since you are using VMs already you can have more control over what specifications of VM to launch based on the script you want to run.&lt;/p&gt;
&lt;p&gt;This means you can have a cheap scheduler server, that launch biggers VMs for the duration of the job. As GCP charges per minute, this can save you money over having a schedule server that is as big as what your most expensive script needs running 24/7.&lt;/p&gt;
&lt;p&gt;This method is largely like the scheduled scripts above, except in this case the scheduled script is also launching VMs to run the job upon.&lt;/p&gt;
&lt;p&gt;Using &lt;code&gt;googleCloudStorageR::gcs_source&lt;/code&gt; you can run an R script straight from where it is hosted upon GCS, meaning all data, authentication files and scripts can be kept seperate from the computation. An example master script is shown below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## intended to be run on a small instance via cron
## use this script to launch other VMs with more expensive tasks
library(googleComputeEngineR)
library(googleCloudStorageR)
gce_global_project(&amp;quot;my-project&amp;quot;)
gce_global_zone(&amp;quot;europe-west1-b&amp;quot;)
gcs_global_bucket(&amp;quot;your-gcs-bucket&amp;quot;)

## auth to same project we&amp;#39;re on
googleAuthR::gar_gce_auth()

## launch the premade VM
vm &amp;lt;- gce_vm(&amp;quot;slave-1&amp;quot;)

## set SSH to use &amp;#39;master&amp;#39; username as configured before
vm &amp;lt;- gce_ssh_setup(vm, username = &amp;quot;master&amp;quot;, ssh_overwrite = TRUE)

## run the script on the VM that will source from GCS
runme &amp;lt;- &amp;quot;Rscript -e \&amp;quot;googleAuthR::gar_gce_auth();googleCloudStorageR::gcs_source(&amp;#39;download.R&amp;#39;, bucket = &amp;#39;your-gcs-bucket&amp;#39;)\&amp;quot;&amp;quot;
out &amp;lt;- docker_cmd(vm, 
                  cmd = &amp;quot;exec&amp;quot;, 
                  args = c(&amp;quot;rstudio&amp;quot;, runme), 
                  wait = TRUE)

## once finished, stop the VM
gce_vm_stop(vm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More detail is again available at the &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/scheduled-rscripts.html#master-slave-scheduler&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt; website&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;google-app-engine-with-flexible-custom-runtimes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4 - Google App Engine with flexible custom runtimes&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/appengine/docs/&#34;&gt;Google App Engine&lt;/a&gt; has always had schedule options, but only for its supported languages of Python, Java, PHP etc. Now with the &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/&#34;&gt;introduction of flexible containers&lt;/a&gt;, any Docker container running any language (including R) can also be run.&lt;/p&gt;
&lt;p&gt;This is potentially the best solution since it runs upon a 100% managed platform, meaning you don’t need to worry about servers at all, and it takes care of things like server maintence, logging etc.&lt;/p&gt;
&lt;div id=&#34;setting-up-your-script-for-app-engine&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Setting up your script for App Engine&lt;/h3&gt;
&lt;p&gt;There are some &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/build&#34;&gt;requirements for the container&lt;/a&gt; that need configuring so it can run:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can not use &lt;code&gt;googleAuthR::gar_gce_auth()&lt;/code&gt; so will need to upload the auth token within the Dockerfile.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;AppEngine expects a web service to be listening on port 8080, so your schedule script needs to be triggered via HTTP requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For authentication, I use the system environment arguments (i.e. those usually set in &lt;code&gt;.Renviron&lt;/code&gt;) that &lt;code&gt;googleAuthR&lt;/code&gt; packages use for auto-authentication. Put the auth file (such as JSON or a &lt;code&gt;.httr-oauth&lt;/code&gt; file) into the deployment folder, then point to its location via specifying in the &lt;code&gt;app.yaml&lt;/code&gt;. Details below.&lt;/p&gt;
&lt;p&gt;To solve the need for being a webservice on port 8080 (which is then proxied to normal webports 80/443), &lt;a href=&#34;https://www.rplumber.io/&#34;&gt;&lt;code&gt;plumber&lt;/code&gt;&lt;/a&gt; is a great service by Jeff Allen of RStudio, which already comes with its own Docker solution. You can then modify that &lt;code&gt;Dockerfile&lt;/code&gt; slightly so that it works on App Engine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;recipe-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Recipe&lt;/h3&gt;
&lt;p&gt;To then schedule your R script on app engine, follow the guide below, first making sure you have setup the &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/&#34;&gt;gcloud CLI&lt;/a&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create a Google Appengine project in the US region (only region that supports flexible containers at the moment)&lt;/li&gt;
&lt;li&gt;Create a scheduled script e.g. &lt;code&gt;schedule.R&lt;/code&gt; - you can use auth from environment files specified in &lt;code&gt;app.yaml&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Make an API out of the script by using &lt;code&gt;plumber&lt;/code&gt; - example:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(googleAuthR)         ## authentication
library(googleCloudStorageR)  ## google cloud storage
library(readr)                ## 
## gcs auto authenticated via environment file 
## pointed to via sys.env GCS_AUTH_FILE

#* @get /demoR
demoScheduleAPI &amp;lt;- function(){
  
  ## download or do something
  something &amp;lt;- tryCatch({
      gcs_get_object(&amp;quot;schedule/test.csv&amp;quot;, 
                     bucket = &amp;quot;mark-edmondson-public-files&amp;quot;)
    }, error = function(ex) {
      NULL
    })
      
  something_else &amp;lt;- data.frame(X1 = 1,
                               time = Sys.time(), 
                               blah = paste(sample(letters, 10, replace = TRUE), collapse = &amp;quot;&amp;quot;))
  something &amp;lt;- rbind(something, something_else)
  
  tmp &amp;lt;- tempfile(fileext = &amp;quot;.csv&amp;quot;)
  on.exit(unlink(tmp))
  write.csv(something, file = tmp, row.names = FALSE)
  ## upload something
  gcs_upload(tmp, 
             bucket = &amp;quot;mark-edmondson-public-files&amp;quot;, 
             name = &amp;quot;schedule/test.csv&amp;quot;)
  
  message(&amp;quot;Done&amp;quot;, Sys.time())
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create Dockerfile. If using &lt;code&gt;containerit&lt;/code&gt; then replace FROM with &lt;code&gt;trestletech/plumber&lt;/code&gt; and add the below lines to use correct AppEngine port:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(containerit)

dockerfile &amp;lt;- dockerfile(&amp;quot;schedule.R&amp;quot;, copy = &amp;quot;script_dir&amp;quot;, soft = TRUE)
write(dockerfile, file = &amp;quot;Dockerfile&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then change/add these lines to the created Dockerfile:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;EXPOSE 8080
ENTRYPOINT [&amp;quot;R&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;pr &amp;lt;- plumber::plumb(commandArgs()[4]); pr$run(host=&amp;#39;0.0.0.0&amp;#39;, port=8080)&amp;quot;]
CMD [&amp;quot;schedule.R&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Example final Dockerfile below. This doesn’t need to be built in say a build trigger as its built upon app engine deployment.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FROM trestletech/plumber
LABEL maintainer=&amp;quot;mark&amp;quot;
RUN export DEBIAN_FRONTEND=noninteractive; apt-get -y update \
 &amp;amp;&amp;amp; apt-get install -y libcairo2-dev \
    libcurl4-openssl-dev \
    libgmp-dev \
    libpng-dev \
    libssl-dev \
    libxml2-dev \
    make \
    pandoc \
    pandoc-citeproc \
    zlib1g-dev
RUN [&amp;quot;install2.r&amp;quot;, &amp;quot;-r &amp;#39;https://cloud.r-project.org&amp;#39;&amp;quot;, &amp;quot;readr&amp;quot;, &amp;quot;googleCloudStorageR&amp;quot;, &amp;quot;Rcpp&amp;quot;, &amp;quot;digest&amp;quot;, &amp;quot;crayon&amp;quot;, &amp;quot;withr&amp;quot;, &amp;quot;mime&amp;quot;, &amp;quot;R6&amp;quot;, &amp;quot;jsonlite&amp;quot;, &amp;quot;xtable&amp;quot;, &amp;quot;magrittr&amp;quot;, &amp;quot;httr&amp;quot;, &amp;quot;curl&amp;quot;, &amp;quot;testthat&amp;quot;, &amp;quot;devtools&amp;quot;, &amp;quot;hms&amp;quot;, &amp;quot;shiny&amp;quot;, &amp;quot;httpuv&amp;quot;, &amp;quot;memoise&amp;quot;, &amp;quot;htmltools&amp;quot;, &amp;quot;openssl&amp;quot;, &amp;quot;tibble&amp;quot;, &amp;quot;remotes&amp;quot;]
RUN [&amp;quot;installGithub.r&amp;quot;, &amp;quot;MarkEdmondson1234/googleAuthR@7917351&amp;quot;, &amp;quot;hadley/rlang@ff87439&amp;quot;]
WORKDIR /payload/
COPY [&amp;quot;.&amp;quot;, &amp;quot;./&amp;quot;]

EXPOSE 8080
ENTRYPOINT [&amp;quot;R&amp;quot;, &amp;quot;-e&amp;quot;, &amp;quot;pr &amp;lt;- plumber::plumb(commandArgs()[4]); pr$run(host=&amp;#39;0.0.0.0&amp;#39;, port=8080)&amp;quot;]
CMD [&amp;quot;schedule.R&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Specify &lt;code&gt;app.yaml&lt;/code&gt; for flexible containers as &lt;a href=&#34;https://cloud.google.com/appengine/docs/flexible/custom-runtimes/&#34;&gt;detailed here&lt;/a&gt;. Add any environment vars such as auth files, that will be included in same deployment folder.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;runtime: custom
env: flex

env_variables:
  GCS_AUTH_FILE: auth.json&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Specify &lt;code&gt;cron.yaml&lt;/code&gt; for the schedule needed:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;yaml&#34;&gt;&lt;code&gt;cron:
- description: &amp;quot;test cron&amp;quot;
  url: /demoR
  schedule: every 1 hours&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;You should now have these files in the deployment folder:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;app.yaml&lt;/code&gt; - configuration of general app settings&lt;/li&gt;
&lt;li&gt;&lt;code&gt;auth.json&lt;/code&gt; - an authentication file specified in env arguments or app.yaml&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cron.yaml&lt;/code&gt; - specification of when your scheduling is&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Dockerfile&lt;/code&gt; - specification of the environment&lt;/li&gt;
&lt;li&gt;&lt;code&gt;schedule.R&lt;/code&gt; - the plumber version of your script containing your endpoints&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Open the terminal in that folder, and deploy via &lt;code&gt;gcloud app deploy --project your-project&lt;/code&gt; and the cron schedule via &lt;code&gt;gcloud app deploy cron.yaml --project your-project&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It will take a while (up to 10 mins) the first time.&lt;/p&gt;
&lt;ol start=&#34;8&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The App Engine should then be deployed on &lt;a href=&#34;https://your-project.appspot.com/&#34; class=&#34;uri&#34;&gt;https://your-project.appspot.com/&lt;/a&gt; - every &lt;code&gt;GET&lt;/code&gt; request to &lt;a href=&#34;https://your-project.appspot.com/demoR&#34; class=&#34;uri&#34;&gt;https://your-project.appspot.com/demoR&lt;/a&gt; (or other endpoints you have specified in R script) will run the R code. The cron example above will run every hour to this endpoint.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Logs for the instance are found &lt;a href=&#34;https://console.cloud.google.com/logs/viewer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This approach is the most flexible, and offers a fully managed platform for your scripts. Scheduled scripts are only the beginning, since deploying such actually gives you a way to run R scripts in response to any HTTP request from any language - triggers could also include if someone updates a spreadsheet, adds a file to a folder, pushes to GitHub etc. which opens up a lot of exciting possibilities. You can also scale it up to become a fully functioning R API.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Hopefully this has given you an idea on your options for R on Google Cloud regarding scheduling. If you have some other easier workflows or suggestions for improvements please put them in the comments below!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>My R Packages</title>
      <link>http://code.markedmondson.me/r-packages/</link>
      <pubDate>Tue, 24 Jan 2017 21:20:50 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/r-packages/</guid>
      <description>&lt;p&gt;A full list of R packages I have published are on &lt;a href=&#34;https://github.com/MarkEdmondson1234&#34;&gt;my Github&lt;/a&gt;, but some notable ones are below.&lt;/p&gt;
&lt;p&gt;Some are part of the &lt;a href=&#34;http://cloudyr.github.io/&#34;&gt;cloudyR project&lt;/a&gt;, which has many packages useful for using R in the cloud. &lt;img src=&#34;http://code.markedmondson.me/images/cloudyr2.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I concentrate on the Google cloud below, but be sure to check out the other packages if you’re looking to work with AWS or other cloud based services.&lt;/p&gt;
&lt;div id=&#34;cran&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CRAN&lt;/h2&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;19%&#34; /&gt;
&lt;col width=&#34;25%&#34; /&gt;
&lt;col width=&#34;55%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;URL&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=googleAuthR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/googleAuthR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/googleAuthR/&#34;&gt;googleAuthR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;The central workhorse for authentication on Google APIs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=googleAnalyticsR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/googleAnalyticsR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/&#34;&gt;googleAnalyticsR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Works with Google Analytics Reporting V3/V4 and Management APIs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=googleComputeEngineR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/googleComputeEngineR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/&#34;&gt;googleComputeEngineR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Launch Virtual Machines within the Google Cloud, via templates or your own Docker containers.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=googleCloudStorageR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/googleCloudStorageR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/googleCloudStorageR/&#34;&gt;googleCloudStorageR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Interact with Google Cloud Storage via R&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=bigQueryR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/bigQueryR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/bigQueryR/&#34;&gt;bigQueryR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Interact with Google BigQuery via R&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;http://cran.r-project.org/package=searchConsoleR&#34;&gt;&lt;img src=&#34;http://www.r-pkg.org/badges/version/searchConsoleR&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;http://code.markedmondson.me/searchConsoleR/&#34;&gt;searchConsoleR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Download Search Console data into R&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can tell, most are aimed towards helping R users with help in digital analytics and cloud based services. You can get some idea of how they can &lt;a href=&#34;http://code.markedmondson.me/digital-analytics-workflow-through-google-cloud/&#34;&gt;work together in a digital analytics workflow here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;github&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GITHUB&lt;/h2&gt;
&lt;p&gt;More experimental packages:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;19%&#34; /&gt;
&lt;col width=&#34;25%&#34; /&gt;
&lt;col width=&#34;55%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;th&gt;URL&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Dev&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/googleMeasureR&#34;&gt;googleMeasureR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Send tracking hits to Google Analytics from R code using the Google Analytics Measurement Protocol&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;a href=&#34;https://travis-ci.org/MarkEdmondson1234/googleLanguageR&#34;&gt;&lt;img src=&#34;https://travis-ci.org/MarkEdmondson1234/googleLanguageR.png?branch=master&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/googleLanguageR&#34;&gt;googleLanguageR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Access Speech to text, translation and NLP text processing APIs&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;&lt;a href=&#34;https://travis-ci.org/MarkEdmondson1234/googleID&#34;&gt;&lt;img src=&#34;https://travis-ci.org/MarkEdmondson1234/googleID.svg?branch=master&#34; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/googleID&#34;&gt;googleID&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;In production, but very small so not on CRAN. Allows user authentication via Google+ API for Shiny and RMarkdown documents.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Dev&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/youtubeAnalyticsR&#34;&gt;youtubeAnalyticsR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Access YouTube Analytics data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Deprecated&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/gtmR&#34;&gt;gtmR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Superceded by &lt;a href=&#34;https://github.com/IronistM/googleTagManageR&#34;&gt;googleTagManagerR&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Reference&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/autoGoogleAPI&#34;&gt;autoGoogleAPI&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;152 R packages auto generated via &lt;code&gt;googleAuthR&lt;/code&gt;’s discovery API feature&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Done&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/gentelellaShiny&#34;&gt;gentelellaShiny&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A custom Shiny theme available in a package&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Deprecated&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/stripeR&#34;&gt;stripeR&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Interact with the Stripe payment API, but superseded by another R package, &lt;a href=&#34;https://cran.r-project.org/web/packages/RStripe/index.html&#34;&gt;RStripe&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Insights sorting by delta metrics in the Google Analytics API v4</title>
      <link>http://code.markedmondson.me/quicker-insight-sort-metric-delta/</link>
      <pubDate>Thu, 01 Dec 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/quicker-insight-sort-metric-delta/</guid>
      <description>

&lt;p&gt;As analysts, we are often called upon to see how website metrics have improved or declined over time.  This is easy enough when looking at trends, but if you are looking to break down over other dimensions, it can involve a lot of ETL to get to what you need.&lt;/p&gt;

&lt;p&gt;For instance, if you are looking at landing page performance of SEO traffic you can sort by the top performers, but not by the top &lt;em&gt;most improved&lt;/em&gt; performers.  To see that you need to first extract your metrics for one month, extract it again for the comparison month, join the datasets on the page dimension and then create and sort by a delta metric.  For large websites, you can be exporting millions of URLs just so you can see say the top 20 most improved.&lt;/p&gt;

&lt;p&gt;This comes from the fact the Google Analytics web UI and Data Studio don&amp;rsquo;t let you sort by the &lt;em&gt;change&lt;/em&gt; of a metric.  However, this is available in the Google Analytics API v4 so a small demo on how to it and how it can be useful is shown here.&lt;/p&gt;

&lt;h2 id=&#34;extracting-the-data&#34;&gt;Extracting the data&lt;/h2&gt;

&lt;p&gt;In v4, you can pass in two date ranges in one call.  When you do this a new ordering type comes available, the &lt;a href=&#34;https://developers.google.com/analytics/devguides/reporting/core/v4/basics#delta_ordering&#34;&gt;&lt;code&gt;DELTA&lt;/code&gt;&lt;/a&gt; which is what we can use to sort the results.&lt;/p&gt;

&lt;p&gt;Bear in mind any metric filters you add will apply to the first date range, not the second.&lt;/p&gt;

&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;

&lt;p&gt;The below is implemented in R using &lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/&#34;&gt;&lt;code&gt;googleAnalyticsR&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We first load the library, authenticate and set our ViewID:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleAnalyticsR)
ga_auth()

al &amp;lt;- google_analytics_account_list()

gaid &amp;lt;- yourViewID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These are some helper functions to get the start and end dates of last month, and the same month the year before:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Start of the month
#&#39; @param x A date
som &amp;lt;- function(x) {
  as.Date(format(x, &amp;quot;%Y-%m-01&amp;quot;))
}

#&#39; End of the month
#&#39; @param x A date
eom &amp;lt;- function(x) {
  som(som(x) + 35) - 1
}

#&#39; Start and end of month
get_start_end_month &amp;lt;- function(x = Sys.Date()){
  c(som(som(x) - 1), som(x) - 1)
}

last_month &amp;lt;- get_start_end_month()
year_before &amp;lt;- get_start_end_month(Sys.Date() - 365)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We now create an SEO filter as we only want to examine SEO traffic, and a transactions over 0 metric filter:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## only organic traffic
seo_filter &amp;lt;- filter_clause_ga4(list(dim_filter(&amp;quot;medium&amp;quot;, 
                                                 &amp;quot;EXACT&amp;quot;, 
                                                 &amp;quot;organic&amp;quot;)
                               ))
                               
## met filters are on the first date
transaction0 &amp;lt;- filter_clause_ga4(list(met_filter(&amp;quot;transactions&amp;quot;, 
                                                  &amp;quot;GREATER_THAN&amp;quot;, 
                                                  0)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the sorting parameter, that we specify to be by the biggest change in transactions from last year at the top of the results:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## order by the delta change of year_before - last_month
delta_trans &amp;lt;- order_type(&amp;quot;transactions&amp;quot;,&amp;quot;DESCENDING&amp;quot;, &amp;quot;DELTA&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We now make the Google Analytics API v4 call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gadata &amp;lt;- google_analytics_4(gaid,
                             date_range = c(year_before, last_month),
                             metrics = c(&amp;quot;visits&amp;quot;,&amp;quot;transactions&amp;quot;,&amp;quot;transactionRevenue&amp;quot;),
                             dimensions = c(&amp;quot;landingPagePath&amp;quot;),
                             dim_filters = seo_filter,
                             met_filters = transaction0,
                             order = delta_trans,
                             max = 20)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should now have the top 20 most declined landing pages from last year measured by e-commerce transactions.  Much easier than downloading all pages and doing the delta calculations yourself.&lt;/p&gt;

&lt;p&gt;If you want to get the absolute number of declined transactions, you can add the column via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gadata$transactions.delta &amp;lt;- gadata$transactions.d2 - gadata$transactions.d1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;With this data you can now focus on making SEO improvements to those pages so they can reclaim their past glory, at the very least its a good starting point for investigations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Efficient anti-sampling with the Google Analytics Reporting API</title>
      <link>http://code.markedmondson.me/anti-sampling-google-analytics-api/</link>
      <pubDate>Fri, 05 Aug 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/anti-sampling-google-analytics-api/</guid>
      <description>

&lt;p&gt;Avoiding sampling is one of the most common reasons people start using the Google Analytics API.  This blog lays out some pseudo-code to do so in an efficient manner, avoiding too many unnecessary API calls.  The approach is used in the v4 calls for the R package &lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/v4.html&#34;&gt;&lt;code&gt;googleAnalyticsR&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;avoiding-the-daily-walk&#34;&gt;Avoiding the daily walk&lt;/h2&gt;

&lt;p&gt;The most common approach to mitigate sampling is to break down the API calls into one call per day.  This has some problems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Its inefficient.&lt;/strong&gt;  If you have 80% sampling or 10% sampling, you use the same number of API calls.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It takes a long time.&lt;/strong&gt;  A year long fetch is 365 calls of 5+ seconds that can equate to a 30mins+ wait.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It doesn’t always work.&lt;/strong&gt; If you have so many sessions its sampled for one day, you will still have sampling, albeit at a lower rate.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;anti-sampling-based-on-session-size&#34;&gt;Anti-sampling based on session size&lt;/h2&gt;

&lt;p&gt;Google Analytics sampling works as &lt;a href=&#34;https://support.google.com/analytics/answer/2637192&#34;&gt;outlined in this Google article&lt;/a&gt;.  The main points are that if your API call covers a date range greater than set session limits, it will return a sampled call.&lt;/p&gt;

&lt;p&gt;The session limits vary according to if you are using Google Analytics 360 and other unknown factors in a sampling algorithm.  Fortunately, this information is available in the API responses via the &lt;code&gt;samplesReadCounts&lt;/code&gt; and &lt;code&gt;samplingSpaceSizes&lt;/code&gt; meta data.  See the &lt;a href=&#34;https://developers.google.com/analytics/devguides/reporting/core/v4/rest/v4/reports/batchGet#ReportData&#34;&gt;v4 API reference&lt;/a&gt; for their definitions.&lt;/p&gt;

&lt;p&gt;These values change per API call, so the general strategy is to make two exploratory API calls first to get the sampling information and the number of sessions over the desired date period, then use that information to construct batches of calls over date ranges that are small enough to avoid sampling, but large enough to not waste API calls.&lt;/p&gt;

&lt;p&gt;The two exploratory API calls to find the meta data are more than made up for once you have saved calls in the actual data fetch.&lt;/p&gt;

&lt;h2 id=&#34;how-it-works-in-practice-80-quicker-data&#34;&gt;How it works in practice - 80%+ quicker data&lt;/h2&gt;

&lt;p&gt;Following this approach, I have found a huge improvement in time spent for sampled calls, making it much more useable in say dynamic dashboards where waiting 30 mins for data is not an option.&lt;/p&gt;

&lt;p&gt;An example response from the &lt;code&gt;googleAnalyticsR&lt;/code&gt; library is shown below - for a month&amp;rsquo;s worth of unsampled data  that would have taken 30 API calls via a daily walk, I get the same in 5 (2 to find batch sizes, 3 to get the data).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; library(googleAnalyticsR)
&amp;gt; ga_auth()
&amp;gt; ga_data &amp;lt;- 
    google_analytics_4(id, 
                       date_range = c(&amp;quot;2016-01-01&amp;quot;,
                                      &amp;quot;2016-02-01&amp;quot;), 
                       metrics = c(&amp;quot;sessions&amp;quot;,
                                   &amp;quot;bounceRate&amp;quot;), 
                       dimensions = c(&amp;quot;date&amp;quot;,
                                      &amp;quot;landingPagePath&amp;quot;,
                                      &amp;quot;source&amp;quot;), 
                       anti_sample = TRUE)
                                
anti_sample set to TRUE. Mitigating sampling via multiple API calls.
Finding how much sampling in data request...
Data is sampled, based on 54.06% of visits.
Downloaded [10] rows from a total of [76796].
Finding number of sessions for anti-sample calculations...
Downloaded [32] rows from a total of [32].
Calculated [3] batches are needed to download [113316] rows unsampled.
Anti-sample call covering 10 days: 2016-01-01, 2016-01-10
Downloaded [38354] rows from a total of [38354].
Anti-sample call covering 20 days: 2016-01-11, 2016-01-30
Downloaded [68475] rows from a total of [68475].
Anti-sample call covering 2 days: 2016-01-31, 2016-02-01
Downloaded [6487] rows from a total of [6487].
Finished unsampled data request, total rows [113316]
Successfully avoided sampling
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The time saved gets even greater the longer the time period you request.&lt;/p&gt;

&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;As with daily walk anti-sample techniques, user metrics and unique users are linked to the date range you are querying, so this technique will not match the numbers as if you queried over the whole date range.&lt;/p&gt;

&lt;p&gt;The sampling session limit is also applied at a web property level, not View for non-GA360 accounts, so its best to use this on a Raw data View, as filters will cause the session calculations be incorrect.&lt;/p&gt;

&lt;h2 id=&#34;example-pseudo-code&#34;&gt;Example pseudo-code&lt;/h2&gt;

&lt;p&gt;R code that implements the above is &lt;a href=&#34;https://github.com/MarkEdmondson1234/googleAnalyticsR/blob/master/R/anti_sample.R&#34;&gt;available here&lt;/a&gt;, but the pseudo-code below is intended for you to port to different languages:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Get the unsampled data
test_call = get_ga_api(full_date_range, 
                       metrics = your_metrics, 
                       dimensions = your_dimensions)

// # Read the sample meta data

// read_counts is the number of sessions before sampling starts
// I make it 10% smaller to ensure its small enough as
// it seems a bit flakey
read_counts = meta_data(test_call, &amp;quot;sampledReadCounts&amp;quot;)
read_counts = read_counts * 0.9

// space_size is total amount of sessions sampling was used for
space_size = meta_data(test_call, &amp;quot;samplingSpaceSizes&amp;quot;)

// dividing by each gives the % of sampling in the API call
samplingPercent = read_counts / space_size

// if there is no sample meta data, its not sampled. We&#39;re done!
if(read_counts = NULL or space_size = NULL):
  return(test_call)
  
// ..otherwise its sampled
// # get info for batch size calculations

// I found rowCount returned from a sampled call was not equal 
// to an unsampled call, so I add 20% to rowCount to adjust
rowCount = meta_data(test_call, &amp;quot;rowCount&amp;quot;)
rowCount = rowCount * 1.2

// get the number of sessions per day
date_sessions = get_ga_api(full_date_range, 
                           metric = &amp;quot;sessions&amp;quot;, 
                           dimensions = &amp;quot;date&amp;quot;)

// get the cumulative number of sessions over the year
date_sessions.cumulative = cumsum(date_sessions.sessions)

// modulus divide the cumulative sessions by the 
// sample read_counts.
date_sessions.sample_bucket = date_sessions.cumulative %% 
                                            read_counts

// get the new date ranges per sample_bucket group
new_date_ranges = get_min_and_max_date(date_sessions)

// new_date_ranges should now hold the smaller date ranges 
// for each batched API call

// # call GA api with new date ranges

total = empty_matrix

for i in new_date_ranges:

  if(new_date_ranges[i].start == new_date_ranges[i].end):
    // only one day so split calls into hourly
    // see below for do_hourly() explanation
    batch_call = do_hourly(new_date_range.start, 
                           metrics = your_metrics, 
                           dimensions = your_dimensions)
  else:
    // multi-day batching
    batch_call = get_ga_api(new_date_ranges[i], 
                            metrics = your_metrics, 
                            dimensions = your_dimensions)
                            
  total = total + batch_call
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;per-hour-fetching-do-hourly&#34;&gt;Per hour fetching do_hourly()&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;do_hourly()&lt;/code&gt; function is very similar to the above code for daily, but with a fetch to examine the session distribution per hour.  I only call it when necessary since it is a lot more API calls and its an edge case.&lt;/p&gt;

&lt;p&gt;If you need anti-sampling for sub-hourly, then you should really be looking at using the &lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/big-query.html&#34;&gt;BigQuery Google Analytics 360 exports&lt;/a&gt;. :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SEO keyword research using searchConsoleR and googleAnalyticsR</title>
      <link>http://code.markedmondson.me/search-console-google-analytics-r-keyword-research/</link>
      <pubDate>Tue, 21 Jun 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/search-console-google-analytics-r-keyword-research/</guid>
      <description>

&lt;p&gt;In this blog we look at a method to estimate where to prioritise your SEO resources, estimating which keywords will give the greatest increase in revenue if you could improve their Google rank.&lt;/p&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&#34;https://data-seo.com/&#34;&gt;Vincent at data-seo.com&lt;/a&gt; who proof read and corrected some errors in the first draft&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Data comes from &lt;a href=&#34;https://www.google.com/webmasters/tools/home&#34;&gt;Google Search Console&lt;/a&gt; and &lt;a href=&#34;https://www.google.com/webmasters/tools/home?hl=en&#34;&gt;Google Analytics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Search Console is used to provide the keywords in these days post &lt;a href=&#34;http://www.notprovidedcount.com/&#34;&gt;(not provided)&lt;/a&gt;.  We then link the two data sets by using the URLs as a key, and estimate how much each keyword made in revenue by splitting them in the same proportion as the traffic they have sent to the page.&lt;/p&gt;

&lt;p&gt;This approach assumes each keyword converts at the same rate once on the page, and will work better with some websites more than others - the best results I have seen are those websites with a lot of content on seperate URLs, such that they capture long-tail queries.  This is because the amount of keywords per URL is small, but with enough volume to make the estimates trustworthy.&lt;/p&gt;

&lt;p&gt;We also try to incorporate magins of error in the results.  This avoids situations where one click on position 40 gets a massive weighting in potential revenue, which in reality could have been a freak event.&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;The method produced a targeted keyword list of 226 from an original seed list of ~21,000.  The top 30 revenue targets are shown in the plot below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://code.markedmondson.me/images/see-potential.png&#34; alt=&#34;seo-forecast-estimate&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Read below on how the plot was generated and what the figures mean.&lt;/p&gt;

&lt;h2 id=&#34;getting-the-data&#34;&gt;Getting the data&lt;/h2&gt;

&lt;p&gt;Google Analytics recently provided more integration between the Search Console imports and the web session data, of which &lt;a href=&#34;http://online-behavior.com/analytics/search-console&#34;&gt;Daniel Waisberg has an excellent walk-through&lt;/a&gt;, which could mean you can skip the first part of this script.&lt;/p&gt;

&lt;p&gt;However, there are circumstances where the integration won&amp;rsquo;t work, such as when the URLs in Google Analytics are a different format to Search Console - with the below script you have control on how to link the URLs, by formatting them to look the same.&lt;/p&gt;

&lt;p&gt;Data is provided via my &lt;a href=&#34;https://cran.r-project.org/web/packages/searchConsoleR/index.html&#34;&gt;searchConsoleR&lt;/a&gt; and &lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/&#34;&gt;googleAnalyticsR&lt;/a&gt; R packages.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(searchConsoleR)
library(googleAnalyticsR)

## authentication with both GA and SC
options(googleAuthR.scopes.selected = 
  c(&amp;quot;https://www.googleapis.com/auth/webmasters&amp;quot;,
    &amp;quot;https://www.googleapis.com/auth/analytics&amp;quot;,
    &amp;quot;https://www.googleapis.com/auth/analytics.readonly&amp;quot;))
    
googleAuthR::gar_auth()

## replace with your GA ID
ga_id &amp;lt;- 1111111 

## date range to fetch
start &amp;lt;- as.character(Sys.Date() - 93)
end &amp;lt;- &amp;quot;2016-06-01&amp;quot;

## Using new GA v4 API
## GAv4 filters
google_seo &amp;lt;- 
  filter_clause_ga4(list(dim_filter(&amp;quot;medium&amp;quot;, 
                                     &amp;quot;EXACT&amp;quot;, 
                                     &amp;quot;organic&amp;quot;),
                          dim_filter(&amp;quot;source&amp;quot;, 
                                     &amp;quot;EXACT&amp;quot;, 
                                     &amp;quot;google&amp;quot;)),
                     operator = &amp;quot;AND&amp;quot;)

## Getting the GA data
gadata &amp;lt;-
  google_analytics_4(ga_id,
                     date_range = c(start,end),
                     metrics = c(&amp;quot;sessions&amp;quot;,
                                 &amp;quot;transactions&amp;quot;,
                                 &amp;quot;transactionRevenue&amp;quot;),
                     dimensions = c(&amp;quot;landingPagePath&amp;quot;),
                     dim_filters = google_seo,
                     order = order_type(&amp;quot;transactions&amp;quot;, 
                                        sort_order = &amp;quot;DESC&amp;quot;, 
                                        orderType = &amp;quot;VALUE&amp;quot;),
                     max = 20000)

## Getting the Search Console data
## The development version &amp;gt;0.2.0.9000 lets you get more than 5000 rows
scdata &amp;lt;- search_analytics(&amp;quot;http://www.example.co.uk&amp;quot;, 
                           startDate = start, endDate = end,
                           dimensions = c(&amp;quot;page&amp;quot;,&amp;quot;query&amp;quot;),
                           rowLimit = 20000)

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;transforming-the-data&#34;&gt;Transforming the data&lt;/h2&gt;

&lt;p&gt;First we change the Search Console URLs into the same format as Google Analytics.  In this example, the hostname is appended to the GA URLs already (a reason why the native support won&amp;rsquo;t work), but you may also need to append the hostname to the GA URLs via &lt;code&gt;paste0(&amp;quot;www.yourdomain.com&amp;quot;, gadata$page)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We also get the search page result (e.g. Page 1 = 1-10, Page 2 = 11-20) as it may be useful.&lt;/p&gt;

&lt;p&gt;For the split of revenue later, the last call calculates the % of clicks going to each URL.&lt;/p&gt;

&lt;h3 id=&#34;search-console-data-transformation&#34;&gt;Search Console data transformation&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)

## get urls in same format

## this will differ from website to website, 
## but in most cases you will need to add the domain to the GA urls:
## gadata$page &amp;lt;- paste0(&amp;quot;www.yourdomain.com&amp;quot;, gadata$landingPagePath)
## gadata has urls www.example.com/pagePath
## scdata has urls in http://www.example.com/pagePath
scdata$page2 &amp;lt;- gsub(&amp;quot;http://&amp;quot;,&amp;quot;&amp;quot;, scdata$page)

## get SERP
scdata$serp &amp;lt;- cut(scdata$position, 
                   breaks = seq(1, 100, 10), 
                   labels = as.character(1:9),
                   include.lowest = TRUE, 
                   ordered_result = TRUE)

## % of SEO traffic to each page per keyword
scdata &amp;lt;- scdata %&amp;gt;% 
  group_by(page2) %&amp;gt;% 
  mutate(clickP = clicks / sum(clicks)) %&amp;gt;%
  ungroup()

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;merging-with-google-analytics&#34;&gt;Merging with Google Analytics&lt;/h3&gt;

&lt;p&gt;Now we merge the data, and calculate the estimates of revenue, transactions and sessions.&lt;/p&gt;

&lt;p&gt;Why estimate sessions when we already have them?  This is how we assess how accurate this approach is - if the clicks is roughly the same as the estimated sessions, we can go further.&lt;/p&gt;

&lt;p&gt;The accuracy metric is assessed as the ratio between the estiamted sessions and the clicks, minus 1.  This will be 0 when 100% accuracy, and then the further away from 0 the figure is the less we trust the results.&lt;/p&gt;

&lt;p&gt;We also round the avergae position to the nearest whole number.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)

## join data on page
joined_data &amp;lt;- gadata %&amp;gt;% 
  left_join(scdata, by = c(landingPagePath = &amp;quot;page2&amp;quot;)) %&amp;gt;%
  mutate(transactionsEst = clickP*transactions,
         revenueEst = clickP*transactionRevenue,
         sessionEst = clickP*sessions,
         accuracyEst = ((sessionEst / clicks) - 1),
         positionRound = round(position))

## we only want clicks over 0, and get rid of a few columns.
tidy_data &amp;lt;- joined_data %&amp;gt;% 
  filter(clicks &amp;gt; 0) %&amp;gt;% 
  select(-page, -sessions, -transactions, -transactionRevenue) 

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;is-it-reliable&#34;&gt;Is it reliable?&lt;/h3&gt;

&lt;p&gt;A histogram of the accuracy estimate shows we consistently over estimate but the clicks and estimated sessions are within a magnitude:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://code.markedmondson.me/images/histogram1.png&#34; alt=&#34;keyword-sessio-click-estimate-histogram&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Most of the session estimates were intrestingly around 1.3 times more than the clicks.  This may be because Search Console clicks act more like Google SEO users, but any other ideas please say in comments!&lt;/p&gt;

&lt;p&gt;From above I discarded all rows with an accuracy &amp;gt; 10 as unreliable, although you may want to be stricter in your criteria.  All figures are to be taken with a pinch of salt with this many assumptions, but if the relative performance looked ok then I feel there is still enough to get some action from the data.&lt;/p&gt;

&lt;h2 id=&#34;creating-the-seo-forecasts&#34;&gt;Creating the SEO forecasts&lt;/h2&gt;

&lt;p&gt;We now use the data to create a click curve table, with estimates on the CTR for each position, and the confidence in those results.&lt;/p&gt;

&lt;p&gt;I first attempted some models on making predictions of click curves for a website, but didn&amp;rsquo;t find any general satisifactory regressions.&lt;/p&gt;

&lt;p&gt;The diagram below uses a weighted &lt;code&gt;loess&lt;/code&gt; within &lt;code&gt;ggplot2&lt;/code&gt; which is good to show trend but not for making predictions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://code.markedmondson.me/images/ctr_scatter.png&#34; alt=&#34;ctr-scatter-plot&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;a-click-curve-to-use&#34;&gt;A click curve to use&lt;/h3&gt;

&lt;p&gt;However, 99% of the time we will only be concerned with the top 10, so it wasn&amp;rsquo;t too taxing to calculate the click through rates per website based on the data they had:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)

click_curve &amp;lt;- tidy_data %&amp;gt;% 
  group_by(positionRound) %&amp;gt;% 
  summarise(CTRmean = sum(clicks) / sum(impressions),
            n = n(),
            click.sum = sum(clicks),
            impressions.sum = sum(impressions),
            sd = sd(ctr),
            E = poisson.test(click.sum)$conf.int[2] / poisson.test(impressions.sum)$conf.int[1],
            lower = CTRmean - E/2,
            upper = CTRmean + E/2) %&amp;gt;% ungroup()

## add % increase to position 1
## could also include other positions
click_curve &amp;lt;- click_curve %&amp;gt;% 
  mutate(CTR1 = CTRmean[1] / CTRmean,
         CTR1.upper = upper[1] / upper,
         CTR1.lower = lower[1] / lower)


&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://code.markedmondson.me/images/ctr-curve.png&#34; alt=&#34;ctr-curve-seo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;These CTR rates are then used to predict how much more traffic/revenue etc. a keyword could get if they moved up to position 1.&lt;/p&gt;

&lt;h3 id=&#34;how-valuable-is-a-keyword-if-position-1&#34;&gt;How valuable is a keyword if position 1?&lt;/h3&gt;

&lt;p&gt;Once happy with the click curve, we now apply it to the original data, and work out estimates on SEO revenue for each keyword if they were at position 1.&lt;/p&gt;

&lt;p&gt;I trust results more if they have had more than 10 clicks, and the accuracy estimate figure is within 10 of 0. I would play around with this limits a little yourself to get something you can work with.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)
## combine with data

predict_click &amp;lt;- tidy_data1 %&amp;gt;% 
  mutate(positionRound = round(position)) %&amp;gt;%
  left_join(click_curve, by=c(positionRound = &amp;quot;positionRound&amp;quot;)) %&amp;gt;%
  mutate(revenueEst1 = revenueEst * CTR1,
         transEst1 = transactionsEst * CTR1,
         clickEst1 = clicks * CTR1,
         sessionsEst1 = sessionEst * CTR1,
         revenueEst1.lower = revenueEst * CTR1.lower,
         revenueEst1.upper = revenueEst * CTR1.upper,
         revenueEst1.change = revenueEst1 - revenueEst)

estimates &amp;lt;- predict_click %&amp;gt;% 
  select(landingPagePath, query, clicks, impressions, 
         ctr, position, serp, revenueEst, revenueEst1, 
         revenueEst1.change, revenueEst1.lower, revenueEst1.upper, 
         accuracyEst) %&amp;gt;%
  arrange(desc(revenueEst1)) %&amp;gt;% 
  dplyr::filter(abs(accuracyEst) &amp;lt; 10, 
                revenueEst1.change &amp;gt; 0, 
                clicks &amp;gt; 10)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;estimates&lt;/code&gt; now in this example holds 226 rows sorted in order or how much revenue they are to make if position #1 in Google. This is from an original keyword list of 21437, which is at least a way to narrow down to important keywords.&lt;/p&gt;

&lt;h2 id=&#34;plotting-the-data&#34;&gt;Plotting the data&lt;/h2&gt;

&lt;p&gt;All that remains to to present the data: limiting the keywords to the top 30 lets you present like below.&lt;/p&gt;

&lt;p&gt;The bars show the range of the estimate, as you can see its quite wide but lets you be more realistic in your expectations.&lt;/p&gt;

&lt;p&gt;The number in the middle of the bar is the current position, with the revenue at the x axis and keyword on the y.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://code.markedmondson.me/images/see-potential.png&#34; alt=&#34;seo-forecast-estimate&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;ggplot2-code&#34;&gt;ggplot2 code&lt;/h3&gt;

&lt;p&gt;To create the plots in this post, please see the ggplot2 code below.  Feel free to modify for your own needs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggplot2)

## CTR per position
ctr_plot &amp;lt;- ggplot(tidy_data, aes(x = position, 
                                  y = ctr
                                  ))
ctr_plot &amp;lt;- ctr_plot + theme_minimal()
ctr_plot &amp;lt;- ctr_plot + coord_cartesian(xlim = c(1,30), 
                                       ylim = c(0, 0.5))
ctr_plot &amp;lt;- ctr_plot + geom_point(aes(alpha = log(clicks),
                                      color = serp, 
                                      size = clicks))
ctr_plot &amp;lt;- ctr_plot + geom_smooth(aes(weight = clicks), 
                                   size = 0.2)
ctr_plot + scale_y_continuous(labels = scales::percent)
ctr_plot

hh &amp;lt;- ggplot(click_curve, aes(positionRound, CTRmean)) 
hh &amp;lt;- hh + theme_minimal()
hh &amp;lt;- hh + geom_line(linetype = 2) + coord_cartesian(xlim = c(1, 30), 
                                                     ylim = c(0,0.5))
hh &amp;lt;- hh + geom_ribbon(aes(positionRound, ymin = lower, ymax = upper), 
                       alpha = 0.2, 
                       fill = &amp;quot;orange&amp;quot;)
hh &amp;lt;- hh + scale_y_continuous(labels = scales::percent)
hh &amp;lt;- hh + geom_point() 
hh &amp;lt;- hh + geom_label(aes(label = scales::percent(CTRmean)))
hh


est_plot &amp;lt;- ggplot(estimates[1:30,], 
                   aes(reorder(query, revenueEst1), 
                       revenueEst1, 
                       ymax = revenueEst1.upper, 
                       ymin =  revenueEst1.lower))
est_plot &amp;lt;- est_plot + theme_minimal() + coord_flip()

est_plot &amp;lt;- est_plot + geom_crossbar(aes(fill = cut(accuracyEst, 
                                                    3, 
                                                    labels = c(&amp;quot;Good&amp;quot;,
                                                               &amp;quot;Ok&amp;quot;,
                                                               &amp;quot;Poor&amp;quot;))
                                                               ), 
                                     alpha = 0.7, 
                                     show.legend = FALSE)
                                     
est_plot &amp;lt;- est_plot + scale_x_discrete(name = &amp;quot;Query&amp;quot;)
est_plot &amp;lt;- est_plot + scale_y_continuous(name = &amp;quot;Estimated SEO Revenue Increase for Google #1&amp;quot;, 
                                          labels = scales::dollar_format(prefix = &amp;quot;£&amp;quot;))
est_plot &amp;lt;- est_plot + geom_label(aes(label = round(position)), 
                                  hjust = &amp;quot;center&amp;quot;)
est_plot &amp;lt;- est_plot + ggtitle(&amp;quot;SEO Potential Revenue (Current position)&amp;quot;)
est_plot

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>