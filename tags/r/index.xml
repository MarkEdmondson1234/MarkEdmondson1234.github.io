<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Mark Edmondson</title>
    <link>http://code.markedmondson.me/tags/r/index.xml</link>
    <description>Recent content in R on Mark Edmondson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-GB</language>
    <copyright>Powered by [Hugo](//gohugo.io). Theme by [PPOffice](http://github.com/ppoffice).</copyright>
    <atom:link href="http://code.markedmondson.me/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Real-time forecasting dashboard with Google Tag Manager, Google Cloud and R Shiny - Part two</title>
      <link>http://code.markedmondson.me/real-time-GTM-google-cloud-r-shiny-2/</link>
      <pubDate>Sun, 22 Jan 2017 14:20:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/real-time-GTM-google-cloud-r-shiny-2/</guid>
      <description>

&lt;p&gt;In part two of this two part series we walk through the steps to stream data from a &lt;a href=&#34;https://www.google.com/analytics/tag-manager/&#34;&gt;Google Tag Manager&lt;/a&gt; (GTM) implementation into a &lt;a href=&#34;https://cloud.google.com/appengine/&#34;&gt;Google App Engine&lt;/a&gt; (GAE) web app, which then adds data to a &lt;a href=&#34;https://cloud.google.com/bigquery/&#34;&gt;BigQuery&lt;/a&gt; table via BigQuery&amp;rsquo;s data streaming capability.  In part two, we go into how to query that table in realtime from R, make a forecast using R, then visualise it in &lt;a href=&#34;https://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt; and the JavaScript visualisation library &lt;a href=&#34;http://jkunst.com/highcharter/&#34;&gt;Highcharts&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Read &lt;a href=&#34;http://code.markedmondson.me/real-time-GTM-google-cloud-r-shiny-1/&#34;&gt;part one here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The project combines several languages where their advantages lie: Python for its interaction with Google APIs and its quick start creating your own API on App Engine, SQL to query the BigQuery data itself, R for its forecasting libraries and the reactive Shiny framework, and JavaScript for the visualisation and data capture at the Google Tag Manager end.&lt;/p&gt;

&lt;h2 id=&#34;thanks&#34;&gt;Thanks&lt;/h2&gt;

&lt;p&gt;This project wouldn&amp;rsquo;t have been possible without the help of the excellent work gone beforehand by &lt;a href=&#34;https://www.analyticspros.com/blog/data-science/streaming-prebid-data-google-bigquery/&#34;&gt;Luke Cempre&amp;rsquo;s post on AnalyticsPros&lt;/a&gt; for streaming data from Google Tag Manager to BigQuery, and &lt;a href=&#34;http://jkunst.com/&#34;&gt;Joshua Kunst&lt;/a&gt; for his help with the Highcharts JavaScript.&lt;/p&gt;

&lt;h2 id=&#34;data-flows&#34;&gt;Data flows&lt;/h2&gt;

&lt;p&gt;There are two data flows in this project.  The first adds data to BigQuery:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;GTM collects data into its dataLayer from a web visit.&lt;/li&gt;
&lt;li&gt;A custom HTML tag in GTM collects the data you want to stream then calls an App Engine URL with its data payload.&lt;/li&gt;
&lt;li&gt;The app engine URL is sent to a queue to add the data to BigQuery.&lt;/li&gt;
&lt;li&gt;The data plus a timestamp is put into a BigQuery row.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then to read the data:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The Shiny app calls Big Query every X seconds.&lt;/li&gt;
&lt;li&gt;The data is aggregated&lt;/li&gt;
&lt;li&gt;A forecast is made with the updated data.&lt;/li&gt;
&lt;li&gt;The Highcharts visualisation reads the changing dataset, and updates the visualisation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This blog will cover the second. A demo &lt;a href=&#34;https://github.com/MarkEdmondson1234/realtimeShiny&#34;&gt;Github repo for the Shiny app is available here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;calling-data-your-options&#34;&gt;Calling data: your options&lt;/h2&gt;

&lt;p&gt;The Google App Engine app on Github includes functions to both read and write data from BigQuery.  You can either call the data via the app engine app, which in turn reads the data via the Python BigQuery library, or if you are using a platofrm that supports reading the data directly from BigQuery then you can use that directly.&lt;/p&gt;

&lt;p&gt;In most cases, you will be better off reading the data directly, as you will be cutting out the middle man.  In some cases the app engine will time out so if you are using it you should make sure your app can handle null results.  But it is useful to have, for those platforms that do not have Big query SDKs, such as visualisation BI tools.&lt;/p&gt;

&lt;h3 id=&#34;option-1-google-app-engine-reading-realtime-data-from-bigquery&#34;&gt;Option 1 - Google App Engine: Reading realtime data from BigQuery&lt;/h3&gt;

&lt;p&gt;The full code for reading and writing data is available at the &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream&#34;&gt;supporting Github repo here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The first blog in this series went through its data input, we now look at the data output.  In production this may be separated out into a different app, but for brevity its here in the same application.&lt;/p&gt;

&lt;p&gt;We first define some environmental variables in the &lt;code&gt;app.yaml&lt;/code&gt; setup file, with the dataset and table and a secret code word:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#[START env]
env_variables:
  DATASET_ID: tests
  TABLE_ID: realtime_markedmondsonme
  SECRET_SALT: change_this_to_something_unique
#[END env]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first function below then queries the BigQuery table we defined in the environmental variables, and turns it into JSON.  By default it will get the last row, or you can pass in the &lt;code&gt;limit&lt;/code&gt; argument to get more rows, or your own &lt;code&gt;q&lt;/code&gt; argument with custom SQL to query the table directly:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# queries and turns into JSON
def get_data(q, limit = 1):
	datasetId = os.environ[&#39;DATASET_ID&#39;]
	tableId   = os.environ[&#39;TABLE_ID&#39;]

	if len(q) &amp;gt; 0:
		query = q % (datasetId, tableId)
	else:
		query = &#39;SELECT * FROM %s.%s ORDER BY ts DESC LIMIT %s&#39; % (datasetId, tableId, limit)

	bqdata = sync_query(query)

	return json.dumps(bqdata)
	
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This next class is called when the &amp;ldquo;get data&amp;rdquo; URL is requested.  A lot of headers are set to ensure no browser caching is done which we don&amp;rsquo;t want since this is a realtime feed.&lt;/p&gt;

&lt;p&gt;For security, we also test via a &lt;code&gt;hash&lt;/code&gt; parameter to make sure its an authorised request, and decide how much data to return via the &lt;code&gt;limit&lt;/code&gt; parameter.&lt;/p&gt;

&lt;p&gt;Finally we call the function above and write that out to the URL response.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class QueryTable(webapp2.RequestHandler):

	def get(self):

    # no caching
		self.response.headers.add_header(&amp;quot;Access-Control-Allow-Origin&amp;quot;, &amp;quot;*&amp;quot;)
		self.response.headers.add_header(&amp;quot;Pragma&amp;quot;, &amp;quot;no-cache&amp;quot;)
		self.response.headers.add_header(&amp;quot;Cache-Control&amp;quot;, &amp;quot;no-cache, no-store, must-revalidate, pre-check=0, post-check=0&amp;quot;)
		self.response.headers.add_header(&amp;quot;Expires&amp;quot;, &amp;quot;Thu, 01 Dec 1994 16:00:00&amp;quot;)
		self.response.headers.add_header(&amp;quot;Content-Type&amp;quot;, &amp;quot;application/json&amp;quot;)

		q      = cgi.escape(self.request.get(&amp;quot;q&amp;quot;))
		myhash = cgi.escape(self.request.get(&amp;quot;hash&amp;quot;))
		limit  = cgi.escape(self.request.get(&amp;quot;limit&amp;quot;))

		salt = os.environ[&#39;SECRET_SALT&#39;]
		test = hashlib.sha224(q+salt).hexdigest()

		if(test != myhash):
			logging.debug(&#39;Expected hash: {}&#39;.format(test))
			logging.error(&amp;quot;Incorrect hash&amp;quot;)
			return

		if len(limit) == 0:
			limit = 1

		self.response.out.write(get_data(q, limit))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;full-app-engine-script&#34;&gt;Full App Engine Script&lt;/h3&gt;

&lt;p&gt;The full script uploaded is available in the Github repository here:  &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream/blob/master/main.py&#34;&gt;main.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;With this script you then need some configuration files for the app and upload it to your Google Project.  A guide on how to deploy this is and more is available from the &lt;a href=&#34;https://github.com/MarkEdmondson1234/ga-bq-stream/blob/master/README.md&#34;&gt;Github repository README&lt;/a&gt;, but once done the app will be available at &lt;code&gt;https://YOUR-PROJECT-ID.appspot.com&lt;/code&gt; and you will call the &lt;code&gt;/bq-streamer&lt;/code&gt; and &lt;code&gt;/bq-get&lt;/code&gt; URLs to send and get data.&lt;/p&gt;

&lt;h3 id=&#34;option-2-calling-bigquery-directly&#34;&gt;Option 2 - Calling BigQuery directly&lt;/h3&gt;

&lt;p&gt;This is the preferred method, as it will be on average 5-10 seconds quicker to get your results, and avoid timeouts.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m using Shiny as it fits into existing work, but you may prefer to use some of the other &lt;a href=&#34;https://cloud.google.com/bigquery/docs/reference/libraries&#34;&gt;BigQuery SDKs out there&lt;/a&gt;. I&amp;rsquo;ll be using &lt;a href=&#34;https://github.com/cloudyr/bigQueryR&#34;&gt;Github version of bigQueryR&lt;/a&gt; &lt;code&gt;&amp;gt; 0.2.0.9000&lt;/code&gt; as it has support for non-cached queries that are needed to see the tables update in realtime.&lt;/p&gt;

&lt;p&gt;A demo &lt;a href=&#34;https://github.com/MarkEdmondson1234/realtimeShiny&#34;&gt;Github repo for the Shiny app is available here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this case, the function in R that is in the Shiny &lt;code&gt;server.R&lt;/code&gt; is below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(bigQueryR)


do_bq &amp;lt;- function(limit){

  ## authenticate offline first, upload the .httr-oauth token
  bqr_auth()
  q &amp;lt;- sprintf(&amp;quot;SELECT * FROM [big-query-r:tests.realtime_markedmondsonme] ORDER BY ts DESC LIMIT %s&amp;quot;, 
               limit)
  
  bqr_query(projectId = &amp;quot;big-query-r&amp;quot;, 
            datasetId = &amp;quot;tests&amp;quot;, 
            query = q, 
            useQueryCache = FALSE)  
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;bqr_auth()&lt;/code&gt; if first run offline to generate the authentication token, which is then uploaded with the app.  Alternatively you can use the &lt;a href=&#34;http://code.markedmondson.me/googleAuthR/reference/gar_auth_service.html&#34;&gt;JSON service auth&lt;/a&gt; or if deploying on Google Compute Engine the automatic Google Cloud auth features of googleAuthR (&lt;code&gt;googleAuthR::gar_gce_auth()&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;The query itself selects all the columns, and orders by the timestamp we supplied in the input post. The function has a parameter so you can select how many rows to collect, which we use later.&lt;/p&gt;

&lt;p&gt;Note the use of &lt;code&gt;useQueryCache = FALSE&lt;/code&gt; to ensure you always get the freshest results.  If this wasn&amp;rsquo;t selected queries of the same type will return the first result they queried, which is no good for these purposes.&lt;/p&gt;

&lt;h2 id=&#34;reactivepoll-the-shiny-realtime-function&#34;&gt;reactivePoll - the Shiny realtime function&lt;/h2&gt;

&lt;p&gt;For realtime applications, &lt;code&gt;shiny::reactivePoll()&lt;/code&gt; is a function that periodically checks a datasource for changes.&lt;/p&gt;

&lt;p&gt;ow, what constitutes &amp;ldquo;realtime&amp;rdquo; is debatable here - for my applications I really only need an update every ~30 seconds.  Practically the Shiny output dims when updating with data, so for periods less than say 10 seconds it may not be the best approach for you - updating directly via JavaScript libraries may be better, and rely on say OpenCPU to provide the forecasting or another JS library.&lt;/p&gt;

&lt;p&gt;However, for my purposes I just need something better than the Google Analytics 4-hour lag in data (for GA360) and this suits well, particularly as you can apply a whole host of R data functions to the output.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;reactivePoll&lt;/code&gt; needs to be supplied with two functions: one to check if the data has changed, the other to make the complete fetch once a change is detected.  For these we just check if the timestamp of the last entry has changed, and if so, then fetch the last 1000 results to make the prediction:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## get 1000 rows of data
get_bq &amp;lt;- function(){

  message(&amp;quot;Getting new data...&amp;quot;)
  check &amp;lt;- do_bq(1000)
  
  rt &amp;lt;- as.data.frame(check, stringsAsFactors = FALSE)
  names(rt) &amp;lt;- c(&amp;quot;pageURL&amp;quot;,&amp;quot;Referrer&amp;quot;,&amp;quot;ts&amp;quot;)
  
  ## turn string into JS timestamp
  rt$timestamp &amp;lt;- as.POSIXct(as.numeric(as.character(rt$ts)), origin=&amp;quot;1970-01-01&amp;quot;)
  
  rt
}

## get 1 row of data, output its timestamp
check_bq &amp;lt;- function(){
  
  check &amp;lt;- do_bq(1)
  
  message(&amp;quot;Checking....check$ts: &amp;quot;, check$ts)
  check$ts
  
}

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is then called in the Shiny server function like so, in this case every 5 seconds:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;shinyServer(function(input, output, session) {
  
  ## checks every 5 seconds for changes
  realtime_data &amp;lt;- reactivePoll(5000, 
                                session, 
                                checkFunc = check_bq, 
                                valueFunc = get_bq)

  ### ... do stuff with realtime_data() ...

}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;transforming-data&#34;&gt;Transforming data&lt;/h2&gt;

&lt;p&gt;We then need to make the forecast, and put the data into the correct format it can be used in the chosen visualisation library, &lt;a href=&#34;http://jkunst.com/highcharter/&#34;&gt;highcharter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The next function takes the output of the &lt;code&gt;realtime_data()&lt;/code&gt; function, aggregates per hour (my lowly blog hasn&amp;rsquo;t enough data to make it worth doing per minute, but YNMV), turns the aggregation into time series objects suitable for the forecast and highcharts functions, then outputs a list.&lt;/p&gt;

&lt;p&gt;In this case I have chosen a very simple forecast function using all the defaults of &lt;code&gt;forecast::forecast()&lt;/code&gt; but this should be tweaked to your particular use cases, such as taking more account of seasonality and so forth.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;transform_rt &amp;lt;- function(rt){
  ## aggregate per hour
  rt_agg &amp;lt;- rt %&amp;gt;% 
    mutate(hour = format(timestamp, format = &amp;quot;%Y-%m-%d %H:00&amp;quot;)) %&amp;gt;% 
    count(hour)
  
  rt_agg$hour &amp;lt;- as.POSIXct(rt_agg$hour, origin=&amp;quot;1970-01-01&amp;quot;)
  
  # ## the number of hits per timestamp
  rt_xts &amp;lt;- xts::xts(rt_agg$n, frequency = 24, order.by = rt_agg$hour)
  rt_ts &amp;lt;- ts(rt_agg$n, frequency = 24)
  
  list(forecast = forecast::forecast(rt_ts, h = 12),
       xts = rt_xts)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;All that remains now is to apply this transformation to new data as it appears (e.g. for each new visit, the hourly aggregate for the last hour increases, and the forecast updates)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  plot_data &amp;lt;- reactive({
    
    req(realtime_data())
    rt &amp;lt;- realtime_data()
    message(&amp;quot;plot_data()&amp;quot;)
    ## aggregate
    transform_rt(rt)
    
  })
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;output-to-highcharts&#34;&gt;Output to Highcharts&lt;/h2&gt;

&lt;p&gt;The final output to Highcharts has been tweaked a bit to get the prediction intervals and so forth:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;  output$hc &amp;lt;- renderHighchart({
    
    req(plot_data())
    ## forcast values object
    fc &amp;lt;- plot_data()$forecast
    
    ## original data
    raw_data &amp;lt;- plot_data()$xts
    
    # plot last 48 hrs only, although forecast accounts for all data
    raw_data &amp;lt;- tail(raw_data, 48)
    raw_x_date &amp;lt;- as.numeric(index(raw_data)) * 1000
    
    ## start time in JS time
    forecast_x_start &amp;lt;- as.numeric(index(raw_data)[length(raw_data)])*1000
    ## each hour after that in seconds, 
    forecast_x_sequence &amp;lt;- seq(3600000, by = 3600000, length.out = 12)
    ## everything * 1000 to get to Javascript time
    forecast_times &amp;lt;- as.numeric(forecast_x_start + forecast_x_sequence)
    
    forecast_values &amp;lt;- as.numeric(fc$mean)
    
    hc &amp;lt;- highchart() %&amp;gt;%
      hc_chart(zoomType = &amp;quot;x&amp;quot;) %&amp;gt;%
      hc_xAxis(type = &amp;quot;datetime&amp;quot;) %&amp;gt;% 
      hc_add_series(type = &amp;quot;line&amp;quot;,
                    name = &amp;quot;data&amp;quot;,
                    data = list_parse2(data.frame(date = raw_x_date, 
                                                  value = raw_data))) %&amp;gt;%
      hc_add_series(type = &amp;quot;arearange&amp;quot;, 
                    name = &amp;quot;80%&amp;quot;,
                    fillOpacity = 0.3,
                    data = list_parse2(data.frame(date = forecast_times,
                                                  upper = as.numeric(fc$upper[,1]),
                                                  lower = as.numeric(fc$lower[,1])))) %&amp;gt;%
      hc_add_series(type = &amp;quot;arearange&amp;quot;, 
                    name = &amp;quot;95%&amp;quot;,
                    fillOpacity = 0.3,
                    data = list_parse2(data.frame(date = forecast_times,
                                                  upper = as.numeric(fc$upper[,2]),
                                                  lower = as.numeric(fc$lower[,2])))) %&amp;gt;% 
      hc_add_series(type = &amp;quot;line&amp;quot;,
                    name = &amp;quot;forecast&amp;quot;,
                    data = list_parse2(data.frame(date = forecast_times, 
                                                  value = forecast_values)))
    
    hc
    
  })
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can then be displayed now in a very simple &lt;code&gt;ui.R&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(shiny)
library(highcharter)

shinyUI(fluidPage(
  titlePanel(&amp;quot;Realtime Shiny Dashboard from BigQuery&amp;quot;),
  highchartOutput(&amp;quot;hc&amp;quot;)
  )
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;to be tweaked and put into a template as needed.&lt;/p&gt;

&lt;p&gt;The gif doesn&amp;rsquo;t quite do it justice, but you get the idea:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/realtime_forcast.gif&#34; alt=&#34;Update as visits registered to the blog&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;improvements&#34;&gt;Improvements&lt;/h2&gt;

&lt;p&gt;Ideally I&amp;rsquo;d like to avoid the Shiny grey-out when new data is fetched and the graph redraw - I fiddled a bit trying to get the JavaScript to take data from an R table and pull it in directly put that didn&amp;rsquo;t work out - I may update it if its figured out later.  However, as I said above for my applicaiton I needed an update only every 60 seconds so it wasn&amp;rsquo;t worth too much trouble over.  If say you needed (and who really &lt;em&gt;needs&lt;/em&gt; this?) a smooth update every 5 seconds say, the grey out would be too often to be useable.&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;The full app then can be tested easily as its realtime :-)&lt;/p&gt;

&lt;p&gt;As I visit my blog it sends data from Google Tag Manager to BigQuery; that tables is queried every 5 seconds from the Shiny app to see if any new visits have occured; if they have the full data set is downloaded; a new forecast is made and output to the Highcharts.&lt;/p&gt;

&lt;p&gt;Whatever your application, the biggest thing I got from trying this project was it was a lot easier than I expected, which I credit the BigQuery platform for, so give it a go and let me know how it goes for you. Improve on the base I have made here, and I&amp;rsquo;d be really interested in the applications beyond reporting you may use it for - real time traffic predictions that modify bids being one example.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Insights sorting by delta metrics in the Google Analytics API v4</title>
      <link>http://code.markedmondson.me/quicker-insight-sort-metric-delta/</link>
      <pubDate>Thu, 01 Dec 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/quicker-insight-sort-metric-delta/</guid>
      <description>

&lt;p&gt;As analysts, we are often called upon to see how website metrics have improved or declined over time.  This is easy enough when looking at trends, but if you are looking to break down over other dimensions, it can involve a lot of ETL to get to what you need.&lt;/p&gt;

&lt;p&gt;For instance, if you are looking at landing page performance of SEO traffic you can sort by the top performers, but not by the top &lt;em&gt;most improved&lt;/em&gt; performers.  To see that you need to first extract your metrics for one month, extract it again for the comparison month, join the datasets on the page dimension and then create and sort by a delta metric.  For large websites, you can be exporting millions of URLs just so you can see say the top 20 most improved.&lt;/p&gt;

&lt;p&gt;This comes from the fact the Google Analytics web UI and Data Studio don&amp;rsquo;t let you sort by the &lt;em&gt;change&lt;/em&gt; of a metric.  However, this is available in the Google Analytics API v4 so a small demo on how to it and how it can be useful is shown here.&lt;/p&gt;

&lt;h2 id=&#34;extracting-the-data&#34;&gt;Extracting the data&lt;/h2&gt;

&lt;p&gt;In v4, you can pass in two date ranges in one call.  When you do this a new ordering type comes available, the &lt;a href=&#34;https://developers.google.com/analytics/devguides/reporting/core/v4/basics#delta_ordering&#34;&gt;&lt;code&gt;DELTA&lt;/code&gt;&lt;/a&gt; which is what we can use to sort the results.&lt;/p&gt;

&lt;p&gt;Bear in mind any metric filters you add will apply to the first date range, not the second.&lt;/p&gt;

&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;

&lt;p&gt;The below is implemented in R using &lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/&#34;&gt;&lt;code&gt;googleAnalyticsR&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We first load the library, authenticate and set our ViewID:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleAnalyticsR)
ga_auth()

al &amp;lt;- google_analytics_account_list()

gaid &amp;lt;- yourViewID
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These are some helper functions to get the start and end dates of last month, and the same month the year before:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Start of the month
#&#39; @param x A date
som &amp;lt;- function(x) {
  as.Date(format(x, &amp;quot;%Y-%m-01&amp;quot;))
}

#&#39; End of the month
#&#39; @param x A date
eom &amp;lt;- function(x) {
  som(som(x) + 35) - 1
}

#&#39; Start and end of month
get_start_end_month &amp;lt;- function(x = Sys.Date()){
  c(som(som(x) - 1), som(x) - 1)
}

last_month &amp;lt;- get_start_end_month()
year_before &amp;lt;- get_start_end_month(Sys.Date() - 365)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We now create an SEO filter as we only want to examine SEO traffic, and a transactions over 0 metric filter:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## only organic traffic
seo_filter &amp;lt;- filter_clause_ga4(list(dim_filter(&amp;quot;medium&amp;quot;, 
                                                 &amp;quot;EXACT&amp;quot;, 
                                                 &amp;quot;organic&amp;quot;)
                               ))
                               
## met filters are on the first date
transaction0 &amp;lt;- filter_clause_ga4(list(met_filter(&amp;quot;transactions&amp;quot;, 
                                                  &amp;quot;GREATER_THAN&amp;quot;, 
                                                  0)))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the sorting parameter, that we specify to be by the biggest change in transactions from last year at the top of the results:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## order by the delta change of year_before - last_month
delta_trans &amp;lt;- order_type(&amp;quot;transactions&amp;quot;,&amp;quot;DESCENDING&amp;quot;, &amp;quot;DELTA&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We now make the Google Analytics API v4 call:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gadata &amp;lt;- google_analytics_4(gaid,
                             date_range = c(year_before, last_month),
                             metrics = c(&amp;quot;visits&amp;quot;,&amp;quot;transactions&amp;quot;,&amp;quot;transactionRevenue&amp;quot;),
                             dimensions = c(&amp;quot;landingPagePath&amp;quot;),
                             dim_filters = seo_filter,
                             met_filters = transaction0,
                             order = delta_trans,
                             max = 20)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should now have the top 20 most declined landing pages from last year measured by e-commerce transactions.  Much easier than downloading all pages and doing the delta calculations yourself.&lt;/p&gt;

&lt;p&gt;If you want to get the absolute number of declined transactions, you can add the column via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gadata$transactions.delta &amp;lt;- gadata$transactions.d2 - gadata$transactions.d1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;With this data you can now focus on making SEO improvements to those pages so they can reclaim their past glory, at the very least its a good starting point for investigations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Launch RStudio Server in the Google Cloud with two lines of R</title>
      <link>http://code.markedmondson.me/launch-rstudio-server-google-cloud-in-two-lines-r/</link>
      <pubDate>Thu, 20 Oct 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/launch-rstudio-server-google-cloud-in-two-lines-r/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;ve written previously about how to get &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download-server/&#34;&gt;RStudio Server&lt;/a&gt; running on Google Compute Engine: the &lt;a href=&#34;http://markedmondson.me/run-r-rstudio-and-opencpu-on-google-compute-engine-free-vm-image&#34;&gt;first in July 2014&lt;/a&gt; gave you a snapshot to download then customise, the second in &lt;a href=&#34;http://code.markedmondson.me/setting-up-scheduled-R-scripts-for-an-analytics-team/&#34;&gt;April 2016&lt;/a&gt; launched via a Docker container.&lt;/p&gt;

&lt;p&gt;Things move on, and I now recommend using the process below that uses the RStudio template in the new on CRAN &lt;a href=&#34;https://cran.r-project.org/web/packages/googleComputeEngineR/&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt;&lt;/a&gt; package.  Not only does it abstract away a lot of the dev-ops set up, but it also gives you more flexibility by taking advantage of &lt;code&gt;Dockerfiles&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;launching-an-rstudio-server&#34;&gt;Launching an Rstudio Server&lt;/h2&gt;

&lt;p&gt;This example is taken from the &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/example-workflows.html#custom-team-rstudio-server&#34;&gt;example workflows&lt;/a&gt; that are on the &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt; website&lt;/a&gt;, which includes other examples for Shiny, OpenCPU and R-clusters.&lt;/p&gt;

&lt;p&gt;You do need to do a bit of &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/articles/installation-and-authentication.html&#34;&gt;initial setup&lt;/a&gt; to setup your Google project and download the authentication file, but after that you just need to issue these two commands:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleComputeEngineR)
vm &amp;lt;- gce_vm(template = &amp;quot;rstudio&amp;quot;,
             name = &amp;quot;my-rstudio&amp;quot;,
             username = &amp;quot;mark&amp;quot;, password = &amp;quot;mark1234&amp;quot;,
             predefined_type = &amp;quot;n1-highmem-2&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And thats it.  Wait a bit, it will output an IP address for you to log in with.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://code.markedmondson.me/images/rstudio-launch-example.png&#34; alt=&#34;rstudio-googleComputeEngineR-launch&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://code.markedmondson.me/images/rstudio-login.png&#34; alt=&#34;rstudio login&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can now carry on by logging in and installing packages as you would on RStudio Desktop, then use &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/gce_vm_stop.html&#34;&gt;&lt;code&gt;gce_vm_stop(vm)&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/gce_vm_start.html&#34;&gt;&lt;code&gt;gce_vm_start(vm)&lt;/code&gt;&lt;/a&gt; to stop and start your instance, or if say you are on a Chromebook and cannot run R locally, use the Google Cloud Web UI to start and stop it.&lt;/p&gt;

&lt;h2 id=&#34;further-customisation&#34;&gt;Further customisation&lt;/h2&gt;

&lt;p&gt;You customise further by creating a custom image that launches a fresh RStudio Server instance with your own packages and files installed.  This takes advantage of some Google Cloud benefits such as the &lt;a href=&#34;https://cloud.google.com/container-registry/&#34;&gt;Container Registry&lt;/a&gt; which lets you save private Docker containers.&lt;/p&gt;

&lt;p&gt;With that, you can save your custom RStudio server to its own custom image, that can be used to launch anew in another instance as needed:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;## push your rstudio image to container registry
gce_push_registry(vm, &amp;quot;my-rstudio&amp;quot;, container_name = &amp;quot;my-rstudio&amp;quot;)

## launch another rstudio instance with your settings
vm2 &amp;lt;- gce_vm(template = &amp;quot;rstudio&amp;quot;,
              name = &amp;quot;my-rstudio-2&amp;quot;,
              username = &amp;quot;mark&amp;quot;, password = &amp;quot;mark1234&amp;quot;,
              predefined_type = &amp;quot;n1-highmem-2&amp;quot;,
              dynamic_image = gce_tag_container(&amp;quot;my-rstudio&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you want to go further still, use &lt;a href=&#34;https://docs.docker.com/engine/reference/builder/&#34;&gt;&lt;code&gt;Dockerfiles&lt;/code&gt;&lt;/a&gt; to customise the underlying linux libraries and CRAN/github packages to install in a more replicable manner - a good way to keep track in Github exactly how your server is configured.&lt;/p&gt;

&lt;p&gt;A &lt;code&gt;Dockerfile&lt;/code&gt; example is shown below - construct this locally:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM rocker/hadleyverse
MAINTAINER Mark Edmondson (r@sunholo.com)

# install cron and R package dependencies
RUN apt-get update &amp;amp;&amp;amp; apt-get install -y \
    cron \
    nano \
    ## clean up
    &amp;amp;&amp;amp; apt-get clean \ 
    &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/ \ 
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds
    
## Install packages from CRAN
RUN install2.r --error \ 
    -r &#39;http://cran.rstudio.com&#39; \
    googleAuthR shinyFiles googleCloudStorage bigQueryR gmailR googleAnalyticsR \
    ## install Github packages
    &amp;amp;&amp;amp; Rscript -e &amp;quot;devtools::install_github(c(&#39;bnosac/cronR&#39;))&amp;quot; \
    ## clean up
    &amp;amp;&amp;amp; rm -rf /tmp/downloaded_packages/ /tmp/*.rds \
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then build it on your VM via &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/reference/docker_build.html&#34;&gt;&lt;code&gt;docker_build&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;docker_build(vm, 
             dockerfile = &amp;quot;file/location/dockerfile&amp;quot;, 
             new_image = &amp;quot;my-custom-image&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can then save this up to the Container Registry and launch as before:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;gce_push_registry(vm, &amp;quot;my-custom-image&amp;quot;, image_name = &amp;quot;my-custom-image&amp;quot;
vm3 &amp;lt;- gce_vm(template = &amp;quot;rstudio&amp;quot;,
              name = &amp;quot;my-rstudio-3&amp;quot;,
              username = &amp;quot;mark&amp;quot;, password = &amp;quot;mark1234&amp;quot;,
              predefined_type = &amp;quot;n1-highmem-2&amp;quot;,
              dynamic_image = gce_tag_container(&amp;quot;my-custom-image&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A digital analytics workflow through the Google Cloud using R</title>
      <link>http://code.markedmondson.me/digital-analytics-workflow-through-google-cloud/</link>
      <pubDate>Mon, 10 Oct 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/digital-analytics-workflow-through-google-cloud/</guid>
      <description>

&lt;p&gt;There are now several packages built upon the &lt;code&gt;googleAuthR&lt;/code&gt; framework which are helpful to a digital analyst who uses R, so this post looks to demonstrate how they all work together.  If you&amp;rsquo;re new to R, and would like to know how it helps with your digital analytics, &lt;a href=&#34;http://analyticsdemystified.com/blog/tim-wilson/&#34;&gt;Tim Wilson&lt;/a&gt; and I ran a workshop last month aimed at getting a digital analyst up and running.  The course material is online at &lt;a href=&#34;http://www.dartistics.com/&#34;&gt;www.dartistics.com&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;diagram&#34;&gt;Diagram&lt;/h2&gt;

&lt;p&gt;A top level overview is shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/r-infrastructure.png&#34; alt=&#34;R-infrastructure-digital-analytics&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The diagram shows &lt;a href=&#34;http://code.markedmondson.me/googleAuthR/&#34;&gt;&lt;code&gt;googleAuthR&lt;/code&gt;&lt;/a&gt; packages, other packages, servers and APIs all of which interact to turn data into actionable analytics.&lt;/p&gt;

&lt;p&gt;The most recent addition is &lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt;&lt;/a&gt; which has helped make great savings in the time it takes to set up servers.  I have in the past blogged about &lt;a href=&#34;http://code.markedmondson.me/setting-up-scheduled-R-scripts-for-an-analytics-team/&#34;&gt;setting up R servers in the Google Cloud&lt;/a&gt;, but it was still more dev-ops than I really wanted to be doing.  Now, I can do setups similar to those I have written about in a couple of lines of R.&lt;/p&gt;

&lt;h2 id=&#34;data-workflow&#34;&gt;Data workflow&lt;/h2&gt;

&lt;p&gt;A suggested workflow for working with data is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Infrastructure&lt;/em&gt; - Creating a computer to run your analysis.  This can be either your own laptop, or a server in the cloud.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Collection&lt;/em&gt; - Downloading your raw data from one or multiple sources, such as APIs or your CRM database.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Transformation&lt;/em&gt; - Turning your raw data into useful information via ETL, modelling or statistics.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Storage&lt;/em&gt; - Storing the information you have created so that it can be automatically updated and used.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Output&lt;/em&gt; - Displaying or using the information into an end user application.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The components of the diagram can be combined into the workflow above.  You can swap out various bits for your own needs, but its possible to use R for all of these steps.&lt;/p&gt;

&lt;p&gt;You could do all of this with an Excel workbook on your laptop.  However, as data analysis becomes more complicated, it makes sense to start breaking out the components into more specialised tools, since Excel will start to strain when you increase the data volume or want reproducibility.&lt;/p&gt;

&lt;h3 id=&#34;infrastructure&#34;&gt;Infrastructure&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://cloudyr.github.io/googleComputeEngineR/&#34;&gt;&lt;code&gt;googleComputeEngineR&lt;/code&gt;&lt;/a&gt; uses the Google Clouds&amp;rsquo; virtual machine instances to create servers, with specific support for R.&lt;/p&gt;

&lt;p&gt;It uses &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker containers&lt;/a&gt; to make launching the servers and applications you need quick and simple, without you needing to know too much dev-ops to get started.&lt;/p&gt;

&lt;p&gt;Due to Docker, the applications created can be more easily transferred into other IT environments, such as within internal client intranets or AWS.&lt;/p&gt;

&lt;p&gt;To help with a digital analytics workflow, &lt;code&gt;googleComputeEngineR&lt;/code&gt; includes templates for the below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RStudio Server&lt;/strong&gt; - an R IDE you can work with from your browser. The server edition means you can access it from anywhere, and can always ensure the correct packages are installed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shiny&lt;/strong&gt; - A server to run your Shiny apps that display your data in dashboards or applications end users can work with.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OpenCPU&lt;/strong&gt; - A server that turns your R code into a JSON API.  Used for turning R output into a format web development teams can use straight within a website.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For instance, launching an RStudio server is as simple as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleComputeEngineR)
vm &amp;lt;- gce_vm_template(&amp;quot;rstudio&amp;quot;, 
                      name = &amp;quot;rstudio-server&amp;quot;, 
                      predefined_type = &amp;quot;f1-micro&amp;quot;, 
                      username = &amp;quot;mark&amp;quot;, 
                      password = &amp;quot;mark1234&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The instance will launch within a few minutes and give you a URL you can then login with.&lt;/p&gt;

&lt;h3 id=&#34;collection&#34;&gt;Collection&lt;/h3&gt;

&lt;p&gt;Once you are logged in to your RStudio Server, you can use all of R&amp;rsquo;s power to download and work with your data.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;googleAuthR&lt;/code&gt; packages can all be authenticated under the same OAuth2 token, to simplify access.&lt;/p&gt;

&lt;p&gt;Other packages useful to digital analytics include APIs such as &lt;a href=&#34;https://github.com/randyzwitch/RSiteCatalyst&#34;&gt;&lt;code&gt;RSiteCatalyst&lt;/code&gt;&lt;/a&gt; and &lt;code&gt;twitteR&lt;/code&gt;.  A full list of digital analytics R packages can be found in the &lt;a href=&#34;https://cran.r-project.org/web/views/WebTechnologies.html&#34;&gt;web technologies section of CRAN Task Views&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another option is the R package &lt;a href=&#34;https://github.com/jennybc/googlesheets&#34;&gt;&lt;code&gt;googlesheets&lt;/code&gt;&lt;/a&gt; by Jenny Bryan, which could either be used as a data source or as a data storage for reports, to be processed onwards later.&lt;/p&gt;

&lt;p&gt;The below example R script fetches data from Google Analytics, SEO data from Google Search Console and CRM data from BigQuery.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleAnalyticsR)
library(searchConsoleR)
library(bigQueryR)
library(googleAuthR)

## authenticate with all services 
gar_auth_service(&amp;quot;auth.json&amp;quot;)

## get search console data
seo_data &amp;lt;- search_analytics(&amp;quot;http://example.com&amp;quot;, 
                             &amp;quot;2015-07-01&amp;quot;, &amp;quot;2015-07-31&amp;quot;, 
                              c(&amp;quot;date&amp;quot;, &amp;quot;query&amp;quot;, &amp;quot;page&amp;quot;))

## get google analytics data
ga_data &amp;lt;- google_analytics_4(1235678,
                              c(&amp;quot;2015-07-01&amp;quot;, &amp;quot;2015-07-31&amp;quot;),
                              metrics = c(&amp;quot;users&amp;quot;),
                              dimensions = c(&amp;quot;date&amp;quot;, &amp;quot;userID&amp;quot; , &amp;quot;landingPagePath&amp;quot;, &amp;quot;campaign&amp;quot;))

## get CRM data from BigQuery
crm_data &amp;lt;- bqr_query(&amp;quot;big-query-database&amp;quot;, &amp;quot;my_data&amp;quot;,
                      &amp;quot;SELECT userID, lifetimeValue FROM [my_dataset]&amp;quot;)
                              
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;transformation&#34;&gt;Transformation&lt;/h3&gt;

&lt;p&gt;This ground is well covered by existing R packages.  My suggestion here is to embrace the &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyverse/index.html&#34;&gt;&lt;code&gt;tidyverse&lt;/code&gt;&lt;/a&gt; and use that to create and generate your information.&lt;/p&gt;

&lt;p&gt;Applications include anomaly detection, measurement of causal effect, clustering and forecasting. Hadley Wickham&amp;rsquo;s book &lt;a href=&#34;http://r4ds.had.co.nz/index.html&#34;&gt;&amp;ldquo;R for Data Science&amp;rdquo;&lt;/a&gt; is a recent compendium of knowledge on this topic, which includes this suggested work flow:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/tidy-r.png&#34; alt=&#34;tidyr&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;storage&#34;&gt;Storage&lt;/h3&gt;

&lt;p&gt;Once you have your data in the format you want, you often need to keep it somewhere it is easily accessible for other systems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/storage/&#34;&gt;Google Cloud Storage&lt;/a&gt; is a cheap, reliable method of storing any type of data object so that its always available for further use, and is heavily used within Google Cloud applications as a central data store.  I use it for storing &lt;code&gt;RData&lt;/code&gt; files or storing &lt;code&gt;csv&lt;/code&gt; files with a public link that is emailed to users when available.  It is accessible in R via the &lt;a href=&#34;http://code.markedmondson.me/googleCloudStorageR/&#34;&gt;&lt;code&gt;googleCloudStorageR&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;

&lt;p&gt;For database style access, &lt;a href=&#34;https://cloud.google.com/bigquery/&#34;&gt;BigQuery&lt;/a&gt; can be queried from many data services, including data visualisation platforms such as Google&amp;rsquo;s Data Studio or Tableau.  BigQuery offers incredibly fast analytical queries for TBs of data, accessible via the &lt;a href=&#34;http://code.markedmondson.me/bigQueryR/&#34;&gt;&lt;code&gt;bigQueryR&lt;/code&gt;&lt;/a&gt; package.&lt;/p&gt;

&lt;p&gt;An example of uploading data is below - again only one authentication is needed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleCloudStorageR)
library(bigQueryR)

## authenticate with all services 
gar_auth_service(&amp;quot;auth.json&amp;quot;)

## upload to Big Query
bqr_upload_data(&amp;quot;projectID&amp;quot;, &amp;quot;datasetID&amp;quot;, &amp;quot;tableID&amp;quot;,
                my_data_for_sql_queries)

## upload to Google Cloud Storage
gcs_upload(&amp;quot;my_rdata_file&amp;quot;)


&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;output&#34;&gt;Output&lt;/h3&gt;

&lt;p&gt;Finally, outputs include &lt;a href=&#34;http://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt; apps, &lt;a href=&#34;http://rmarkdown.rstudio.com/&#34;&gt;RMarkdown&lt;/a&gt;, a &lt;a href=&#34;https://gist.github.com/MarkEdmondson1234/ddcac436cbdfd4557639522573bfc7b6&#34;&gt;scheduled email&lt;/a&gt; or an R API call using &lt;a href=&#34;https://www.opencpu.org/&#34;&gt;OpenCPU&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All &lt;code&gt;googleAuthR&lt;/code&gt; functions are Shiny and RMarkdown compatible for user authentication - this means a user can login themselves and access their own data whilst using the logic of your app to gain insight, without you needing access to their data at all.  An example of an RMarkdown app taking advantage of this is the demo of the &lt;a href=&#34;https://github.com/MarkEdmondson1234/gentelellaShiny&#34;&gt;GentelellaShiny GA dashboard&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/gentellaShinyGA.png&#34; alt=&#34;gentella-shiny&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can launch OpenCPU and Shiny servers just as easily as RStudio Server via &lt;code&gt;googleComputeEngineR&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleComputeEngineR)

## creates a Shiny server
vm2 &amp;lt;- gce_vm_template(&amp;quot;shiny&amp;quot;, 
                      name = &amp;quot;shiny-server&amp;quot;, 
                      predefined_type = &amp;quot;f1-micro&amp;quot;, 
                      username = &amp;quot;mark&amp;quot;, 
                      password = &amp;quot;mark1234&amp;quot;)

## creates an OpenCPU server                      
vm3 &amp;lt;- gce_vm_template(&amp;quot;opencpu&amp;quot;, 
                      name = &amp;quot;opencpu-server&amp;quot;, 
                      predefined_type = &amp;quot;f1-micro&amp;quot;, 
                      username = &amp;quot;mark&amp;quot;, 
                      password = &amp;quot;mark1234&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Shiny Apps or RMarkdown HTML files can then be uploaded for display to the end user.  If the server needs more power, simply save the container and relaunch with a bigger RAM or CPU.&lt;/p&gt;

&lt;p&gt;OpenCPU is the technology demonstrated in my recent EARL London talk on using R to &lt;a href=&#34;http://code.markedmondson.me/predictClickOpenCPU/supercharge.html&#34;&gt;forecast HTML prefetching and deploying through Google Tag Manager&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Your Shiny, RMarkdown or OpenCPU functions can download data via:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(googleCloudStorageR)
library(bigQueryR)

## authenticate with all services 
gar_auth_service(&amp;quot;auth.json&amp;quot;)

## download Google Cloud Storage
my_data &amp;lt;- gce_get_object(&amp;quot;my_rdata_file&amp;quot;)

## query data from Big Query dependent on user input
query &amp;lt;- bqr_query(&amp;quot;big-query-database&amp;quot;, &amp;quot;my_data&amp;quot;,
                   &amp;quot;SELECT userID, lifetimeValue FROM [my_dataset]&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Hopefully this has shown where some efficiencies could be made in your own digital analysis. For me, the reduction of computer servers to atoms of work has expanded the horizons on what is possible: applications such as sending big calculations to the cloud if taking too long locally; being able to send clients entire clusters of computers with a data application ready and working; and having customised R environments for every occasion, such as R workshops.&lt;/p&gt;

&lt;p&gt;For the future, I hope to introduce Spark clusters via &lt;a href=&#34;https://cloud.google.com/dataproc/&#34;&gt;Google Dataproc&lt;/a&gt;, giving the ability to use machine learning directly on a dataset without needing to download locally; scheduled scripts that launch servers as needed; and working with Google&amp;rsquo;s newly launched &lt;a href=&#34;https://cloud.google.com/ml/&#34;&gt;machine learning APIs&lt;/a&gt; that dovetail into the Google Cloud.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Efficient anti-sampling with the Google Analytics Reporting API</title>
      <link>http://code.markedmondson.me/anti-sampling-google-analytics-api/</link>
      <pubDate>Fri, 05 Aug 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/anti-sampling-google-analytics-api/</guid>
      <description>

&lt;p&gt;Avoiding sampling is one of the most common reasons people start using the Google Analytics API.  This blog lays out some pseudo-code to do so in an efficient manner, avoiding too many unnecessary API calls.  The approach is used in the v4 calls for the R package &lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/v4.html&#34;&gt;&lt;code&gt;googleAnalyticsR&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;avoiding-the-daily-walk&#34;&gt;Avoiding the daily walk&lt;/h2&gt;

&lt;p&gt;The most common approach to mitigate sampling is to break down the API calls into one call per day.  This has some problems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Its inefficient.&lt;/strong&gt;  If you have 80% sampling or 10% sampling, you use the same number of API calls.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It takes a long time.&lt;/strong&gt;  A year long fetch is 365 calls of 5+ seconds that can equate to a 30mins+ wait.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;It doesn’t always work.&lt;/strong&gt; If you have so many sessions its sampled for one day, you will still have sampling, albeit at a lower rate.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;anti-sampling-based-on-session-size&#34;&gt;Anti-sampling based on session size&lt;/h2&gt;

&lt;p&gt;Google Analytics sampling works as &lt;a href=&#34;https://support.google.com/analytics/answer/2637192&#34;&gt;outlined in this Google article&lt;/a&gt;.  The main points are that if your API call covers a date range greater than set session limits, it will return a sampled call.&lt;/p&gt;

&lt;p&gt;The session limits vary according to if you are using Google Analytics 360 and other unknown factors in a sampling algorithm.  Fortunately, this information is available in the API responses via the &lt;code&gt;samplesReadCounts&lt;/code&gt; and &lt;code&gt;samplingSpaceSizes&lt;/code&gt; meta data.  See the &lt;a href=&#34;https://developers.google.com/analytics/devguides/reporting/core/v4/rest/v4/reports/batchGet#ReportData&#34;&gt;v4 API reference&lt;/a&gt; for their definitions.&lt;/p&gt;

&lt;p&gt;These values change per API call, so the general strategy is to make two exploratory API calls first to get the sampling information and the number of sessions over the desired date period, then use that information to construct batches of calls over date ranges that are small enough to avoid sampling, but large enough to not waste API calls.&lt;/p&gt;

&lt;p&gt;The two exploratory API calls to find the meta data are more than made up for once you have saved calls in the actual data fetch.&lt;/p&gt;

&lt;h2 id=&#34;how-it-works-in-practice-80-quicker-data&#34;&gt;How it works in practice - 80%+ quicker data&lt;/h2&gt;

&lt;p&gt;Following this approach, I have found a huge improvement in time spent for sampled calls, making it much more useable in say dynamic dashboards where waiting 30 mins for data is not an option.&lt;/p&gt;

&lt;p&gt;An example response from the &lt;code&gt;googleAnalyticsR&lt;/code&gt; library is shown below - for a month&amp;rsquo;s worth of unsampled data  that would have taken 30 API calls via a daily walk, I get the same in 5 (2 to find batch sizes, 3 to get the data).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;&amp;gt; library(googleAnalyticsR)
&amp;gt; ga_auth()
&amp;gt; ga_data &amp;lt;- 
    google_analytics_4(id, 
                       date_range = c(&amp;quot;2016-01-01&amp;quot;,
                                      &amp;quot;2016-02-01&amp;quot;), 
                       metrics = c(&amp;quot;sessions&amp;quot;,
                                   &amp;quot;bounceRate&amp;quot;), 
                       dimensions = c(&amp;quot;date&amp;quot;,
                                      &amp;quot;landingPagePath&amp;quot;,
                                      &amp;quot;source&amp;quot;), 
                       anti_sample = TRUE)
                                
anti_sample set to TRUE. Mitigating sampling via multiple API calls.
Finding how much sampling in data request...
Data is sampled, based on 54.06% of visits.
Downloaded [10] rows from a total of [76796].
Finding number of sessions for anti-sample calculations...
Downloaded [32] rows from a total of [32].
Calculated [3] batches are needed to download [113316] rows unsampled.
Anti-sample call covering 10 days: 2016-01-01, 2016-01-10
Downloaded [38354] rows from a total of [38354].
Anti-sample call covering 20 days: 2016-01-11, 2016-01-30
Downloaded [68475] rows from a total of [68475].
Anti-sample call covering 2 days: 2016-01-31, 2016-02-01
Downloaded [6487] rows from a total of [6487].
Finished unsampled data request, total rows [113316]
Successfully avoided sampling
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The time saved gets even greater the longer the time period you request.&lt;/p&gt;

&lt;h2 id=&#34;limitations&#34;&gt;Limitations&lt;/h2&gt;

&lt;p&gt;As with daily walk anti-sample techniques, user metrics and unique users are linked to the date range you are querying, so this technique will not match the numbers as if you queried over the whole date range.&lt;/p&gt;

&lt;p&gt;The sampling session limit is also applied at a web property level, not View for non-GA360 accounts, so its best to use this on a Raw data View, as filters will cause the session calculations be incorrect.&lt;/p&gt;

&lt;h2 id=&#34;example-pseudo-code&#34;&gt;Example pseudo-code&lt;/h2&gt;

&lt;p&gt;R code that implements the above is &lt;a href=&#34;https://github.com/MarkEdmondson1234/googleAnalyticsR/blob/master/R/anti_sample.R&#34;&gt;available here&lt;/a&gt;, but the pseudo-code below is intended for you to port to different languages:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Get the unsampled data
test_call = get_ga_api(full_date_range, 
                       metrics = your_metrics, 
                       dimensions = your_dimensions)

// # Read the sample meta data

// read_counts is the number of sessions before sampling starts
// I make it 10% smaller to ensure its small enough as
// it seems a bit flakey
read_counts = meta_data(test_call, &amp;quot;sampledReadCounts&amp;quot;)
read_counts = read_counts * 0.9

// space_size is total amount of sessions sampling was used for
space_size = meta_data(test_call, &amp;quot;samplingSpaceSizes&amp;quot;)

// dividing by each gives the % of sampling in the API call
samplingPercent = read_counts / space_size

// if there is no sample meta data, its not sampled. We&#39;re done!
if(read_counts = NULL or space_size = NULL):
  return(test_call)
  
// ..otherwise its sampled
// # get info for batch size calculations

// I found rowCount returned from a sampled call was not equal 
// to an unsampled call, so I add 20% to rowCount to adjust
rowCount = meta_data(test_call, &amp;quot;rowCount&amp;quot;)
rowCount = rowCount * 1.2

// get the number of sessions per day
date_sessions = get_ga_api(full_date_range, 
                           metric = &amp;quot;sessions&amp;quot;, 
                           dimensions = &amp;quot;date&amp;quot;)

// get the cumulative number of sessions over the year
date_sessions.cumulative = cumsum(date_sessions.sessions)

// modulus divide the cumulative sessions by the 
// sample read_counts.
date_sessions.sample_bucket = date_sessions.cumulative %% 
                                            read_counts

// get the new date ranges per sample_bucket group
new_date_ranges = get_min_and_max_date(date_sessions)

// new_date_ranges should now hold the smaller date ranges 
// for each batched API call

// # call GA api with new date ranges

total = empty_matrix

for i in new_date_ranges:

  if(new_date_ranges[i].start == new_date_ranges[i].end):
    // only one day so split calls into hourly
    // see below for do_hourly() explanation
    batch_call = do_hourly(new_date_range.start, 
                           metrics = your_metrics, 
                           dimensions = your_dimensions)
  else:
    // multi-day batching
    batch_call = get_ga_api(new_date_ranges[i], 
                            metrics = your_metrics, 
                            dimensions = your_dimensions)
                            
  total = total + batch_call
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;per-hour-fetching-do-hourly&#34;&gt;Per hour fetching do_hourly()&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;do_hourly()&lt;/code&gt; function is very similar to the above code for daily, but with a fetch to examine the session distribution per hour.  I only call it when necessary since it is a lot more API calls and its an edge case.&lt;/p&gt;

&lt;p&gt;If you need anti-sampling for sub-hourly, then you should really be looking at using the &lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/big-query.html&#34;&gt;BigQuery Google Analytics 360 exports&lt;/a&gt;. :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SEO keyword research using searchConsoleR and googleAnalyticsR</title>
      <link>http://code.markedmondson.me/search-console-google-analytics-r-keyword-research/</link>
      <pubDate>Tue, 21 Jun 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/search-console-google-analytics-r-keyword-research/</guid>
      <description>

&lt;p&gt;In this blog we look at a method to estimate where to prioritise your SEO resources, estimating which keywords will give the greatest increase in revenue if you could improve their Google rank.&lt;/p&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Thanks to &lt;a href=&#34;https://data-seo.com/&#34;&gt;Vincent at data-seo.com&lt;/a&gt; who proof read and corrected some errors in the first draft&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Data comes from &lt;a href=&#34;https://www.google.com/webmasters/tools/home&#34;&gt;Google Search Console&lt;/a&gt; and &lt;a href=&#34;https://www.google.com/webmasters/tools/home?hl=en&#34;&gt;Google Analytics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Search Console is used to provide the keywords in these days post &lt;a href=&#34;http://www.notprovidedcount.com/&#34;&gt;(not provided)&lt;/a&gt;.  We then link the two data sets by using the URLs as a key, and estimate how much each keyword made in revenue by splitting them in the same proportion as the traffic they have sent to the page.&lt;/p&gt;

&lt;p&gt;This approach assumes each keyword converts at the same rate once on the page, and will work better with some more websites more than others - the best results I have seen are those websites with a lot of content on seperate URLs, such that they capture long-tail queries.  This is because the amount of keywords per URL is small, but with enough volume to make the estimates trustworthy.&lt;/p&gt;

&lt;p&gt;We also try to incorporate magins of error in the results.  This avoids situations where one click on position 40 gets a massive weighting in potential revenue, which in reality could have been a freak event.&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;The method produced a targeted keyword list of 226 from an original seed list of ~21,000.  The top 30 revenue targets are shown in the plot below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://code.markedmondson.me/images/see-potential.png&#34; alt=&#34;seo-forecast-estimate&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Read below on how the plot was generated and what the figures mean.&lt;/p&gt;

&lt;h2 id=&#34;getting-the-data&#34;&gt;Getting the data&lt;/h2&gt;

&lt;p&gt;Google Analytics recently provided more integration between the Search Console imports and the web session data, of which &lt;a href=&#34;http://online-behavior.com/analytics/search-console&#34;&gt;Daniel Waisberg has an excellent walk-through&lt;/a&gt;, which could mean you can skip the first part of this script.&lt;/p&gt;

&lt;p&gt;However, there are circumstances where the integration won&amp;rsquo;t work, such as when the URLs in Google Analytics are a different format to Search Console - with the below script you have control on how to link the URLs, by formatting them to look the same.&lt;/p&gt;

&lt;p&gt;Data is provided via my &lt;a href=&#34;https://cran.r-project.org/web/packages/searchConsoleR/index.html&#34;&gt;searchConsoleR&lt;/a&gt; and &lt;a href=&#34;http://code.markedmondson.me/googleAnalyticsR/&#34;&gt;googleAnalyticsR&lt;/a&gt; R packages.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(searchConsoleR)
library(googleAnalyticsR)

## authentication with both GA and SC
options(googleAuthR.scopes.selected = 
  c(&amp;quot;https://www.googleapis.com/auth/webmasters&amp;quot;,
    &amp;quot;https://www.googleapis.com/auth/analytics&amp;quot;,
    &amp;quot;https://www.googleapis.com/auth/analytics.readonly&amp;quot;))
    
googleAuthR::gar_auth()

## replace with your GA ID
ga_id &amp;lt;- 1111111 

## date range to fetch
start &amp;lt;- as.character(Sys.Date() - 93)
end &amp;lt;- &amp;quot;2016-06-01&amp;quot;

## Using new GA v4 API
## GAv4 filters
google_seo &amp;lt;- 
  filter_clause_ga4(list(dim_filter(&amp;quot;medium&amp;quot;, 
                                     &amp;quot;EXACT&amp;quot;, 
                                     &amp;quot;organic&amp;quot;),
                          dim_filter(&amp;quot;source&amp;quot;, 
                                     &amp;quot;EXACT&amp;quot;, 
                                     &amp;quot;google&amp;quot;)),
                     operator = &amp;quot;AND&amp;quot;)

## Getting the GA data
gadata &amp;lt;-
  google_analytics_4(ga_id,
                     date_range = c(start,end),
                     metrics = c(&amp;quot;sessions&amp;quot;,
                                 &amp;quot;transactions&amp;quot;,
                                 &amp;quot;transactionRevenue&amp;quot;),
                     dimensions = c(&amp;quot;landingPagePath&amp;quot;),
                     dim_filters = google_seo,
                     order = order_type(&amp;quot;transactions&amp;quot;, 
                                        sort_order = &amp;quot;DESC&amp;quot;, 
                                        orderType = &amp;quot;VALUE&amp;quot;),
                     max = 20000)

## Getting the Search Console data
## The development version &amp;gt;0.2.0.9000 lets you get more than 5000 rows
scdata &amp;lt;- search_analytics(&amp;quot;http://www.example.co.uk&amp;quot;, 
                           startDate = start, endDate = end,
                           dimensions = c(&amp;quot;page&amp;quot;,&amp;quot;query&amp;quot;),
                           rowLimit = 20000)

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;transforming-the-data&#34;&gt;Transforming the data&lt;/h2&gt;

&lt;p&gt;First we change the Search Console URLs into the same format as Google Analytics.  In this example, the hostname is appended to the GA URLs already (a reason why the native support won&amp;rsquo;t work), but you may also need to append the hostname to the GA URLs via &lt;code&gt;paste0(&amp;quot;www.yourdomain.com&amp;quot;, gadata$page)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We also get the search page result (e.g. Page 1 = 1-10, Page 2 = 11-20) as it may be useful.&lt;/p&gt;

&lt;p&gt;For the split of revenue later, the last call calculates the % of clicks going to each URL.&lt;/p&gt;

&lt;h3 id=&#34;search-console-data-transformation&#34;&gt;Search Console data transformation&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)

## get urls in same format

## this will differ from website to website, 
## but in most cases you will need to add the domain to the GA urls:
## gadata$page &amp;lt;- paste0(&amp;quot;www.yourdomain.com&amp;quot;, gadata$landingPagePath)
## gadata has urls www.example.com/pagePath
## scdata has urls in http://www.example.com/pagePath
scdata$page2 &amp;lt;- gsub(&amp;quot;http://&amp;quot;,&amp;quot;&amp;quot;, scdata$page)

## get SERP
scdata$serp &amp;lt;- cut(scdata$position, 
                   breaks = seq(1, 100, 10), 
                   labels = as.character(1:9),
                   include.lowest = TRUE, 
                   ordered_result = TRUE)

## % of SEO traffic to each page per keyword
scdata &amp;lt;- scdata %&amp;gt;% 
  group_by(page2) %&amp;gt;% 
  mutate(clickP = clicks / sum(clicks)) %&amp;gt;%
  ungroup()

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;merging-with-google-analytics&#34;&gt;Merging with Google Analytics&lt;/h3&gt;

&lt;p&gt;Now we merge the data, and calculate the estimates of revenue, transactions and sessions.&lt;/p&gt;

&lt;p&gt;Why estimate sessions when we already have them?  This is how we assess how accurate this approach is - if the clicks is roughly the same as the estimated sessions, we can go further.&lt;/p&gt;

&lt;p&gt;The accuracy metric is assessed as the ratio between the estiamted sessions and the clicks, minus 1.  This will be 0 when 100% accuracy, and then the further away from 0 the figure is the less we trust the results.&lt;/p&gt;

&lt;p&gt;We also round the avergae position to the nearest whole number.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)

## join data on page
joined_data &amp;lt;- gadata %&amp;gt;% 
  left_join(scdata, by = c(landingPagePath = &amp;quot;page2&amp;quot;)) %&amp;gt;%
  mutate(transactionsEst = clickP*transactions,
         revenueEst = clickP*transactionRevenue,
         sessionEst = clickP*sessions,
         accuracyEst = ((sessionEst / clicks) - 1),
         positionRound = round(position))

## we only want clicks over 0, and get rid of a few columns.
tidy_data &amp;lt;- joined_data %&amp;gt;% 
  filter(clicks &amp;gt; 0) %&amp;gt;% 
  select(-page, -sessions, -transactions, -transactionRevenue) 

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;is-it-reliable&#34;&gt;Is it reliable?&lt;/h3&gt;

&lt;p&gt;A histogram of the accuracy estimate shows we consistently over estimate but the clicks and estimated sessions are within a magnitude:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://code.markedmondson.me/images/histogram1.png&#34; alt=&#34;keyword-sessio-click-estimate-histogram&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Most of the session estimates were intrestingly around 1.3 times more than the clicks.  This may be because Search Console clicks act more like Google SEO users, but any other ideas please say in comments!&lt;/p&gt;

&lt;p&gt;From above I discarded all rows with an accuracy &amp;gt; 10 as unreliable, although you may want to be stricter in your criteria.  All figures are to be taken with a pinch of salt with this many assumptions, but if the relative performance looked ok then I feel there is still enough to get some action from the data.&lt;/p&gt;

&lt;h2 id=&#34;creating-the-seo-forecasts&#34;&gt;Creating the SEO forecasts&lt;/h2&gt;

&lt;p&gt;We now use the data to create a click curve table, with estimates on the CTR for each position, and the confidence in those results.&lt;/p&gt;

&lt;p&gt;I first attempted some models on making predictions of click curves for a website, but didn&amp;rsquo;t find any general satisifactory regressions.&lt;/p&gt;

&lt;p&gt;The diagram below uses a weighted &lt;code&gt;loess&lt;/code&gt; within &lt;code&gt;ggplot2&lt;/code&gt; which is good to show trend but not for making predictions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://code.markedmondson.me/images/ctr_scatter.png&#34; alt=&#34;ctr-scatter-plot&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;a-click-curve-to-use&#34;&gt;A click curve to use&lt;/h3&gt;

&lt;p&gt;However, 99% of the time we will only be concerned with the top 10, so it wasn&amp;rsquo;t too taxing to calculate the click through rates per website based on the data they had:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)

click_curve &amp;lt;- tidy_data %&amp;gt;% 
  group_by(positionRound) %&amp;gt;% 
  summarise(CTRmean = sum(clicks) / sum(impressions),
            n = n(),
            click.sum = sum(clicks),
            impressions.sum = sum(impressions),
            sd = sd(ctr),
            E = poisson.test(click.sum)$conf.int[2] / poisson.test(impressions.sum)$conf.int[1],
            lower = CTRmean - E/2,
            upper = CTRmean + E/2) %&amp;gt;% ungroup()

## add % increase to position 1
## could also include other positions
click_curve &amp;lt;- click_curve %&amp;gt;% 
  mutate(CTR1 = CTRmean[1] / CTRmean,
         CTR1.upper = upper[1] / upper,
         CTR1.lower = lower[1] / lower)


&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://code.markedmondson.me/images/ctr-curve.png&#34; alt=&#34;ctr-curve-seo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;These CTR rates are then used to predict how much more traffic/revenue etc. a keyword could get if they moved up to position 1.&lt;/p&gt;

&lt;h3 id=&#34;how-valuable-is-a-keyword-if-position-1&#34;&gt;How valuable is a keyword if position 1?&lt;/h3&gt;

&lt;p&gt;Once happy with the click curve, we now apply it to the original data, and work out estimates on SEO revenue for each keyword if they were at position 1.&lt;/p&gt;

&lt;p&gt;I trust results more if they have had more than 10 clicks, and the accuracy estimate figure is within 10 of 0. I would play around with this limits a little yourself to get something you can work with.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)
## combine with data

predict_click &amp;lt;- tidy_data1 %&amp;gt;% 
  mutate(positionRound = round(position)) %&amp;gt;%
  left_join(click_curve, by=c(positionRound = &amp;quot;positionRound&amp;quot;)) %&amp;gt;%
  mutate(revenueEst1 = revenueEst * CTR1,
         transEst1 = transactionsEst * CTR1,
         clickEst1 = clicks * CTR1,
         sessionsEst1 = sessionEst * CTR1,
         revenueEst1.lower = revenueEst * CTR1.lower,
         revenueEst1.upper = revenueEst * CTR1.upper,
         revenueEst1.change = revenueEst1 - revenueEst)

estimates &amp;lt;- predict_click %&amp;gt;% 
  select(landingPagePath, query, clicks, impressions, 
         ctr, position, serp, revenueEst, revenueEst1, 
         revenueEst1.change, revenueEst1.lower, revenueEst1.upper, 
         accuracyEst) %&amp;gt;%
  arrange(desc(revenueEst1)) %&amp;gt;% 
  dplyr::filter(abs(accuracyEst) &amp;lt; 10, 
                revenueEst1.change &amp;gt; 0, 
                clicks &amp;gt; 10)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;estimates&lt;/code&gt; now in this example holds 226 rows sorted in order or how much revenue they are to make if position #1 in Google. This is from an original keyword list of 21437, which is at least a way to narrow down to important keywords.&lt;/p&gt;

&lt;h2 id=&#34;plotting-the-data&#34;&gt;Plotting the data&lt;/h2&gt;

&lt;p&gt;All that remains to to present the data: limiting the keywords to the top 30 lets you present like below.&lt;/p&gt;

&lt;p&gt;The bars show the range of the estimate, as you can see its quite wide but lets you be more realistic in your expectations.&lt;/p&gt;

&lt;p&gt;The number in the middle of the bar is the current position, with the revenue at the x axis and keyword on the y.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://code.markedmondson.me/images/see-potential.png&#34; alt=&#34;seo-forecast-estimate&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;ggplot2-code&#34;&gt;ggplot2 code&lt;/h3&gt;

&lt;p&gt;To create the plots in this post, please see the ggplot2 code below.  Feel free to modify for your own needs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(ggplot2)

## CTR per position
ctr_plot &amp;lt;- ggplot(tidy_data, aes(x = position, 
                                  y = ctr
                                  ))
ctr_plot &amp;lt;- ctr_plot + theme_minimal()
ctr_plot &amp;lt;- ctr_plot + coord_cartesian(xlim = c(1,30), 
                                       ylim = c(0, 0.5))
ctr_plot &amp;lt;- ctr_plot + geom_point(aes(alpha = log(clicks),
                                      color = serp, 
                                      size = clicks))
ctr_plot &amp;lt;- ctr_plot + geom_smooth(aes(weight = clicks), 
                                   size = 0.2)
ctr_plot + scale_y_continuous(labels = scales::percent)
ctr_plot

hh &amp;lt;- ggplot(click_curve, aes(positionRound, CTRmean)) 
hh &amp;lt;- hh + theme_minimal()
hh &amp;lt;- hh + geom_line(linetype = 2) + coord_cartesian(xlim = c(1, 30), 
                                                     ylim = c(0,0.5))
hh &amp;lt;- hh + geom_ribbon(aes(positionRound, ymin = lower, ymax = upper), 
                       alpha = 0.2, 
                       fill = &amp;quot;orange&amp;quot;)
hh &amp;lt;- hh + scale_y_continuous(labels = scales::percent)
hh &amp;lt;- hh + geom_point() 
hh &amp;lt;- hh + geom_label(aes(label = scales::percent(CTRmean)))
hh


est_plot &amp;lt;- ggplot(estimates[1:30,], 
                   aes(reorder(query, revenueEst1), 
                       revenueEst1, 
                       ymax = revenueEst1.upper, 
                       ymin =  revenueEst1.lower))
est_plot &amp;lt;- est_plot + theme_minimal() + coord_flip()

est_plot &amp;lt;- est_plot + geom_crossbar(aes(fill = cut(accuracyEst, 
                                                    3, 
                                                    labels = c(&amp;quot;Good&amp;quot;,
                                                               &amp;quot;Ok&amp;quot;,
                                                               &amp;quot;Poor&amp;quot;))
                                                               ), 
                                     alpha = 0.7, 
                                     show.legend = FALSE)
                                     
est_plot &amp;lt;- est_plot + scale_x_discrete(name = &amp;quot;Query&amp;quot;)
est_plot &amp;lt;- est_plot + scale_y_continuous(name = &amp;quot;Estimated SEO Revenue Increase for Google #1&amp;quot;, 
                                          labels = scales::dollar_format(prefix = &amp;quot;£&amp;quot;))
est_plot &amp;lt;- est_plot + geom_label(aes(label = round(position)), 
                                  hjust = &amp;quot;center&amp;quot;)
est_plot &amp;lt;- est_plot + ggtitle(&amp;quot;SEO Potential Revenue (Current position)&amp;quot;)
est_plot

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Scheduling R scripts for a team using RStudio Server, Docker, Github and Google Compute Engine</title>
      <link>http://code.markedmondson.me/setting-up-scheduled-R-scripts-for-an-analytics-team/</link>
      <pubDate>Thu, 21 Apr 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/setting-up-scheduled-R-scripts-for-an-analytics-team/</guid>
      <description>

&lt;p&gt;&lt;em&gt;edit 20th November, 2016 - now everything in this post is abstracted away and available in the googleComputeEngineR package - I would say its a lot easier to use that.  Here is a post on getting started with it. &lt;a href=&#34;http://code.markedmondson.me/launch-rstudio-server-google-cloud-in-two-lines-r/&#34;&gt;http://code.markedmondson.me/launch-rstudio-server-google-cloud-in-two-lines-r/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This blog will give you steps that allows you to run on Google Compute Engine a server that has these features:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RStudio Server instance with multiple login.&lt;/li&gt;
&lt;li&gt;Apache to host a welcome webpage.&lt;/li&gt;
&lt;li&gt;Restart the server to securely load private Github R packages / Docker images.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;A shared cron folder that runs every day at 0630 that user&amp;rsquo;s can put their own scripts into.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is inspired by conversations with &lt;a href=&#34;https://twitter.com/chipoglesby&#34;&gt;&lt;code&gt;@Chipoglesby&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/RichardFergie&#34;&gt;&lt;code&gt;@RichardFergie&lt;/code&gt;&lt;/a&gt; on twitter about this kind of setup, and was useful to me as a way to organise my thoughts on the subject ;).  If you have any suggestions on improvements please tweet to me at &lt;a href=&#34;https://twitter.com/HoloMarkeD&#34;&gt;&lt;code&gt;@HoloMarkeD&lt;/code&gt;&lt;/a&gt; or email via the blog.&lt;/p&gt;

&lt;h2 id=&#34;motivation-in-search-of-a-4-day-work-week&#34;&gt;Motivation: In search of a 4-day work week&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;ve just started a new job at &lt;a href=&#34;http://iihnordic.dk/&#34;&gt;IIH Nordic&lt;/a&gt; where a part of my duties is to look for ways to automate the boring tasks that come up in a digital web analytics team.&lt;/p&gt;

&lt;p&gt;IIH Nordic have an aim to introduce a 4-day work week within a couple of years, so the motivation is to find out how to save 20% of the current work schedule without harming productivity.  Downloading data and creating reports is a big time-sink that looks ripe for optimisation.&lt;/p&gt;

&lt;h3 id=&#34;introducing-the-wonders-of-r-to-a-team&#34;&gt;Introducing the wonders of R to a team&lt;/h3&gt;

&lt;p&gt;The current strategy is to introduce the team to R and train everyone up in running simple scripts that download the data they need for their reports.  We will tackle data transformation/ETL, statistics and visualisation/presentation later, but the first goal is to solve the &amp;ldquo;How can I get data&amp;rdquo; problem.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m well placed to help due to writing &lt;a href=&#34;https://github.com/MarkEdmondson1234/googleAuthR&#34;&gt;&lt;code&gt;googleAuthR&lt;/code&gt;&lt;/a&gt;, so have played with R libraries with simple authentication and functions to download from &lt;a href=&#34;https://github.com/MarkEdmondson1234/googleAnalyticsR_public&#34;&gt;&lt;code&gt;Google Analytics&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/MarkEdmondson1234/searchConsoleR&#34;&gt;&lt;code&gt;Search Console&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://github.com/MarkEdmondson1234/bigQueryR&#34;&gt;&lt;code&gt;BigQuery&lt;/code&gt;&lt;/a&gt;, as well as using Randy Zwitch&amp;rsquo;s &lt;a href=&#34;https://github.com/randyzwitch/RSiteCatalyst&#34;&gt;&lt;code&gt;Adobe Analytics&lt;/code&gt;&lt;/a&gt; package.  These libraries alone cover 90% of the data sources we need.&lt;/p&gt;

&lt;h3 id=&#34;why-use-rstudio-server-in-the-cloud&#34;&gt;Why use RStudio Server in the cloud?&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;ve had a few internal workshops now and the team look comfortable writing the R-scripts, but during the process I noticed a few issues that are solved using RStudio Server.  Using it means:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Everyone is working on the same versions and libraries.&lt;/li&gt;
&lt;li&gt;Scripts can be logged for quality and errors.&lt;/li&gt;
&lt;li&gt;Scheduled scripts can be run from the server not someones local PC.&lt;/li&gt;
&lt;li&gt;Training material can be made available on same server.&lt;/li&gt;
&lt;li&gt;Web accessible private login for remote working.&lt;/li&gt;
&lt;li&gt;Can use R from an iPad or Google Chromebook!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;setting-up-rstudio-server-on-google-cloud-compute-gce&#34;&gt;Setting up RStudio Server on Google Cloud Compute (GCE)&lt;/h2&gt;

&lt;p&gt;I have a old blog post on the &lt;a href=&#34;http://markedmondson.me/run-r-rstudio-and-opencpu-on-google-compute-engine-free-vm-image&#34;&gt;installation of RStudio Server on GCE&lt;/a&gt;, but this will update that since technology and my understanding has improved.&lt;/p&gt;

&lt;p&gt;A brief overview of the components are below:&lt;/p&gt;

&lt;h3 id=&#34;docker&#34;&gt;Docker&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt; is a virtual machine-lite that can run on anything, which means that the set-up is very transferable to other operating systems like Windows or OSX.&lt;/p&gt;

&lt;p&gt;It offers easier set-up via pre-created Docker images, and is embraced and &lt;a href=&#34;https://cloud.google.com/compute/docs/containers/container_vms&#34;&gt;well supported by Google&lt;/a&gt; via its &lt;a href=&#34;https://cloud.google.com/container-registry/docs/&#34;&gt;container registry&lt;/a&gt;, giving you unlimited private repositories, unlike Docker Hub that gives you only one.  It provides reproducibility and scale.&lt;/p&gt;

&lt;h3 id=&#34;github&#34;&gt;Github&lt;/h3&gt;

&lt;p&gt;Github is the go-to place to review and share code, and offers web-hooks that means it can push updates when you update a repository.  Every coder should use a version control system so its good practice to introduce it to the team, and it also allows installation of experimental and private R packages via &lt;code&gt;devtools::install_github&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;Google also has support for using Github via its &lt;a href=&#34;https://cloud.google.com/source-repositories/docs/setting-up-repositories&#34;&gt;Cloud Source Repositories&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;google-compute-engine&#34;&gt;Google Compute Engine&lt;/h3&gt;

&lt;p&gt;The up-and-coming pretender to AWS&amp;rsquo;s cloud computing crown, the Google Cloud is a natural choice for me as it offers integrations for Google Analytics and BigQuery.  The server images it offers have a great API and user interface, and it offers very quick I/O and restart times on Google&amp;rsquo;s world-class infrastructure, and the Docker container support as mentioned makes things more simple to scale in the future.&lt;/p&gt;

&lt;h2 id=&#34;setup-steps&#34;&gt;Setup steps&lt;/h2&gt;

&lt;p&gt;Now we get to the actual commands you use to get things up and running.  It will give you:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RStudio Server instance with multiple login.&lt;/li&gt;
&lt;li&gt;Apache to host a welcome webpage.&lt;/li&gt;
&lt;li&gt;Restart the server to load your private Github R packages if you have any.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;A shared cron folder that runs every day at 0430 that user&amp;rsquo;s can put their own scripts into.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In general I use the command line from a local terminal, but all actions can also be carried out within the &lt;a href=&#34;https://console.cloud.google.com/compute/instances&#34;&gt;Compute Engine web interface&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;create-the-google-cloud-engine&#34;&gt;Create the Google Cloud Engine&lt;/h3&gt;

&lt;p&gt;Here is a reference for &lt;a href=&#34;https://cloud.google.com/compute/docs/containers/vm-image/&#34;&gt;GCE docker enabled VMs&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download and install the &lt;a href=&#34;https://cloud.google.com/sdk/&#34;&gt;Google Cloud SDK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Authorize and set-up billing for a Google Project&lt;/li&gt;
&lt;li&gt;Start up your terminal and set your project and zone:&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;  gcloud auth login
  
  gcloud config set project your-project-name
  
  gcloud config set compute/region europe-west1
  
  gcloud config set compute/zone europe-west1-b
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Issue this &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/reference/compute/instances/create&#34;&gt;gcloud create command&lt;/a&gt; to start up a docker enabled instance:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;edit 18th September 2016 to update to new images as per &lt;a href=&#34;https://cloud.google.com/compute/docs/containers/vm-image/&#34;&gt;https://cloud.google.com/compute/docs/containers/vm-image/&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud compute instances create your-r-server-name \
  --image-family gci-stable \
  --image-project google-containers \
  --tags http-server
  --scopes https://www.googleapis.com/auth/devstorage.read_write \
  --machine-type n1-standard-1

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unlike previously, this new (from Aug 2016) container ready VM comes with Google Cloud Storage scopes already set, so you can use private docker repos.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;--tags&lt;/code&gt; flag sets the default http firewall rules to apply to this instance so we can reach it via the internet on port 80.&lt;/p&gt;

&lt;p&gt;You should now be able to see your instance has launched in the &lt;a href=&#34;https://console.cloud.google.com/compute/instances&#34;&gt;Google Compute Dashboard&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;create-persistent-disks&#34;&gt;Create persistent disks&lt;/h3&gt;

&lt;p&gt;Anything new in the Docker container will disappear on a restart if you also don&amp;rsquo;t commit and push the image, so to avoid that data and scripts are linked to the container via the &lt;code&gt;--volumes&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;We also link to persistent disk rather than the VM&amp;rsquo;s own, which has the advantage of being able to connect (read-only) to multiple servers at the same time, should you need it.  This also means if you need a more powerful server, you can safely create one knowing you will have the same data and scripts available.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud compute disks create --size=200GB my-data-disk
gcloud compute instances attach-disk your-r-server-name --disk my-data-disk
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;initial-configuration-on-google-compute-engine&#34;&gt;Initial Configuration on Google Compute Engine&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Login to GCE&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;  gcloud compute ssh your-r-server-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are now making commands from within the GCE VM.&lt;/p&gt;

&lt;p&gt;If you have any problems, use the web interface to login via the Cloud Shell.  Sometimes you had to add your username to the instance, and then login with that user like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;  gcloud compute ssh user@your-r-server-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See &lt;a href=&#34;https://cloud.google.com/compute/docs/troubleshooting#ssherrors&#34;&gt;here&lt;/a&gt; for more diagnostics help.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Format and mount the persistent disk&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;See instructions &lt;a href=&#34;https://cloud.google.com/compute/docs/disks/persistent-disks&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;```sh
sudo mkfs.ext4 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/disk/by-id/google-persistent-disk-1

##make mount point
sudo mkdir /mnt/data/
sudo mount -o discard,defaults /dev/disk/by-id/google-persistent-disk-1 /mnt/data/

## user folders
sudo mkdir /mnt/data/home/

## custom packages
sudo mkdir /mnt/data/R/

##permissions
sudo chmod a+w /mnt/data

## make it mount automatically each time
echo &#39;/dev/disk/by-id/google-persistent-disk-1 /mnt/data ext4 discard,defaults 1 1&#39; | sudo tee -a /etc/fstab
```
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Install and Configure Apache&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This gives you a user-friendly webpage pointing to the RStudio login, and I also use it as a place to put training material such as RMarkdown documents.&lt;/p&gt;

&lt;p&gt;I suppose this could also be done via another Docker container if you have a more complicated Apache setup to use.&lt;/p&gt;

&lt;p&gt;The ProxyPassMatch line is needed for the Shiny engine thats within RStudio Server to work.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo apt-get update
sudo apt-get -y install apache2

## Need proxy and proxy_http to create nice URLs, proxy_wstunnel for Shiny
sudo a2enmod proxy proxy_http proxy_wstunnel

## this may be 000-default.conf rather than 000-default 
## depending on version of apache
echo &#39;&amp;lt;VirtualHost *:80&amp;gt;
        ServerAdmin your@email.com

        DocumentRoot /var/www

        ProxyPassMatch ^/rstudio/p/([0-9]+)/(websocket|.*/websocket)/$ ws://localhost:8787/p/$1/$2/
        ProxyPass /rstudio/ http://localhost:8787/
        ProxyPassReverse /rstudio/ http://localhost:8787/
        RedirectMatch permanent ^/rstudio$ /rstudio/

        ErrorLog ${APACHE_LOG_DIR}/error.log

        LogLevel warn

        CustomLog ${APACHE_LOG_DIR}/access.log combined

&amp;lt;/VirtualHost&amp;gt;&#39; | sudo tee /etc/apache2/sites-enabled/000-default

sudo service apache2 restart

## A startup HTML page for you to customise
echo &#39;&amp;lt;!doctype html&amp;gt;&amp;lt;html&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;RStudio on Google Cloud Compute&amp;lt;/h1&amp;gt;&amp;lt;ul&amp;gt;&amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;./rstudio/&amp;quot;&amp;gt;RStudio Server&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;&#39; | sudo tee /var/www/index.html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should now be able to see your server running Apache.&lt;/p&gt;

&lt;p&gt;I then upload a website to &lt;code&gt;/var/www/&lt;/code&gt; via the &lt;a href=&#34;https://cloud.google.com/sdk/gcloud/reference/compute/copy-files&#34;&gt;&lt;code&gt;gcloud copy-files&lt;/code&gt; command&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;An example, assuming your website is in your local folder &lt;code&gt;~/dev/website/www/&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo gcloud --project your-project-name compute copy-files ~/dev/website/www/ your-r-server-name:/var/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is what we have at IIH Nordic:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/rstudio-server-webpage.png&#34; alt=&#34;batching_example&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;a-download-folder&#34;&gt;A download folder&lt;/h4&gt;

&lt;p&gt;Now, you may want to have a dropbox style folder for the data your scripts are running, say scheduled data downloads.  There are a few ways to skin this cat, such as uploading to cloud storage in your script, but the simplest way for me was to use the Apache functionality to create a logged in download area.&lt;/p&gt;

&lt;p&gt;For this, you need to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create a folder on your data disk where scripts will dump their data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir /mnt/data/downloads
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Create a symlink to an Apache web server folder&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ln -s /mnt/data/downloads /var/www/downloads
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[optional] Style the folder with CSS so it looks nice using say &lt;a href=&#34;http://adamwhitcroft.com/apaxy/&#34;&gt;Apaxy&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Update your Apache config to have logged in access to the folder.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The extra Apache config is below.  It requires installation of &lt;code&gt;sudo a2enmod headers&lt;/code&gt; and a restart.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;### In download folder, auto-download .txt,.csv, and .pdf files
### Protect with a password
&amp;lt;Directory /var/www/downloads/&amp;gt;
      &amp;lt;FilesMatch &amp;quot;.+\.(txt|csv|pdf)$&amp;quot;&amp;gt;
          ForceType application/octet-stream
          Header set Content-Disposition attachment
      &amp;lt;/FilesMatch&amp;gt;
      AuthType Basic
      AuthName &amp;quot;Enter password&amp;quot;
      AuthUserFile /etc/apache2/.htpasswd
      Require valid-user
      Order allow,deny
      Allow from all
      Options Indexes FollowSymLinks
      AllowOverride All
&amp;lt;/Directory&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;download-and-run-the-rstudio-server-docker-image&#34;&gt;Download and Run the RStudio Server Docker image&lt;/h3&gt;

&lt;p&gt;We first download a pre-prepared RStudio Docker container created by the &lt;a href=&#34;http://dirk.eddelbuettel.com/blog/2014/10/23/&#34;&gt;Rocker&lt;/a&gt; team led by Dirk Eddelbuettel. This is what our custom image will be based upon.  Here we download a variant that also loads &lt;a href=&#34;https://hub.docker.com/r/rocker/hadleyverse/&#34;&gt;RStudio and all of Hadley&amp;rsquo;s packages&lt;/a&gt; to give us a great base to work from.&lt;/p&gt;

&lt;p&gt;Most of the below is gleaned from the &lt;a href=&#34;https://github.com/rocker-org/rocker/wiki&#34;&gt;Rocker Wiki&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The below docker command runs the docker image - if that image is not available it will look for the image on Docker hub and download. This will take a long time as it downloads everything, perhaps time to put the kettle on.&lt;/p&gt;

&lt;p&gt;Subsequent times will load quickly from local version.&lt;/p&gt;

&lt;p&gt;We run it with a custom username and password we want, as this will be exposed to the web and we don&amp;rsquo;t want the defaults to be exposed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## Run the docker image with RStudio and Hadley Universe
sudo docker run --name rstudio-server -d -p 8787:8787 \
     -e USER=YOUR_USERNAME -e PASSWORD=YOUR_PW \
     -v /mnt/data/:/home/ \
     rocker/hadleyverse
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;configure-rstudio-server-docker-image&#34;&gt;Configure RStudio Server Docker image&lt;/h3&gt;

&lt;p&gt;Now it could be that you are done from here - you should have a working RStudio interface available on the IP of your container (&lt;a href=&#34;http://your-vm-ip-address/rstudio/auth-sign-in&#34;&gt;http://your-vm-ip-address/rstudio/auth-sign-in&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;But we will configure it a bit more, adding users, more packages, and scheduled jobs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you have a lot of configurations then it is better to create your own &lt;a href=&#34;https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/&#34;&gt;DOCKERFILE&lt;/a&gt; and build the image yourself.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We log in to the running docker container here:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo docker exec -it rstudio-server bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a good one to remember for when you are doing future customisation.&lt;/p&gt;

&lt;p&gt;You are now in the Docker container.&lt;/p&gt;

&lt;p&gt;Install stuff then CTRL-D to come out again to commit and push your changes.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## Make users that will create a directory on the data disk
adduser mark

## [Optional] Install packages
## install as sudo to ensure all user&#39;s have access
sudo su - -c &amp;quot;R -e \&amp;quot;install.packages(&#39;abc&#39;, repos=&#39;http://cran.rstudio.com/&#39;)\&amp;quot;&amp;quot;

## [Optional] Install libraries from Github packages
sudo su - -c &amp;quot;R -e \&amp;quot;devtools::install_github(&#39;MarkEdmondson1234/bigQueryR&#39;)\&amp;quot;&amp;quot;

## [Optional] Install libraries from private Github packages
sudo su - -c &amp;quot;R -e \&amp;quot;devtools::install_github(&#39;MarkEdmondson1234/privatePackge&#39;, auth_token=YOUR_GITHUB_PAT)\&amp;quot;&amp;quot;
````

### Configure scheduling via CRON

A big reason to have a server is for the team to schedule their data fetching scripts.  We achieve this by running CRON within the Docker container (to ensure all packages are installed) and then providing a link to a folder that runs the script when they need it.

First we install CRON in the container:

```sh
## Download and install cron
sudo apt-get update
sudo apt-get -y install cron nano
sudo service cron start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then schedule scripts via RScript to run daily.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## annoying nano bug in Docker
export TERM=xterm

## open up the cron editor (select 2)
sudo crontab -e

## add this to the bottom of file
## runs script at 0420 every day
# m h  dom mon dow   command
20 4 * * * /home/cron/r-cron.R &amp;gt;/home/cron/cronlog.log 2&amp;gt;&amp;amp;1

## CTRL-X and Y to save changes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;r-cron.R&lt;/code&gt; script needs to have &lt;code&gt;#!/usr/bin/Rscript&lt;/code&gt; at the top to run correctly.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#!/usr/bin/Rscript
cat(&amp;quot;Cron script started: &amp;quot;, date())

....do R stuff

cat(&amp;quot;Cron script stopped: &amp;quot;, date())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Repeat for all scripts you want to run.&lt;/p&gt;

&lt;h3 id=&#34;sending-emails&#34;&gt;Sending emails&lt;/h3&gt;

&lt;p&gt;Its useful to send an email once a script has run successfully (or not), one that uses Mailrun is below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;#&#39; Email a user a report is ready
#&#39;
#&#39; Requires an account at Mailgun: https://mailgun.com
#&#39; Pre-verification can only send to a whitelist of emails you configure
#&#39;
#&#39; @param email Email to send to
#&#39; @param mail_message Any extra info
#&#39;
#&#39; @return TRUE if successful email sent
#&#39; @import httr
#&#39; @export
sendEmail &amp;lt;- function(email = &amp;quot;XXXXX@you.com&amp;quot;,
                      mail_message = &amp;quot;Hello&amp;quot;){

  url &amp;lt;- &amp;quot;https://api.mailgun.net/v3/sandbox5f2XXXXXXXa.mailgun.org/messages&amp;quot;
  ## username:password so api_key is all after the api:
  api_key &amp;lt;- &amp;quot;key-c5957XXXXXXXXXXXbb9cf8ce&amp;quot;
  the_body &amp;lt;-
    list(
      from=&amp;quot;Mailgun Sandbox &amp;lt;postmaster@sandbox5XXXXXXXXa.mailgun.org&amp;gt;&amp;quot;,
      to=email,
      subject=&amp;quot;Mailgun from R&amp;quot;,
      text=mailmessage,
    )

  req &amp;lt;- httr::POST(url,
                    httr::authenticate(&amp;quot;api&amp;quot;, api_key),
                    encode = &amp;quot;form&amp;quot;,
                    body = the_body)

  httr::stop_for_status(req)
  
  TRUE

}

&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;scheduling-packages&#34;&gt;Scheduling packages&lt;/h3&gt;

&lt;p&gt;There is also an R package that manages cron, &lt;a href=&#34;https://github.com/bnosac/cronR&#34;&gt;cronR&lt;/a&gt; , which now has an RStudio Server addin, which looks like a good option.&lt;/p&gt;

&lt;h2 id=&#34;pushing-the-docker-changes&#34;&gt;Pushing the Docker changes&lt;/h2&gt;

&lt;p&gt;We now commit and push changes to the &lt;a href=&#34;https://cloud.google.com/container-registry/docs/&#34;&gt;Google Docker Hub&lt;/a&gt;.  The Docker command docs are &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/commit/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## CTRL-D to come out of the docker container again

## to get the container id e.g. c3f279d17e0a
sudo docker ps 

## commit with a message
sudo docker commit -a &amp;quot;Mark&amp;quot; -m &amp;quot;Added R stuff&amp;quot; \
    CONTAINER_ID yourname/your-new-docker-image

## list your new image with the old
sudo docker images

## tag the image with the location
sudo docker tag yourname/your-new-docker-image \
                gcr.io/your-project-id/your-new-docker-image

## push to Google Docker registry
sudo gcloud docker push \
     gcr.io/your-project-id/your-new-docker-image
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You could now pull this image using:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo gcloud docker pull \
  gcr.io/your-project-id/your-new-image-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip;but this will be taken care of in the startup behaviour below.&lt;/p&gt;

&lt;p&gt;Remember to commit any changes each time you change the configuration of RStudio.&lt;/p&gt;

&lt;h2 id=&#34;setup-restart-behaviour&#34;&gt;Setup restart behaviour&lt;/h2&gt;

&lt;p&gt;Now we want to configure the above to happen everytime the VM starts up.  I use a startup script for pulling the latest docker image and updating any packages or data from github so to refresh I just need to restart the server.&lt;/p&gt;

&lt;h3 id=&#34;download-latest-custom-packages-and-data&#34;&gt;Download latest custom packages and data&lt;/h3&gt;

&lt;p&gt;In the custom metadata for the VM, we need the field &lt;code&gt;startup-script&lt;/code&gt; and then optional other metadata.&lt;/p&gt;

&lt;p&gt;The metadata is kept seperate away from your running containers, but available via the Google metadata commands.  This can be used for things like passwords and security settings you would prefer not to be shipped in a Docker container, and is easier to manage - just edit the metadata keys.&lt;/p&gt;

&lt;p&gt;Create one metadata field per bash script variable - examples below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;github-user: your-githubname

github-repo: your-github-repo

github-data: your-github-data

github-pat: your-github-pat
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This startup script loads the metadata above and downloads custom R packages and data files from github.&lt;/p&gt;

&lt;p&gt;You can save this locally as &lt;code&gt;startup.sh&lt;/code&gt; and upload via &lt;code&gt;gcloud&lt;/code&gt; or paste it into the interface into a metadata field called &lt;code&gt;startup-script&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;#!/bin/bash
GH_USER=$(curl http://metadata/computeMetadata/v1/instance/attributes/github-user -H &amp;quot;Metadata-Flavor: Google&amp;quot;)
GH_REPO=$(curl http://metadata/computeMetadata/v1/instance/attributes/github-repo -H &amp;quot;Metadata-Flavor: Google&amp;quot;)
GH_DATA=$(curl http://metadata/computeMetadata/v1/instance/attributes/github-data -H &amp;quot;Metadata-Flavor: Google&amp;quot;)
GH_PAT=$(curl http://metadata/computeMetadata/v1/instance/attributes/github-pat -H &amp;quot;Metadata-Flavor: Google&amp;quot;)

## Run Docker image
docker run --name rstudio-server \
       -d -p 8787:8787 \
       -e USER=ADMIN_USERNAME \
       -e PASSWORD=ADMIN_PW \
       -v /mnt/data/:/home/ \
       gcr.io/your-project-name/your-image-name

## Data files: update from github
cd /mnt/data/user_name/project_name/data
git pull &#39;https://&#39;$GH_USER&#39;:&#39;$GH_PAT&#39;@github.com/&#39;$GH_USER&#39;/&#39;$GH_DATA&#39;.git&#39;

## Private packages 
# 1. pull from github
cd /mnt/data/R/privatePackageName
git pull &#39;https://&#39;$GH_USER&#39;:&#39;$GH_PAT&#39;@github.com/&#39;$GH_USER&#39;/&#39;$GH_REPO&#39;.git&#39;

# 2. Install the package from the local file you just updated from Git
sudo docker exec -it rstudio-server \
     sudo su - -c &amp;quot;R -e \&amp;quot;devtools::install(&#39;/home/R/localPackageName/&#39;)\&amp;quot;&amp;quot;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can debug your startup script by connecting to your instance and viewing &lt;code&gt;cat /var/log/startupscript.log&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You could also have a &lt;code&gt;shutdown-script&lt;/code&gt; that would execute before any shutdown.  Check out the &lt;a href=&#34;https://cloud.google.com/compute/docs/metadata&#34;&gt;list of metadata&lt;/a&gt; you can pass into scripts.&lt;/p&gt;

&lt;h2 id=&#34;launch&#34;&gt;Launch&lt;/h2&gt;

&lt;p&gt;Now we relaunch the newly configured VM from your local computer, to test the startup script works.  This will copy over all the configurations from the RStudio server above.&lt;/p&gt;

&lt;p&gt;Save the &lt;code&gt;startup.sh&lt;/code&gt; script to a file on your local machine, cd into the same folder and add the metadata:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gcloud compute instances add-metadata your-r-server-name \
  --metadata-from-file startup-script=startup.sh \
  --metadata github-user=YOUR-USER,github-repo=YOUR_REPO,github-data=YOUR_DATA,github-pat=YOUR_PAT
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now reset, cross fingers:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## reset
gcloud compute instances reset your-r-server-name
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After a couple of minutes everything should now be running as configured before.&lt;/p&gt;

&lt;p&gt;If you want to stop or start the server again, use the below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;## stop (no billing) but not deleted
gcloud compute instances stop your-r-server-name

## start up a stopped server
gcloud compute instances start your-r-server-name
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;the-future&#34;&gt;The future&lt;/h2&gt;

&lt;p&gt;This should put you in a great position to support R-scripts to the team, but also in a scalable way where starting up faster and bigger machines is just a case of updating configuration files.&lt;/p&gt;

&lt;p&gt;I would like to start up containers using the container manifest syntax but couldn&amp;rsquo;t get it to work for me yet, but for just one VM it means a few less lines in the start up script.&lt;/p&gt;

&lt;p&gt;We also have &lt;a href=&#34;http://iihnordic.dk/blog/posts/2016/marts/creating-a-content-recommendation-engine-using-r-opencpu-and-gtm&#34;&gt;OpenCPU&lt;/a&gt; and &lt;a href=&#34;https://www.rstudio.com/products/shiny/shiny-server/&#34;&gt;Shiny Server&lt;/a&gt; in the Google project, as we move into providing data output such as visualisations, APIs and dashboards.  They are setup in a similar fashion, just swap out the Docker image for the appropriate version you need.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>googleAuthR 0.2.0</title>
      <link>http://code.markedmondson.me/googleAuthR-0.2.0/</link>
      <pubDate>Fri, 05 Feb 2016 23:03:57 +0100</pubDate>
      
      <guid>http://code.markedmondson.me/googleAuthR-0.2.0/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/googleAuthR&#34;&gt;googleAuthR&lt;/a&gt; is now on CRAN version 0.2.0.&lt;/p&gt;

&lt;p&gt;This release is the result of using the library myself to create three working Google API libraries, and tweaking the googleAuthR code to better support the process.  As a result all of these libraries are now able to be authorised with one Google OAuth2 login flow:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/googleAnalyticsR_public&#34;&gt;googleAnalyticsR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/searchConsoleR&#34;&gt;searchConsoleR&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/bigQueryR&#34;&gt;bigQueryR&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;batching&#34;&gt;Batching&lt;/h2&gt;

&lt;p&gt;This means the libraries above and any other created with &lt;code&gt;googleAuthR&lt;/code&gt; can take advatage of batching: this uses a Google API feature that means you can send multiple API calls at once.  As the time for big fetches is usually in the wait for API responses, this is a huge time saver for big data fetches.&lt;/p&gt;

&lt;p&gt;For example, it is now implemented within &lt;a href=&#34;https://github.com/MarkEdmondson1234/googleAnalyticsR_public&#34;&gt;googleAnalyticsR&lt;/a&gt; when walking through data per day to mitigate sampling.  Testing on a two year walk through landing page data, it sped up from ~20 mins to ~2 minutes(!)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;../images/batch_example.png&#34; alt=&#34;batching_example&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;service-accounts&#34;&gt;Service Accounts&lt;/h2&gt;

&lt;p&gt;There is also now support for &lt;a href=&#34;https://developers.google.com/identity/protocols/OAuth2ServiceAccount&#34;&gt;service accounts&lt;/a&gt;, meaning no OAuth2 flow is needed: a user can just upload or use a JSON file they download from their own Google API console.  As some APIs like Big Query don&amp;rsquo;t have read only scope yet, this also gives greater security options for apps, as a user can give limited access to a app via projects.&lt;/p&gt;

&lt;h2 id=&#34;plans-for-the-future&#34;&gt;Plans for the future&lt;/h2&gt;

&lt;p&gt;I hope the library is of use, it is certainly making my workflow a lot quicker.  I love hearing what people are doing with it.  Next on my planned list is a &lt;code&gt;GoogleAuthenticateR&lt;/code&gt; library, that works with pure user authentication of Shiny apps via the G+ API.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m also working with offline authentication, meaning apps can work in the background by saving refresh tokens to a database.  Combined with my new &lt;a href=&#34;https://github.com/MarkEdmondson1234/stripeR&#34;&gt;&lt;code&gt;stripeR&lt;/code&gt;&lt;/a&gt; this makes paid Shiny apps that work for users when offline a possibility.&lt;/p&gt;

&lt;h2 id=&#34;full-change-list&#34;&gt;Full change list&lt;/h2&gt;

&lt;p&gt;All changes from news.md is listed below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Added ability to add your own custom headers to requests via customConfig in &lt;code&gt;gar_api_generator&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Add &amp;lsquo;localhost&amp;rsquo; to shiny URL detection.&lt;/li&gt;
&lt;li&gt;Google Service accounts now supported. Authenticate via &amp;ldquo;Service Account Key&amp;rdquo; JSON.&lt;/li&gt;
&lt;li&gt;Exposed &lt;code&gt;gar_shiny_getUrl&lt;/code&gt; and the authentication type (&lt;code&gt;online/offline&lt;/code&gt;) in &lt;code&gt;renderLogin&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;renderLogin&lt;/code&gt; : logout now has option revoke to revoke authentication token&lt;/li&gt;
&lt;li&gt;Added option for &lt;code&gt;googleAuthR.jsonlite.simplifyVector&lt;/code&gt; for content parsing for compatibility for some APIs&lt;/li&gt;
&lt;li&gt;Batch Google API requests now implemented. See readme or &lt;code&gt;?gar_batch&lt;/code&gt; and &lt;code&gt;?gar_batch_walk&lt;/code&gt; for details.&lt;/li&gt;
&lt;li&gt;If data parsing fails, return the raw content so you can test and modify your data parsing function&lt;/li&gt;
&lt;li&gt;Missed Jenny credit now corrected&lt;/li&gt;
&lt;li&gt;Add tip about using &lt;code&gt;!is.null(access_token())&lt;/code&gt; to detect login state&lt;/li&gt;
&lt;li&gt;Add HTTP backoff for certain errors (#6) from Johann&lt;/li&gt;
&lt;li&gt;Remove possible &lt;code&gt;NULL&lt;/code&gt; entries from path and pars argument lists&lt;/li&gt;
&lt;li&gt;Reduced some unnecessary message feedback&lt;/li&gt;
&lt;li&gt;moved &lt;code&gt;with_shiny&lt;/code&gt;environment lookup to within generated function&lt;/li&gt;
&lt;li&gt;added gzip to headers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/MarkEdmondson1234/googleAuthR&#34;&gt;&lt;img src=&#34;https://ga-beacon.appspot.com/UA-73050356-1/115064340704113209584/googleAuthR/v0.2.0&#34; alt=&#34;Analytics&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
